{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac8b4ec5",
   "metadata": {},
   "source": [
    "# Notebook 02: OASIS-2 Cohort Definition for Longitudinal Analysis\n",
    "\n",
    "**Project Phase:** 1 (Data Processing - Cohort Definition)\n",
    "**Dataset:** OASIS-2 Longitudinal MRI & Clinical Data\n",
    "\n",
    "**Purpose:**\n",
    "This notebook defines the final analysis cohort from the OASIS-2 dataset based on specific inclusion and exclusion criteria. It builds upon the raw clinical data and the MRI scan file verification results from Notebook 01. The key objectives are:\n",
    "1.  Load the raw clinical dataset and the MRI verification details.\n",
    "2.  Apply inclusion criteria:\n",
    "    * Baseline CDR score (e.g., selecting subjects who are Cognitively Normal or have Mild Cognitive Impairment at their first visit).\n",
    "    * Minimum number of longitudinal visits per subject to ensure sufficient data for sequence modeling.\n",
    "    * Availability of verified MRI scan files for the selected visits.\n",
    "3.  Log all cohort definition criteria and step-by-step filtering statistics to Weights & Biases (W&B).\n",
    "4.  Save the final cohort DataFrame locally as `final_analysis_cohort.csv`.\n",
    "5.  Log the final cohort DataFrame as a versioned artifact to W&B for use in downstream feature engineering and modeling notebooks.\n",
    "\n",
    "**Workflow:**\n",
    "1.  **Setup:** Import libraries, set up `src` path, load `config.json`, and define initial paths including inputs from Notebook 01.\n",
    "2.  **W&B Initialization:** Start a new W&B run for this cohort definition task using the `initialize_wandb_run` utility. Define the output directory for this notebook.\n",
    "3.  **Load Input Data:** Load the raw clinical data (from original Excel) and the `verification_details.csv` (from Notebook 01).\n",
    "4.  **Filter by Baseline CDR:** Select subjects based on their CDR score at their first available visit. Log counts.\n",
    "5.  **Filter by Minimum Visits:** Analyze visit counts for the remaining subjects, decide on a minimum visit threshold, and apply the filter. Log counts and criteria.\n",
    "6.  **Filter by MRI Availability:** Retain only those visits from the filtered cohort that have a corresponding verified MRI scan (based on `verification_details.csv`). Log counts.\n",
    "7.  **Summarize & Save Final Cohort:** Print characteristics of the final cohort, save it locally, and log it as a W&B artifact.\n",
    "8.  **Define Prediction Task (High-Level):** Briefly log the intended prediction task for which this cohort is being prepared.\n",
    "9.  **Finalize W&B Run.**\n",
    "\n",
    "**Input:**\n",
    "* `config.json`: Main project configuration file.\n",
    "* Raw OASIS-2 Clinical Data Excel file (path from `config.json`).\n",
    "* `verification_details.csv`: Output from Notebook 01, containing MRI file verification status (path constructed based on `config.json` and Notebook 01's output structure).\n",
    "\n",
    "**Output:**\n",
    "* **Local Files (in notebook-specific output directory, e.g., `notebooks/outputs/02_Cohort_Definition_OASIS2/`):**\n",
    "    * `final_analysis_cohort.csv` (Key input for Notebook 03)\n",
    "    * Plots related to visit count distributions (if any generated in this notebook).\n",
    "* **W&B Run:**\n",
    "    * Logged run configuration (including input paths and cohort criteria).\n",
    "    * Step-by-step subject/visit counts after each filtering stage.\n",
    "    * Final cohort criteria (baseline CDR, min visits).\n",
    "    * The `final_analysis_cohort.csv` logged as a W&B Artifact (e.g., `analysis_cohort-OASIS2-CDR_X_X-MinV_Y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ca3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: notebooks/02_Cohort_Definition.ipynb\n",
    "# Purpose: Define the analysis cohort for longitudinal prediction based on OASIS-2 data.\n",
    "#          Applies criteria for baseline status, minimum visits, and MRI availability.\n",
    "#          Logs decisions and outputs the final cohort definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d68c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be0742c",
   "metadata": {},
   "source": [
    "## 1. Setup: Project Configuration, Paths, and Utilities\n",
    "\n",
    "This section initializes the notebook environment:\n",
    "* Determines the project's root directory to enable access to shared resources and modules.\n",
    "* Adds the `src` directory to the Python system path for importing custom utility functions.\n",
    "* Imports necessary custom utilities, particularly for W&B run initialization and path management.\n",
    "* Loads the main project configuration from `config.json`.\n",
    "* Defines key dataset identifiers and notebook-specific parameters.\n",
    "* Resolves and prints essential input paths (raw clinical data, MRI verification results from Notebook 01) and sets up the primary output directory for this notebook's locally saved files (e.g., the final cohort CSV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ff165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Project Setup, Configuration Loading, and Utility Imports ---\n",
    "print(\"--- Initializing Project Setup & Configuration ---\")\n",
    "\n",
    "# Initialize\n",
    "PROJECT_ROOT = None\n",
    "base_config = {}\n",
    "\n",
    "try:\n",
    "    # --- 1. Determine Project Root and Add src to Python Path ---\n",
    "    current_notebook_path = Path.cwd() # Assumes notebook is run from its directory\n",
    "    # Try to find project root assuming standard structure: <PROJECT_ROOT>/notebooks/02_...\n",
    "    potential_project_root = current_notebook_path.parent \n",
    "    if (potential_project_root / \"src\").is_dir() and \\\n",
    "       (potential_project_root / \"config.json\").is_file():\n",
    "        PROJECT_ROOT = potential_project_root\n",
    "    else: # Fallback: assume current working directory IS the project root\n",
    "        PROJECT_ROOT = current_notebook_path \n",
    "    \n",
    "    if not (PROJECT_ROOT / \"src\").is_dir() or not (PROJECT_ROOT / \"config.json\").is_file():\n",
    "        raise FileNotFoundError(\n",
    "            f\"CRITICAL: 'src' directory or 'config.json' not found relative to determined \"\n",
    "            f\"PROJECT_ROOT: {PROJECT_ROOT}. Ensure 'config.json' is at the project root.\"\n",
    "        )\n",
    "\n",
    "    if str(PROJECT_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(PROJECT_ROOT)) # Add project root to allow 'from src...'\n",
    "    print(f\"PROJECT_ROOT successfully set to: {PROJECT_ROOT}\")\n",
    "    print(f\"Added '{str(PROJECT_ROOT)}' to sys.path.\")\n",
    "\n",
    "    # --- 2. Import Custom Utilities ---\n",
    "    from src.wandb_utils import initialize_wandb_run\n",
    "    from src.plotting_utils import finalize_plot \n",
    "    print(\"Successfully imported custom utilities\")\n",
    "\n",
    "    # --- 3. Load Main Project Configuration ---\n",
    "    CONFIG_PATH_MAIN = PROJECT_ROOT / 'config.json'\n",
    "    with open(CONFIG_PATH_MAIN, 'r', encoding='utf-8') as f:\n",
    "        base_config = json.load(f)\n",
    "    print(f\"Main project configuration loaded from: {CONFIG_PATH_MAIN}\")\n",
    "\n",
    "    # --- 4. Define Dataset, Notebook Specifics, and Key Paths for NB02 ---\n",
    "    DATASET_IDENTIFIER = \"oasis2\" # Specific to this notebook's current focus\n",
    "    NOTEBOOK_MODULE_NAME = \"02_Cohort_Definition\" # For job_type and output folder naming\n",
    "    \n",
    "    # Key from config.json's 'pipeline_artefact_locators_oasis2' for this notebook's output subfolder\n",
    "    NB02_OUTPUT_LOCATOR_KEY = \"cohort_def_subdir\" \n",
    "\n",
    "    # Resolve critical INPUT paths from base_config\n",
    "    INPUT_DATA_PATH_CLINICAL = PROJECT_ROOT / base_config['data']['clinical_excel_path']\n",
    "    \n",
    "    output_dir_base_from_config = PROJECT_ROOT / base_config['data']['output_dir_base']\n",
    "    dataset_locators = base_config.get(f\"pipeline_artefact_locators_{DATASET_IDENTIFIER}\", {})\n",
    "    if not dataset_locators:\n",
    "        raise KeyError(f\"pipeline_artefact_locators_{DATASET_IDENTIFIER} section not found or empty in config.json.\")\n",
    "\n",
    "    # Path to verification_details.csv (output from Notebook 01)\n",
    "    nb01_output_subdir_name_key = \"exploration_subdir\"\n",
    "    nb01_verification_fname_key = \"verification_csv_fname\"\n",
    "    \n",
    "    nb01_output_subdir_name = dataset_locators.get(nb01_output_subdir_name_key)\n",
    "    nb01_verification_fname = dataset_locators.get(nb01_verification_fname_key)\n",
    "\n",
    "    if not nb01_output_subdir_name or not nb01_verification_fname:\n",
    "        raise KeyError(f\"Missing '{nb01_output_subdir_name_key}' or '{nb01_verification_fname_key}' \"\n",
    "                       f\"in '{f'pipeline_artefact_locators_{DATASET_IDENTIFIER}'}' of config.json.\")\n",
    "    VERIFICATION_CSV_PATH_NB02_INPUT = output_dir_base_from_config / nb01_output_subdir_name / nb01_verification_fname\n",
    "\n",
    "    # Define the main OUTPUT directory for THIS notebook's files (e.g., final_analysis_cohort.csv)\n",
    "    # This uses a static output directory pattern for NB02, derived from config.\n",
    "    notebook_output_folder_name_from_locators = dataset_locators.get(\n",
    "        NB02_OUTPUT_LOCATOR_KEY, \n",
    "        f\"{NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER}_default_outputs\" # Fallback name\n",
    "    )\n",
    "    output_dir = output_dir_base_from_config / notebook_output_folder_name_from_locators\n",
    "    output_dir.mkdir(parents=True, exist_ok=True) # Create if it doesn't exist\n",
    "\n",
    "    # --- Print and Verify Key Paths ---\n",
    "    print(f\"\\nKey paths defined for Notebook 02 ({DATASET_IDENTIFIER}):\")\n",
    "    print(f\"  Input Raw Clinical Data Excel: {INPUT_DATA_PATH_CLINICAL}\")\n",
    "    print(f\"  Input MRI Verification CSV (from NB01): {VERIFICATION_CSV_PATH_NB02_INPUT}\")\n",
    "    print(f\"  Notebook Output Directory (for local saves like final_analysis_cohort.csv): {output_dir}\")\n",
    "    \n",
    "    # Critical Input File Checks\n",
    "    if not INPUT_DATA_PATH_CLINICAL.is_file():\n",
    "        raise FileNotFoundError(f\"CRITICAL: Input clinical data excel file not found at specified path: {INPUT_DATA_PATH_CLINICAL}\")\n",
    "    if not VERIFICATION_CSV_PATH_NB02_INPUT.is_file():\n",
    "        raise FileNotFoundError(f\"CRITICAL: MRI verification CSV (from NB01) not found at specified path: {VERIFICATION_CSV_PATH_NB02_INPUT}. \"\n",
    "                                \"Ensure Notebook 01 ran successfully and config.json locators are correct.\")\n",
    "    print(\"All critical input paths for NB02 verified.\")\n",
    "\n",
    "except (FileNotFoundError, KeyError, ValueError) as e_setup: # Catch specific expected errors\n",
    "    print(f\"CRITICAL ERROR during setup in Notebook 02: {e_setup}\")\n",
    "    # exit() # Optional: exit if setup fails\n",
    "except Exception as e_general:\n",
    "    print(f\"An unexpected CRITICAL ERROR occurred during setup in Notebook 02: {e_general}\")\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c48177",
   "metadata": {},
   "source": [
    "## 2. Initialize Weights & Biases Run\n",
    "\n",
    "A new W&B run is initialized for this cohort definition notebook. This run will track the configuration parameters used for defining the cohort, step-by-step filtering statistics, and the final cohort dataset as an artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b120460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize W&B Run for THIS Notebook Execution (NB02) ---\n",
    "print(\"\\n--- Initializing Weights & Biases Run for Notebook 02 ---\")\n",
    "\n",
    "# Configuration specific to this NB02 run\n",
    "nb02_run_config_log = {\n",
    "    \"notebook_name_code\": f\"{NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER}\",\n",
    "    \"dataset_source\": DATASET_IDENTIFIER,\n",
    "    \"input_raw_clinical_data_path\": str(INPUT_DATA_PATH_CLINICAL), # From Cell 3\n",
    "    \"input_mri_verification_csv_path\": str(VERIFICATION_CSV_PATH_NB02_INPUT), # From Cell 3\n",
    "    \"output_dir_for_local_saves\": str(output_dir), # From Cell 3\n",
    "    \"execution_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Cohort selection criteria (baseline CDR, min_visits) will be added later via run.config.update()\n",
    "}\n",
    "\n",
    "# Extract notebook number for naming and job type\n",
    "nb_number_prefix_nb02 = NOTEBOOK_MODULE_NAME.split('_')[0] if '_' in NOTEBOOK_MODULE_NAME else \"NB\"\n",
    "job_specific_type_nb02 = f\"{nb_number_prefix_nb02}-CohortDefinition-{DATASET_IDENTIFIER}\"\n",
    "custom_elements_for_name_nb02 = [nb_number_prefix_nb02, DATASET_IDENTIFIER.upper(), \"CohortDef\"]\n",
    "\n",
    "run = initialize_wandb_run(\n",
    "    base_project_config=base_config,\n",
    "    job_group=\"DataProcessing\",\n",
    "    job_specific_type=job_specific_type_nb02,\n",
    "    run_specific_config=nb02_run_config_log,\n",
    "    custom_run_name_elements=custom_elements_for_name_nb02,\n",
    "    notes=f\"{DATASET_IDENTIFIER.upper()}: Defining analysis cohort based on inclusion criteria.\"\n",
    ")\n",
    "\n",
    "if run:\n",
    "    print(f\"W&B run '{run.name}' (Job Type: '{run.job_type}') initialized. View at: {run.url}\")\n",
    "else:\n",
    "    print(\"Proceeding without W&B logging for this session (W&B run initialization failed).\")\n",
    "    # output_dir should still be defined from Cell 3 for local saves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f86a4a3",
   "metadata": {},
   "source": [
    "## 3. Load Raw Clinical Data\n",
    "\n",
    "Load the raw longitudinal clinical and demographic data from the Excel file specified in `config.json`. This dataset forms the basis from which the analysis cohort will be derived. The shape of the loaded data and a W&B artifact for the raw data are logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b795504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Raw Clinical Data ---\n",
    "# INPUT_DATA_PATH_CLINICAL should be defined in the setup cell\n",
    "print(f\"\\n--- Loading Raw Clinical Data from: {INPUT_DATA_PATH_CLINICAL} ---\")\n",
    "clinical_df_raw = None # Initialize to ensure it's defined in case of errors\n",
    "\n",
    "try:\n",
    "    if INPUT_DATA_PATH_CLINICAL is None or not INPUT_DATA_PATH_CLINICAL.is_file():\n",
    "         raise FileNotFoundError(f\"Input clinical data file path not defined or file not found: {INPUT_DATA_PATH_CLINICAL}\")\n",
    "    \n",
    "    clinical_df_raw = pd.read_excel(INPUT_DATA_PATH_CLINICAL)\n",
    "    print(f\"Raw clinical data loaded successfully. Shape: {clinical_df_raw.shape}\")\n",
    "\n",
    "    if clinical_df_raw.empty:\n",
    "        print(\"CRITICAL ERROR: Loaded raw clinical dataframe is empty. Stopping execution.\")\n",
    "        if run: run.finish(exit_code=1) # Finish W&B run with error status\n",
    "        # exit() # Or raise an error to halt the notebook\n",
    "\n",
    "    # Log initial raw data characteristics to W&B\n",
    "    if run: \n",
    "        run.log({'cohort_definition/00_raw_clinical_rows': clinical_df_raw.shape[0],\n",
    "                 'cohort_definition/00_raw_clinical_columns': clinical_df_raw.shape[1]})\n",
    "        \n",
    "        # Log the raw data file as a W&B artifact for complete traceability\n",
    "        print(\"Logging raw clinical data file as W&B artifact...\")\n",
    "        raw_data_artifact_name = f\"raw_clinical_data_source_{DATASET_IDENTIFIER}\"\n",
    "        raw_data_artifact = wandb.Artifact(\n",
    "            raw_data_artifact_name, \n",
    "            type=f\"raw_dataset_{DATASET_IDENTIFIER}\", # Use a consistent type for raw datasets\n",
    "            description=f\"Raw clinical and demographic data for {DATASET_IDENTIFIER} dataset, \"\n",
    "                        f\"loaded from {INPUT_DATA_PATH_CLINICAL.name}.\",\n",
    "            metadata={\n",
    "                \"source_file_path\": str(INPUT_DATA_PATH_CLINICAL), \n",
    "                \"shape_rows\": clinical_df_raw.shape[0],\n",
    "                \"shape_columns\": clinical_df_raw.shape[1],\n",
    "                \"load_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\") # From import time\n",
    "            }\n",
    "        )\n",
    "        raw_data_artifact.add_file(str(INPUT_DATA_PATH_CLINICAL), name=\"source_excel_file.xlsx\")\n",
    "        run.log_artifact(raw_data_artifact, aliases=[\"original\", f\"{time.strftime('%Y%m%d')}\"])\n",
    "        print(f\"Raw data artifact '{raw_data_artifact_name}' logged to W&B.\")\n",
    "\n",
    "except FileNotFoundError as e_fnf:\n",
    "    print(f\"CRITICAL ERROR: {e_fnf}\")\n",
    "    if run: run.finish(exit_code=1)\n",
    "    # exit()\n",
    "except ImportError: # For pd.read_excel if 'openpyxl' is missing\n",
    "     print(f\"CRITICAL ERROR loading Excel file: Missing 'openpyxl' library. Please install it: `pip install openpyxl`\")\n",
    "     if run: run.finish(exit_code=1)\n",
    "     # exit()\n",
    "except Exception as e_load_raw:\n",
    "    print(f\"CRITICAL ERROR occurred while loading the raw clinical data: {e_load_raw}\")\n",
    "    if run: run.finish(exit_code=1)\n",
    "    # exit()\n",
    "\n",
    "# Ensure clinical_df_raw is defined for subsequent cells, even if empty after an error (if not exiting)\n",
    "if clinical_df_raw is None:\n",
    "    clinical_df_raw = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd7af04",
   "metadata": {},
   "source": [
    "## 4. Load MRI Scan Verification Results\n",
    "\n",
    "Load the `verification_details.csv` file, which was generated by Notebook 01. This CSV contains information on which `MRI ID`s (scan sessions) have successfully located raw scan files (`.img` + `.hdr` pairs) in the local filesystem. This step is crucial for ensuring that the cohort definition only includes subjects/visits for whom MRI data is actually available for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load MRI Scan Verification Results (Output from Notebook 01) ---\n",
    "# VERIFICATION_CSV_PATH_NB02_INPUT should be defined in the setup cell\n",
    "print(f\"\\n--- Loading MRI Verification Results from: {VERIFICATION_CSV_PATH_NB02_INPUT} ---\")\n",
    "verification_df = None # Initialize\n",
    "verified_mri_ids = set() # Initialize as an empty set\n",
    "\n",
    "try:\n",
    "    if VERIFICATION_CSV_PATH_NB02_INPUT is None or not VERIFICATION_CSV_PATH_NB02_INPUT.is_file():\n",
    "         raise FileNotFoundError(f\"MRI verification CSV file path not defined or file not found: {VERIFICATION_CSV_PATH_NB02_INPUT}. \"\n",
    "                                 \"Please ensure Notebook 01 ran successfully and saved this file, \"\n",
    "                                 \"and that config.json locators are correct.\")\n",
    "    \n",
    "    verification_df = pd.read_csv(VERIFICATION_CSV_PATH_NB02_INPUT)\n",
    "    print(f\"MRI verification data loaded successfully. Shape: {verification_df.shape}\")\n",
    "\n",
    "    # Extract the set of MRI IDs that passed verification (e.g., folder exists and contains valid scans)\n",
    "    # The criteria for \"passed\" might be more stringent, e.g., 'mprs_found_count' > 0\n",
    "    # For now, using 'mri_folder_exists'\n",
    "    if 'mri_folder_exists' in verification_df.columns and 'mri_id' in verification_df.columns:\n",
    "        verified_mri_ids = set(verification_df[verification_df['mri_folder_exists'] == True]['mri_id'].unique())\n",
    "        print(f\"Found {len(verified_mri_ids)} unique MRI IDs with existing scan folders based on verification file.\")\n",
    "    else:\n",
    "        print(\"Warning: 'mri_folder_exists' or 'mri_id' column not found in verification CSV. Cannot determine verified MRI IDs.\")\n",
    "        # verified_mri_ids remains an empty set\n",
    "\n",
    "    if run: \n",
    "        run.log({'cohort_definition/00_input_verified_mri_ids_count': len(verified_mri_ids)})\n",
    "        # Log the verification_details.csv used as an input artifact to this run\n",
    "        verif_input_artifact_name = f\"input_verification_details_{DATASET_IDENTIFIER}\"\n",
    "        verif_input_artifact = wandb.Artifact(\n",
    "            verif_input_artifact_name,\n",
    "            type=\"input_metadata\",\n",
    "            description=f\"MRI verification details CSV used as input for cohort definition. From NB01 output: {VERIFICATION_CSV_PATH_NB02_INPUT.name}\"\n",
    "        )\n",
    "        verif_input_artifact.add_file(str(VERIFICATION_CSV_PATH_NB02_INPUT))\n",
    "        run.log_artifact(verif_input_artifact)\n",
    "        print(f\"Logged '{VERIFICATION_CSV_PATH_NB02_INPUT.name}' as an input artifact to W&B.\")\n",
    "\n",
    "\n",
    "except FileNotFoundError as e_fnf_verif:\n",
    "    print(f\"CRITICAL ERROR: {e_fnf_verif}\")\n",
    "    print(\"Cannot proceed with cohort definition without MRI verification results.\")\n",
    "    if run: run.finish(exit_code=1)\n",
    "    # exit()\n",
    "except Exception as e_load_verif:\n",
    "    print(f\"CRITICAL ERROR occurred while loading MRI verification results: {e_load_verif}\")\n",
    "    if run: run.finish(exit_code=1)\n",
    "    # exit()\n",
    "\n",
    "# Ensure verification_df and verified_mri_ids are defined for subsequent cells\n",
    "if verification_df is None:\n",
    "    verification_df = pd.DataFrame()\n",
    "# verified_mri_ids is already initialized to an empty set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d2442c",
   "metadata": {},
   "source": [
    "## 5. Cohort Definition - Step 1: Filter by Baseline CDR\n",
    "\n",
    "Apply the first inclusion criterion based on the subject's cognitive status at their *first available visit within the raw clinical data*. Subjects are included if their baseline CDR score is 0.0 (Cognitively Normal) or 0.5 (Very Mild Cognitive Impairment). The number of unique subjects and total visits remaining after this filter are logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd04bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cohort Definition Step 1: Filter by Baseline CDR ---\n",
    "print(\"\\n--- Applying Baseline CDR Filter (Keeping CDR=0.0 and CDR=0.5) ---\")\n",
    "\n",
    "df_baseline_filtered = pd.DataFrame() # Initialize\n",
    "num_subjects_baseline_criteria = 0\n",
    "\n",
    "if 'clinical_df_raw' in locals() and not clinical_df_raw.empty:\n",
    "    # Define the baseline CDR criteria\n",
    "    baseline_cdr_criteria = [0.0, 0.5]\n",
    "    print(f\"Baseline CDR inclusion criteria: {baseline_cdr_criteria}\")\n",
    "\n",
    "    # Ensure data is sorted by 'Subject ID' and 'Visit' to correctly identify the first visit\n",
    "    # Using a copy to avoid modifying clinical_df_raw if it's used elsewhere unfiltered\n",
    "    clinical_df_for_baseline_check = clinical_df_raw.sort_values(by=['Subject ID', 'Visit'])\n",
    "    \n",
    "    # Get data for the first visit of each subject\n",
    "    # .loc[...idxmin()] is a robust way to get the entire row for the first visit\n",
    "    first_visit_data_df = clinical_df_for_baseline_check.loc[\n",
    "        clinical_df_for_baseline_check.groupby('Subject ID')['Visit'].idxmin()\n",
    "    ]\n",
    "\n",
    "    # Identify subjects meeting the baseline CDR criteria\n",
    "    if 'CDR' in first_visit_data_df.columns and 'Subject ID' in first_visit_data_df.columns:\n",
    "        subjects_meeting_criteria_arr = first_visit_data_df[\n",
    "            first_visit_data_df['CDR'].isin(baseline_cdr_criteria)\n",
    "        ]['Subject ID'].unique()\n",
    "        num_subjects_baseline_criteria = len(subjects_meeting_criteria_arr)\n",
    "        print(f\"Found {num_subjects_baseline_criteria} unique subjects meeting baseline CDR criteria.\")\n",
    "\n",
    "        # Filter the original raw dataframe to keep all visits of these selected subjects\n",
    "        df_baseline_filtered = clinical_df_raw[\n",
    "            clinical_df_raw['Subject ID'].isin(subjects_meeting_criteria_arr)\n",
    "        ].copy() # Use .copy() to avoid SettingWithCopyWarning on slices\n",
    "        print(f\"DataFrame shape after baseline CDR filter: {df_baseline_filtered.shape} \"\n",
    "              f\"({df_baseline_filtered['Subject ID'].nunique()} subjects)\")\n",
    "    else:\n",
    "        print(\"Warning: 'CDR' or 'Subject ID' column not found in first_visit_data_df. Cannot apply baseline CDR filter.\")\n",
    "        df_baseline_filtered = clinical_df_raw.copy() # Proceed with unfiltered data if CDR filter fails\n",
    "\n",
    "    # Log results to W&B\n",
    "    if run:\n",
    "        # Log the criteria used to W&B config for this run\n",
    "        wandb.config.update({'cohort_criteria/baseline_cdr_included': baseline_cdr_criteria}, allow_val_change=True)\n",
    "        run.log({\n",
    "            'cohort_definition/01_subjects_meeting_baseline_cdr': num_subjects_baseline_criteria,\n",
    "            'cohort_definition/01_visits_after_baseline_cdr_filter': len(df_baseline_filtered),\n",
    "            'cohort_definition/01_subjects_after_baseline_cdr_filter': df_baseline_filtered['Subject ID'].nunique()\n",
    "        })\n",
    "else:\n",
    "    print(\"Skipping baseline CDR filter as raw clinical data (clinical_df_raw) is not available or empty.\")\n",
    "    # df_baseline_filtered remains an empty DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c979542a",
   "metadata": {},
   "source": [
    "## 6. Cohort Definition - Step 2: Filter by Minimum Number of Visits\n",
    "\n",
    "To ensure sufficient longitudinal data for sequence modeling, subjects are filtered based on a minimum number of recorded visits. This section first analyzes the distribution of visit counts for subjects who met the baseline CDR criteria. Based on this distribution, a data-informed threshold for the minimum number of visits is determined and applied. Statistics before and after this filter, along with the chosen threshold, are logged to W&B. A visualization of the visit count distribution is also generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd876d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cohort Definition Step 2: Check and Apply Minimum Visits Filter ---\n",
    "print(\"\\n--- Checking and Applying Minimum Visits Filter ---\")\n",
    "\n",
    "df_min_visits_filtered = pd.DataFrame() # Initialize\n",
    "num_subjects_min_visits = 0\n",
    "min_visits_required = 0 # Initialize\n",
    "\n",
    "# Ensure df_baseline_filtered exists and is not empty from the previous step\n",
    "if 'df_baseline_filtered' in locals() and not df_baseline_filtered.empty:\n",
    "    # Count visits per subject *within the baseline-filtered group*\n",
    "    visits_per_subject_filtered = df_baseline_filtered.groupby('Subject ID')['Visit'].count()\n",
    "\n",
    "    # --- Analyze visit counts before setting threshold ---\n",
    "    print(\"\\nDistribution of visit counts (for subjects meeting baseline CDR criteria):\")\n",
    "    visit_counts_distribution = visits_per_subject_filtered.value_counts().sort_index()\n",
    "    print(visit_counts_distribution)\n",
    "\n",
    "    total_subjects_after_cdr_filter = df_baseline_filtered['Subject ID'].nunique() # More robust count\n",
    "    \n",
    "    # Calculate counts and percentages for different visit thresholds\n",
    "    count_ge_2_visits = sum(visits_per_subject_filtered >= 2)\n",
    "    count_ge_3_visits = sum(visits_per_subject_filtered >= 3)\n",
    "    count_ge_4_visits = sum(visits_per_subject_filtered >= 4)\n",
    "\n",
    "    percent_ge_2_visits = count_ge_2_visits / total_subjects_after_cdr_filter if total_subjects_after_cdr_filter > 0 else 0\n",
    "    percent_ge_3_visits = count_ge_3_visits / total_subjects_after_cdr_filter if total_subjects_after_cdr_filter > 0 else 0\n",
    "    percent_ge_4_visits = count_ge_4_visits / total_subjects_after_cdr_filter if total_subjects_after_cdr_filter > 0 else 0\n",
    "\n",
    "    print(f\"\\nSubjects meeting baseline CDR criteria: {total_subjects_after_cdr_filter}\")\n",
    "    print(f\"  Number with >= 2 visits: {count_ge_2_visits} ({percent_ge_2_visits:.1%})\")\n",
    "    print(f\"  Number with >= 3 visits: {count_ge_3_visits} ({percent_ge_3_visits:.1%})\")\n",
    "    print(f\"  Number with >= 4 visits: {count_ge_4_visits} ({percent_ge_4_visits:.1%})\")\n",
    "\n",
    "    # Log cohort check stats to W&B\n",
    "    if run:\n",
    "        run.log({\n",
    "            'cohort_check/01_total_baseline_criteria_subjects': total_subjects_after_cdr_filter,\n",
    "            'cohort_check/02_subjects_ge_2_visits': count_ge_2_visits,\n",
    "            'cohort_check/03_subjects_ge_3_visits': count_ge_3_visits,\n",
    "            'cohort_check/04_subjects_ge_4_visits': count_ge_4_visits,\n",
    "            'cohort_check/05_percent_ge_2_visits': percent_ge_2_visits,\n",
    "            'cohort_check/06_percent_ge_3_visits': percent_ge_3_visits,\n",
    "            'cohort_check/07_percent_ge_4_visits': percent_ge_4_visits\n",
    "        })\n",
    "        try:\n",
    "            visit_counts_table_df = visit_counts_distribution.reset_index()\n",
    "            visit_counts_table_df.columns = ['number_of_visits', 'subject_count'] # Rename for clarity\n",
    "            visit_counts_wandb_table = wandb.Table(dataframe=visit_counts_table_df)\n",
    "            run.log({\"cohort_check/08_visit_count_distribution_table\": visit_counts_wandb_table})\n",
    "        except Exception as e_wandb_table:\n",
    "            print(f\"Warning: Could not log visit count distribution table to W&B. Error: {e_wandb_table}\")\n",
    "\n",
    "    # Visualize visit counts distribution\n",
    "    fig_visit_counts, ax_visit_counts = plt.subplots(figsize=(10, 6))\n",
    "    sns.countplot(x=visits_per_subject_filtered, ax=ax_visit_counts, color='skyblue', stat='count')\n",
    "    ax_visit_counts.set_title(f'Visit Counts per Subject (After Baseline CDR Filter: {baseline_cdr_criteria})')\n",
    "    ax_visit_counts.set_xlabel('Number of Visits Recorded per Subject')\n",
    "    ax_visit_counts.set_ylabel('Number of Subjects')\n",
    "    finalize_plot(fig_visit_counts, plt, run, \n",
    "                  f\"charts_cohort_def_{DATASET_IDENTIFIER}/01_visit_counts_distribution\", \n",
    "                  output_dir / '01_cohort_visit_counts.png')\n",
    "\n",
    "    # --- Make data-driven decision on min_visits_required ---\n",
    "    # Example logic: Prioritize >=3 visits if a substantial portion of the cohort has them.\n",
    "    # Adjust thresholds and cohort size checks (e.g., count_ge_2_visits > 30) as needed for the specific dataset.\n",
    "    if percent_ge_3_visits >= 0.40 and count_ge_3_visits > 20: # If >=40% have 3+ visits AND it's a reasonable number\n",
    "        min_visits_required = 3\n",
    "    elif count_ge_2_visits > 20: # Else, if enough subjects have at least 2 visits\n",
    "        min_visits_required = 2\n",
    "    else: # Fallback if cohort becomes very small\n",
    "        min_visits_required = 2 # Or 1, depending on absolute minimum for modeling\n",
    "        print(f\"Warning: Low number of subjects with multiple visits. Setting min_visits_required = {min_visits_required}. \"\n",
    "              \"Consider re-evaluating baseline criteria if final cohort size is too small.\")\n",
    "    print(f\"\\nDecision: Setting minimum visits required per subject = {min_visits_required}.\")\n",
    "\n",
    "    if run:\n",
    "        wandb.config.update({'cohort_criteria/min_visits_required': min_visits_required}, allow_val_change=True)\n",
    "\n",
    "    # Identify subjects meeting the minimum visit count\n",
    "    subjects_with_enough_visits = visits_per_subject_filtered[visits_per_subject_filtered >= min_visits_required].index\n",
    "    num_subjects_min_visits = len(subjects_with_enough_visits)\n",
    "    print(f\"Found {num_subjects_min_visits} unique subjects meeting baseline CDR and >= {min_visits_required} visits criteria.\")\n",
    "\n",
    "    # Filter the DataFrame to keep all visits of these subjects\n",
    "    df_min_visits_filtered = df_baseline_filtered[df_baseline_filtered['Subject ID'].isin(subjects_with_enough_visits)].copy()\n",
    "    print(f\"DataFrame shape after min visits filter: {df_min_visits_filtered.shape} ({df_min_visits_filtered['Subject ID'].nunique()} subjects)\")\n",
    "\n",
    "    if run:\n",
    "        run.log({\n",
    "            'cohort_definition/02_subjects_after_min_visits_filter': num_subjects_min_visits,\n",
    "            'cohort_definition/02_visits_after_min_visits_filter': len(df_min_visits_filtered)\n",
    "        })\n",
    "else:\n",
    "    print(\"Skipping minimum visits filter as 'df_baseline_filtered' is empty or not defined.\")\n",
    "    # Ensure df_min_visits_filtered is an empty DataFrame for subsequent steps if pipeline were to continue\n",
    "    df_min_visits_filtered = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aa0399",
   "metadata": {},
   "source": [
    "## 7. Cohort Definition - Step 3: Filter by MRI Availability\n",
    "\n",
    "The final cohort inclusion step ensures that all selected visits have corresponding, successfully verified MRI scan files. This filter uses the set of `verified_mri_ids` generated in Notebook 01 (and loaded earlier in this notebook). Visits without a verified MRI are removed. The number of visits removed and the final cohort size (subjects and visits) are logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c9e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cohort Definition Step 3: Filter by MRI Availability ---\n",
    "print(\"\\n--- Applying MRI Verification Filter ---\")\n",
    "\n",
    "cohort_df_final = pd.DataFrame() # Initialize\n",
    "final_subjects = 0\n",
    "final_visits = 0\n",
    "\n",
    "# Ensure df_min_visits_filtered exists and is not empty from the previous step\n",
    "# Also ensure verified_mri_ids was loaded and is a set\n",
    "if 'df_min_visits_filtered' in locals() and not df_min_visits_filtered.empty and \\\n",
    "   'verified_mri_ids' in locals() and isinstance(verified_mri_ids, set):\n",
    "\n",
    "    initial_visits_before_mri_filter = len(df_min_visits_filtered)\n",
    "    \n",
    "    # Filter to keep only rows where 'MRI ID' is in the set of verified_mri_ids\n",
    "    # Assumes 'MRI ID' column exists in df_min_visits_filtered\n",
    "    if 'MRI ID' in df_min_visits_filtered.columns:\n",
    "        cohort_df_final = df_min_visits_filtered[df_min_visits_filtered['MRI ID'].isin(verified_mri_ids)].copy()\n",
    "        \n",
    "        final_subjects = cohort_df_final['Subject ID'].nunique() if 'Subject ID' in cohort_df_final.columns else 0\n",
    "        final_visits = len(cohort_df_final)\n",
    "        visits_removed_by_mri_filter = initial_visits_before_mri_filter - final_visits\n",
    "\n",
    "        print(f\"Removed {visits_removed_by_mri_filter} visits due to missing or unverified MRI scans.\")\n",
    "        print(f\"Final cohort for modeling: {final_visits} visits from {final_subjects} subjects.\")\n",
    "\n",
    "        if run:\n",
    "            run.log({\n",
    "                'cohort_definition/03_visits_before_mri_filter': initial_visits_before_mri_filter,\n",
    "                'cohort_definition/03_visits_removed_by_mri_filter': visits_removed_by_mri_filter,\n",
    "                'cohort_definition/03_final_subject_count': final_subjects,\n",
    "                'cohort_definition/03_final_visit_count': final_visits\n",
    "            })\n",
    "    else:\n",
    "        print(\"Warning: 'MRI ID' column not found in DataFrame. Cannot apply MRI verification filter.\")\n",
    "        cohort_df_final = df_min_visits_filtered.copy() # No filtering done\n",
    "        final_subjects = cohort_df_final['Subject ID'].nunique() if 'Subject ID' in cohort_df_final.columns else 0\n",
    "        final_visits = len(cohort_df_final)\n",
    "        print(f\"Proceeding with {final_visits} visits from {final_subjects} subjects without MRI filter.\")\n",
    "\n",
    "\n",
    "    if final_visits == 0:\n",
    "        print(\"CRITICAL ERROR: No visits remaining after applying all filters. \"\n",
    "              \"Check data, verification CSV, and cohort definition criteria.\")\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "else:\n",
    "    print(\"Skipping MRI verification filter as preceding DataFrame ('df_min_visits_filtered') is empty or not defined, \"\n",
    "          \"or 'verified_mri_ids' not available.\")\n",
    "    # Ensure cohort_df_final is an empty DataFrame if pipeline were to continue\n",
    "    cohort_df_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aeb46b",
   "metadata": {},
   "source": [
    "## 8. Final Cohort Summary and Saving\n",
    "\n",
    "Summarize the characteristics of the `cohort_df_final` after all inclusion/exclusion criteria have been applied. This final cohort DataFrame is then saved locally to a CSV file (e.g., `final_analysis_cohort_oasis2.csv`) in this notebook's designated output directory. This CSV file is also logged as a versioned data artifact to Weights & Biases, making it easily accessible for downstream notebooks (e.g., Notebook 03 for feature engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc863dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summarize, Save Final Cohort, and Log as W&B Artifact ---\n",
    "print(\"\\n--- Final Cohort Defined & Saving ---\")\n",
    "\n",
    "# Ensure cohort_df_final and other necessary variables are defined\n",
    "if 'cohort_df_final' in locals() and not cohort_df_final.empty and \\\n",
    "   'final_subjects' in locals() and 'final_visits' in locals() and \\\n",
    "   'baseline_cdr_criteria' in locals() and 'min_visits_required' in locals():\n",
    "\n",
    "    print(f\"Final Cohort Summary for {DATASET_IDENTIFIER.upper()}:\")\n",
    "    print(f\"  Total Unique Subjects: {final_subjects}\")\n",
    "    print(f\"  Total Visits (Scan Sessions): {final_visits}\")\n",
    "    print(f\"  Applied Baseline CDR criteria: {baseline_cdr_criteria}\")\n",
    "    print(f\"  Applied Minimum Visits criteria: >= {min_visits_required}\")\n",
    "    print(f\"  Applied MRI Verified criteria: Yes (visits kept only if MRI was verified in NB01)\")\n",
    "\n",
    "    # Define path for saving the final cohort DataFrame\n",
    "    final_cohort_filename = f\"final_analysis_cohort_{DATASET_IDENTIFIER}.csv\"\n",
    "    final_cohort_path = output_dir / final_cohort_filename # output_dir defined in Cell 3\n",
    "    \n",
    "    try:\n",
    "        cohort_df_final.to_csv(final_cohort_path, index=False)\n",
    "        print(f\"\\nFinal cohort DataFrame saved locally to: {final_cohort_path}\")\n",
    "\n",
    "        # Log final cohort DataFrame as a W&B artifact\n",
    "        if run:\n",
    "            print(\"Logging final cohort DataFrame as W&B artifact...\")\n",
    "            # Create a descriptive artifact name including key criteria\n",
    "            artifact_name_cohort = (f\"analysis-cohort-{DATASET_IDENTIFIER}\"\n",
    "                                    f\"-BCDR_{'_'.join(map(str, baseline_cdr_criteria)).replace('.', 'p')}\"\n",
    "                                    f\"-MinV_{min_visits_required}\")\n",
    "            \n",
    "            cohort_artifact_description = (\n",
    "                f\"Final analysis cohort for {DATASET_IDENTIFIER} after all filtering criteria. \"\n",
    "                f\"Baseline CDR in {baseline_cdr_criteria}, Minimum Visits >= {min_visits_required}, MRI verified.\"\n",
    "            )\n",
    "            cohort_artifact_metadata = {\n",
    "                'num_subjects': final_subjects, \n",
    "                'num_visits': final_visits,\n",
    "                'baseline_cdr_criteria_applied': baseline_cdr_criteria, \n",
    "                'min_visits_required_applied': min_visits_required,\n",
    "                'mri_verified_filter_applied': True,\n",
    "                'source_notebook': f\"{NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER}\",\n",
    "                'dataset_identifier': DATASET_IDENTIFIER\n",
    "            }\n",
    "            \n",
    "            final_cohort_wandb_artifact = wandb.Artifact(\n",
    "                artifact_name_cohort,\n",
    "                type=f\"processed_dataset_{DATASET_IDENTIFIER}\", # Consistent type for processed datasets\n",
    "                description=cohort_artifact_description,\n",
    "                metadata=cohort_artifact_metadata\n",
    "            )\n",
    "            final_cohort_wandb_artifact.add_file(str(final_cohort_path), name=final_cohort_filename) # Add the saved CSV\n",
    "            run.log_artifact(final_cohort_wandb_artifact, aliases=[\"latest_cohort\", f\"final_{time.strftime('%Y%m%d')}\"])\n",
    "            print(f\"Final cohort artifact '{artifact_name_cohort}' logged to W&B.\")\n",
    "\n",
    "    except Exception as e_save_final_cohort:\n",
    "        print(f\"Warning: Could not save or log final cohort DataFrame. Error: {e_save_final_cohort}\")\n",
    "else:\n",
    "    print(\"Final cohort (cohort_df_final) is empty or key variables for summary are missing. Skipping final summary and save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa60f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Prediction Task ---\n",
    "print(\"\\n--- Defining Prediction Task ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef502b64",
   "metadata": {},
   "source": [
    "### Prediction Task Definition (Phase 1 & 2)\n",
    "\n",
    "\n",
    "* **Target Variable:** Predict the **CDR score** at the next available visit (visit `k+1`).\n",
    "* **Input Features Strategy:** Use features from all available prior visits up to and including the current visit (visit `k`).\n",
    "* **Feature Types:**\n",
    "    * **Time-Varying:** Clinical scores (e.g., MMSE at visit `k`), Age (at visit `k`), time since baseline/previous visit, MRI-derived features (from scan at visit `k`).\n",
    "    * **Static (Planned):** Baseline CDR, Baseline MMSE, Sex, Education (EDUC), SES. These will be concatenated to the input at each time step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feea7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run:\n",
    "    wandb.config.update({\n",
    "        'prediction/target_variable': 'CDR_next_visit',\n",
    "        'prediction/input_strategy': 'all_prior_visits_plus_static',\n",
    "        'prediction/time_varying_features': ['Age_visit', 'MMSE_visit', 'MRI_features_visit', 'Time_interval'], # Example list\n",
    "        'prediction/static_features_planned': ['Baseline_CDR', 'Baseline_MMSE', 'Sex', 'EDUC', 'SES'] # Planned\n",
    "    })\n",
    "    print(\"Prediction task configuration logged to W&B.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce2ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Note on Next Steps: Preprocessing ---\n",
    "print(\"\\n--- Next Steps: Preprocessing ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d4b617",
   "metadata": {},
   "source": [
    "The next stage involves preprocessing the `final_analysis_cohort.csv` data to make it suitable for sequence modeling:\n",
    "\n",
    "\n",
    "1.  **Feature Engineering:** Create necessary features like 'Time since baseline/previous visit'. Extract Baseline CDR/MMSE to be used as static features.\n",
    "2.  **Sequence Creation:** Group data by subject and create sequences of visits. Define the input sequence (visits 1 to k) and target (CDR at visit k+1) for each prediction point. Handle sequences of varying lengths (padding/masking).\n",
    "3.  **Data Splitting:** Split subjects into Training, Validation, and Test sets *before* any scaling or imputation that involves learning parameters from the data. Ensure subjects from the same family (if applicable) stay in the same split.\n",
    "4.  **Clinical Feature Scaling:** Scale numerical clinical features (e.g., Age, MMSE) appropriately (e.g., StandardScaler fit on the training set).\n",
    "5.  **Missing Value Imputation (Within Sequence):** Decide on a strategy for handling missing values *within* the time-varying features of a sequence (e.g., forward fill, mean imputation based on training set, model-based imputation).\n",
    "6.  **MRI Preprocessing:** Define and implement the pipeline to process the verified T1w NIfTI files (e.g., registration, skull stripping, feature extraction using 3D CNN or ViT). This is a major separate step.\n",
    "7.  **Combine & Save Processed Data:** Integrate clinical sequences and pointers to processed MRI features, saving the final model-ready data splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad5377",
   "metadata": {},
   "source": [
    "## Finalize Run\n",
    "\n",
    "Finish the Weights & Biases run associated with this cohort definition process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Cohort Definition complete. Finishing W&B run. ---\")\n",
    "if run:\n",
    "    run.finish()\n",
    "    print(\"W&B run finished.\")\n",
    "else:\n",
    "    print(\"No active W&B run to finish.\")\n",
    "\n",
    "print(\"\\nScript execution finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro_predcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
