{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: notebooks/02_Cohort_Definition.ipynb\n",
    "# Purpose: Define the analysis cohort for longitudinal prediction based on OASIS-2 data.\n",
    "#          Applies criteria for baseline status, minimum visits, and MRI availability.\n",
    "#          Logs decisions and outputs the final cohort definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d68c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ff165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config Loading ---\n",
    "print(\"--- Loading Configuration ---\")\n",
    "CONFIG_PATH = Path('../config.json') # Path relative to the notebook location\n",
    "try:\n",
    "    PROJECT_ROOT = CONFIG_PATH.parent.resolve()\n",
    "    print(f\"Project Root detected as: {PROJECT_ROOT}\")\n",
    "\n",
    "    with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    print(\"Configuration loaded successfully.\")\n",
    "\n",
    "    # Define key variables from config\n",
    "    INPUT_DATA_PATH = PROJECT_ROOT / config['data']['clinical_excel_path']\n",
    "    OUTPUT_DIR_BASE = PROJECT_ROOT / config['data']['output_dir_base']\n",
    "    WANDB_PROJECT = config['wandb']['project_name']\n",
    "    WANDB_ENTITY = config['wandb'].get('entity', None)\n",
    "\n",
    "    # Define specific output dir for this notebook and create it\n",
    "    NOTEBOOK_NAME = \"02_Cohort_Definition\"\n",
    "    output_dir = OUTPUT_DIR_BASE / NOTEBOOK_NAME\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Outputs will be saved to: {output_dir}\")\n",
    "\n",
    "    # Define path to verification results from Notebook 01\n",
    "    NB01_OUTPUT_DIR = OUTPUT_DIR_BASE / \"01_Data_Exploration\"\n",
    "    VERIFICATION_CSV_PATH = NB01_OUTPUT_DIR / \"verification_details.csv\" # ASSUMPTION: NB01 saved this\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Configuration file not found at {CONFIG_PATH}\")\n",
    "    exit()\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Missing key {e} in configuration file.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading the config file: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ade4d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def finalize_plot(fig, run, wandb_key, save_path):\n",
    "    \"\"\"Handles logging plot to W&B, saving locally, showing, and closing.\"\"\"\n",
    "    if run:\n",
    "        run.log({wandb_key: wandb.Image(fig)})\n",
    "    if save_path:\n",
    "        try:\n",
    "            fig.savefig(save_path, bbox_inches='tight')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save plot to {save_path}. Error: {e}\")\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b120460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize W&B Run ---\n",
    "print(\"\\n--- Initializing Weights & Biases Run ---\")\n",
    "run = None # Initialize run to None\n",
    "try:\n",
    "    run = wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        entity=WANDB_ENTITY,\n",
    "        job_type=\"cohort-definition\",\n",
    "        name=f\"{NOTEBOOK_NAME}-run-{time.strftime('%Y%m%d-%H%M')}\",\n",
    "        config={ # Log key config choices for this job\n",
    "            \"source_data_path\": str(INPUT_DATA_PATH),\n",
    "            \"verification_data_path\": str(VERIFICATION_CSV_PATH)\n",
    "            # Cohort criteria will be added via wandb.config.update()\n",
    "        }\n",
    "    )\n",
    "    print(f\"W&B run '{run.name}' initialized successfully. View at: {run.url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing W&B: {e}\")\n",
    "    print(\"Proceeding without W&B logging.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad55ecec",
   "metadata": {},
   "source": [
    "## Load Clinical Data\n",
    "\n",
    "Load the raw longitudinal clinical and demographic data from the specified Excel file using pandas. We also initialize Weights & Biases here to track this exploration run and log the source data as an artifact for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b795504",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- Loading Raw Clinical Data from: {INPUT_DATA_PATH} ---\")\n",
    "try:\n",
    "    if not INPUT_DATA_PATH.is_file():\n",
    "         raise FileNotFoundError(f\"Input data file not found at {INPUT_DATA_PATH}\")\n",
    "    clinical_df_raw = pd.read_excel(INPUT_DATA_PATH)\n",
    "    print(f\"Raw clinical data loaded successfully. Shape: {clinical_df_raw.shape}\")\n",
    "    if run: run.log({'cohort_definition/00_raw_rows': len(clinical_df_raw)})\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading clinical data: {e}\")\n",
    "    if run: run.finish()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd7af04",
   "metadata": {},
   "source": [
    "## Load MRI Verification Results\n",
    "\n",
    "Load the detailed verification results (`verification_details.csv`) saved by Notebook 01. This file contains information on which `MRI ID`s correspond to successfully located raw scan files (`.img` + `.hdr` pairs) on the local disk. This is needed to ensure our final cohort only includes visits with available imaging data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- Loading MRI Verification Results from: {VERIFICATION_CSV_PATH} ---\")\n",
    "try:\n",
    "    if not VERIFICATION_CSV_PATH.is_file():\n",
    "         raise FileNotFoundError(f\"Verification results file not found at {VERIFICATION_CSV_PATH}. Please ensure Notebook 01 saved it.\")\n",
    "    verification_df = pd.read_csv(VERIFICATION_CSV_PATH)\n",
    "    # --- Get the set of MRI IDs that passed verification ---\n",
    "    # Adjust criteria if needed, e.g., check mprs_found_count > 0\n",
    "    verified_mri_ids = set(verification_df[verification_df['mri_folder_exists'] == True]['mri_id'].unique())\n",
    "    print(f\"Loaded verification results. Found {len(verified_mri_ids)} unique MRI IDs with existing folders.\")\n",
    "    if run: run.log({'cohort_definition/00_verified_mri_ids': len(verified_mri_ids)})\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Cannot proceed without verification results to filter the cohort.\")\n",
    "    if run: run.finish()\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading the verification results: {e}\")\n",
    "    if run: run.finish()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d2442c",
   "metadata": {},
   "source": [
    "## Cohort Definition Step 1: Filter by Baseline CDR\n",
    "\n",
    "Apply the first inclusion criterion based on the subject's cognitive status at their first available visit. We include subjects whose baseline CDR score was 0.0 (Cognitively Normal) or 0.5 (Mild Cognitive Impairment). Log the number of subjects and visits remaining after this filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd04bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Applying Baseline CDR Filter (Keeping CDR=0 and CDR=0.5) ---\")\n",
    "baseline_cdr_criteria = [0.0, 0.5]\n",
    "\n",
    "# Find first visit data for each subject\n",
    "# Ensure data is sorted by visit to correctly identify the first\n",
    "clinical_df_raw_sorted = clinical_df_raw.sort_values(by=['Subject ID', 'Visit'])\n",
    "first_visit_data = clinical_df_raw_sorted.loc[clinical_df_raw_sorted.groupby('Subject ID')['Visit'].idxmin()]\n",
    "\n",
    "# Identify subjects meeting baseline criteria\n",
    "subjects_meeting_baseline_cdr = first_visit_data[first_visit_data['CDR'].isin(baseline_cdr_criteria)]['Subject ID'].unique()\n",
    "num_subjects_baseline_criteria = len(subjects_meeting_baseline_cdr)\n",
    "print(f\"Found {num_subjects_baseline_criteria} unique subjects with baseline CDR in {baseline_cdr_criteria}.\")\n",
    "\n",
    "# Filter the main dataframe to keep only visits from these subjects\n",
    "df_baseline_filtered = clinical_df_raw[clinical_df_raw['Subject ID'].isin(subjects_meeting_baseline_cdr)].copy()\n",
    "print(f\"DataFrame shape after baseline CDR filter: {df_baseline_filtered.shape}\")\n",
    "\n",
    "if run:\n",
    "    wandb.config.update({'cohort_criteria/baseline_cdr_included': baseline_cdr_criteria})\n",
    "    run.log({\n",
    "        'cohort_definition/01_subjects_after_baseline_cdr_filter': num_subjects_baseline_criteria,\n",
    "        'cohort_definition/01_visits_after_baseline_cdr_filter': len(df_baseline_filtered)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c979542a",
   "metadata": {},
   "source": [
    "## Cohort Definition Step 2: Check and Apply Minimum Visits Filter\n",
    "\n",
    "Analyze the distribution of the total number of visits for the subjects selected in Step 1. Based on this distribution (aiming to balance longitudinal information content with cohort size), make a data-driven decision for the minimum number of visits required per subject (e.g., >=2 or >=3). Apply this filter and log the chosen criterion and resulting cohort size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd876d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Checking and Applying Minimum Visits Filter ---\")\n",
    "\n",
    "if df_baseline_filtered.empty:\n",
    "    print(\"No subjects remaining after baseline filter. Stopping.\")\n",
    "    if run: run.finish()\n",
    "    exit()\n",
    "\n",
    "# Count visits per subject *within the baseline-filtered group*\n",
    "visits_per_subject_filtered = df_baseline_filtered.groupby('Subject ID')['Visit'].count()\n",
    "\n",
    "# --- Analyze visit counts before setting threshold ---\n",
    "print(\"\\nDistribution of visit counts (for subjects meeting baseline criteria):\")\n",
    "visit_counts_dist = visits_per_subject_filtered.value_counts().sort_index()\n",
    "print(visit_counts_dist)\n",
    "\n",
    "total_subjects_step1 = num_subjects_baseline_criteria\n",
    "count_ge_2 = sum(visits_per_subject_filtered >= 2)\n",
    "count_ge_3 = sum(visits_per_subject_filtered >= 3)\n",
    "count_ge_4 = sum(visits_per_subject_filtered >= 4)\n",
    "\n",
    "percent_ge_2 = count_ge_2 / total_subjects_step1 if total_subjects_step1 > 0 else 0\n",
    "percent_ge_3 = count_ge_3 / total_subjects_step1 if total_subjects_step1 > 0 else 0\n",
    "percent_ge_4 = count_ge_4 / total_subjects_step1 if total_subjects_step1 > 0 else 0\n",
    "\n",
    "print(f\"\\nSubjects meeting baseline criteria: {total_subjects_step1}\")\n",
    "print(f\"Number with >= 2 visits: {count_ge_2} ({percent_ge_2:.1%})\")\n",
    "print(f\"Number with >= 3 visits: {count_ge_3} ({percent_ge_3:.1%})\")\n",
    "print(f\"Number with >= 4 visits: {count_ge_4} ({percent_ge_4:.1%})\")\n",
    "\n",
    "# Log cohort check stats\n",
    "if run:\n",
    "    run.log({\n",
    "        'cohort_check/total_baseline_criteria_subjects': total_subjects_step1,\n",
    "        'cohort_check/subjects_ge_2_visits': count_ge_2,\n",
    "        'cohort_check/subjects_ge_3_visits': count_ge_3,\n",
    "        'cohort_check/subjects_ge_4_visits': count_ge_4,\n",
    "        'cohort_check/percent_ge_2_visits': percent_ge_2,\n",
    "        'cohort_check/percent_ge_3_visits': percent_ge_3,\n",
    "        'cohort_check/percent_ge_4_visits': percent_ge_4\n",
    "    })\n",
    "    # Log distribution table\n",
    "    try:\n",
    "        visit_counts_table = wandb.Table(dataframe=visit_counts_dist.reset_index().rename(columns={'index': 'num_visits', 'Visit': 'subject_count'}))\n",
    "        run.log({\"cohort_check/visit_count_distribution\": visit_counts_table})\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not log visit count table to W&B. Error: {e}\")\n",
    "\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.countplot(x=visits_per_subject_filtered, ax=ax, color='skyblue') # Use countplot directly on the Series\n",
    "ax.set_title(f'Number of Visits per Subject (Baseline CDR in {baseline_cdr_criteria})')\n",
    "ax.set_xlabel('Number of Visits')\n",
    "ax.set_ylabel('Number of Subjects')\n",
    "finalize_plot(fig, run, \"charts/cohort_check/visits_per_subject\", output_dir / 'cohort_visit_counts.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ac4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Make data-driven decision on min_visits_required ---\n",
    "# Example logic: Require >=3 if at least 40% have it, otherwise require >=2 if cohort size is decent\n",
    "if percent_ge_3 >= 0.40:\n",
    "    min_visits_required = 3\n",
    "    print(f\"\\nDecision: >= 40% ({percent_ge_3:.1%}) of subjects have 3+ visits. Setting min_visits_required = 3.\")\n",
    "elif count_ge_2 > 50 : # Ensure at least a reasonable number of subjects have >= 2 visits\n",
    "    min_visits_required = 2\n",
    "    print(f\"\\nDecision: Less than 40% ({percent_ge_3:.1%}) of subjects have 3+ visits.\")\n",
    "    print(f\"Relaxing criterion to min_visits_required = 2 ({percent_ge_2:.1%} have >=2 visits).\")\n",
    "else:\n",
    "    min_visits_required = 3 # Default to 3 if cohort is very small anyway or percentages are odd\n",
    "    print(f\"\\nWarning: Low number of subjects with multiple visits ({percent_ge_2:.1%} have >=2). Defaulting to min_visits_required = 3.\")\n",
    "    print(\"Consider re-evaluating baseline criteria if cohort size is too small.\")\n",
    "\n",
    "# Log final decision\n",
    "if run:\n",
    "    wandb.config.update({'cohort_criteria/min_visits_required': min_visits_required})\n",
    "\n",
    "# Identify subjects meeting the final minimum visit count\n",
    "subjects_with_enough_visits = visits_per_subject_filtered[visits_per_subject_filtered >= min_visits_required].index.unique()\n",
    "num_subjects_min_visits = len(subjects_with_enough_visits)\n",
    "print(f\"\\nFound {num_subjects_min_visits} unique subjects meeting baseline CDR and >= {min_visits_required} visits criteria.\")\n",
    "\n",
    "# Filter the DataFrame further\n",
    "df_min_visits_filtered = df_baseline_filtered[df_baseline_filtered['Subject ID'].isin(subjects_with_enough_visits)].copy()\n",
    "print(f\"DataFrame shape after min visits filter: {df_min_visits_filtered.shape}\")\n",
    "\n",
    "if run:\n",
    "    run.log({\n",
    "        'cohort_definition/02_subjects_after_min_visits_filter': num_subjects_min_visits,\n",
    "        'cohort_definition/02_visits_after_min_visits_filter': len(df_min_visits_filtered)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aa0399",
   "metadata": {},
   "source": [
    "## Cohort Definition Step 3: Filter by MRI Availability\n",
    "\n",
    "Apply the final inclusion criterion by ensuring that only visits with verified corresponding MRI scan files (based on the results loaded from Notebook 01) are retained in the cohort. Log the number of visits removed at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c9e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Applying MRI Verification Filter ---\")\n",
    "\n",
    "if df_min_visits_filtered.empty:\n",
    "    print(\"No subjects remaining after minimum visits filter. Stopping.\")\n",
    "    if run: run.finish()\n",
    "    exit()\n",
    "\n",
    "initial_visits_step3 = len(df_min_visits_filtered)\n",
    "cohort_df_final = df_min_visits_filtered[df_min_visits_filtered['MRI ID'].isin(verified_mri_ids)].copy()\n",
    "final_subjects = cohort_df_final['Subject ID'].nunique()\n",
    "final_visits = len(cohort_df_final)\n",
    "visits_removed_mri = initial_visits_step3 - final_visits\n",
    "\n",
    "print(f\"Removed {visits_removed_mri} visits due to missing/unverified MRI scans.\")\n",
    "print(f\"Final cohort for modeling: {final_visits} visits from {final_subjects} subjects.\")\n",
    "\n",
    "if run:\n",
    "    run.log({\n",
    "        'cohort_definition/03_visits_before_mri_filter': initial_visits_step3,\n",
    "        'cohort_definition/03_visits_removed_for_mri': visits_removed_mri,\n",
    "        'cohort_definition/03_final_visits': final_visits,\n",
    "        'cohort_definition/03_final_subjects': final_subjects\n",
    "    })\n",
    "\n",
    "if final_visits == 0:\n",
    "    print(\"Error: No visits remaining after applying all filters. Check data and criteria.\")\n",
    "    if run: run.finish()\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aeb46b",
   "metadata": {},
   "source": [
    "## Final Cohort Summary and Saving\n",
    "\n",
    "Print a summary of the final cohort characteristics (number of subjects, visits) after applying all filters. Save the resulting cohort DataFrame (`cohort_df_final`) locally as `final_analysis_cohort.csv` (in this notebook's output directory). Log this final cohort DataFrame as a versioned artifact to Weights & Biases for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc863dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Final Cohort Defined ---\")\n",
    "print(f\"Total Subjects: {final_subjects}\")\n",
    "print(f\"Total Visits (Scan Sessions): {final_visits}\")\n",
    "print(f\"Baseline CDR criteria: {baseline_cdr_criteria}\")\n",
    "print(f\"Minimum Visits criteria: >= {min_visits_required}\")\n",
    "print(\"MRI Verified criteria: Corresponding MRI folder and img/hdr pair found.\")\n",
    "\n",
    "# Save the final cohort DataFrame\n",
    "final_cohort_path = output_dir / \"final_analysis_cohort.csv\"\n",
    "try:\n",
    "    cohort_df_final.to_csv(final_cohort_path, index=False)\n",
    "    print(f\"Final cohort DataFrame saved locally to: {final_cohort_path}\")\n",
    "\n",
    "    # Log final cohort as W&B artifact\n",
    "    if run:\n",
    "        print(\"Logging final cohort DataFrame as W&B artifact...\")\n",
    "        cohort_artifact = wandb.Artifact(f\"analysis_cohort-OASIS2-CDR_{'_'.join(map(str, baseline_cdr_criteria))}-MinV_{min_visits_required}\",\n",
    "                                         type=\"analysis-dataset\",\n",
    "                                         description=f\"Final cohort data after inclusion/exclusion and MRI verification. Baseline CDR={baseline_cdr_criteria}, MinVisits>={min_visits_required}.\",\n",
    "                                         metadata={'num_subjects': final_subjects, 'num_visits': final_visits,\n",
    "                                                   'baseline_cdr_criteria': baseline_cdr_criteria, 'min_visits_required': min_visits_required})\n",
    "        cohort_artifact.add_file(str(final_cohort_path))\n",
    "        run.log_artifact(cohort_artifact)\n",
    "        print(\"Final cohort artifact logged.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not save or log final cohort DataFrame. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa60f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Prediction Task ---\n",
    "print(\"\\n--- Defining Prediction Task ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef502b64",
   "metadata": {},
   "source": [
    "### Prediction Task Definition (Phase 1 & 2)\n",
    "\n",
    "\n",
    "* **Target Variable:** Predict the **CDR score** at the next available visit (visit `k+1`).\n",
    "* **Input Features Strategy:** Use features from all available prior visits up to and including the current visit (visit `k`).\n",
    "* **Feature Types:**\n",
    "    * **Time-Varying:** Clinical scores (e.g., MMSE at visit `k`), Age (at visit `k`), time since baseline/previous visit, MRI-derived features (from scan at visit `k`).\n",
    "    * **Static (Planned):** Baseline CDR, Baseline MMSE, Sex, Education (EDUC), SES. These will be concatenated to the input at each time step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feea7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run:\n",
    "    wandb.config.update({\n",
    "        'prediction/target_variable': 'CDR_next_visit',\n",
    "        'prediction/input_strategy': 'all_prior_visits_plus_static',\n",
    "        'prediction/time_varying_features': ['Age_visit', 'MMSE_visit', 'MRI_features_visit', 'Time_interval'], # Example list\n",
    "        'prediction/static_features_planned': ['Baseline_CDR', 'Baseline_MMSE', 'Sex', 'EDUC', 'SES'] # Planned\n",
    "    })\n",
    "    print(\"Prediction task configuration logged to W&B.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce2ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Note on Next Steps: Preprocessing ---\n",
    "print(\"\\n--- Next Steps: Preprocessing ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d4b617",
   "metadata": {},
   "source": [
    "The next stage involves preprocessing the `final_analysis_cohort.csv` data to make it suitable for sequence modeling:\n",
    "\n",
    "\n",
    "1.  **Feature Engineering:** Create necessary features like 'Time since baseline/previous visit'. Extract Baseline CDR/MMSE to be used as static features.\n",
    "2.  **Sequence Creation:** Group data by subject and create sequences of visits. Define the input sequence (visits 1 to k) and target (CDR at visit k+1) for each prediction point. Handle sequences of varying lengths (padding/masking).\n",
    "3.  **Data Splitting:** Split subjects into Training, Validation, and Test sets *before* any scaling or imputation that involves learning parameters from the data. Ensure subjects from the same family (if applicable) stay in the same split.\n",
    "4.  **Clinical Feature Scaling:** Scale numerical clinical features (e.g., Age, MMSE) appropriately (e.g., StandardScaler fit on the training set).\n",
    "5.  **Missing Value Imputation (Within Sequence):** Decide on a strategy for handling missing values *within* the time-varying features of a sequence (e.g., forward fill, mean imputation based on training set, model-based imputation).\n",
    "6.  **MRI Preprocessing:** Define and implement the pipeline to process the verified T1w NIfTI files (e.g., registration, skull stripping, feature extraction using 3D CNN or ViT). This is a major separate step.\n",
    "7.  **Combine & Save Processed Data:** Integrate clinical sequences and pointers to processed MRI features, saving the final model-ready data splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad5377",
   "metadata": {},
   "source": [
    "## Finalize Run\n",
    "\n",
    "Finish the Weights & Biases run associated with this cohort definition process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Cohort Definition complete. Finishing W&B run. ---\")\n",
    "if run:\n",
    "    run.finish()\n",
    "    print(\"W&B run finished.\")\n",
    "else:\n",
    "    print(\"No active W&B run to finish.\")\n",
    "\n",
    "print(\"\\nScript execution finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro_pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
