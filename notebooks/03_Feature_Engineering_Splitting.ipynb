{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227f981e",
   "metadata": {},
   "source": [
    "# Notebook 03: OASIS-2 Feature Engineering & Data Splitting\n",
    "\n",
    "**Project Phase:** 1 (Data Processing - Feature Engineering & Splitting)\n",
    "**Dataset:** OASIS-2 Longitudinal MRI & Clinical Data\n",
    "\n",
    "**Purpose:**\n",
    "This notebook takes the defined analysis cohort (output from Notebook 02) and performs critical feature engineering and data splitting steps to prepare for model training. The objectives are:\n",
    "1.  Load the `final_analysis_cohort.csv` data.\n",
    "2.  Engineer time-based features essential for longitudinal modeling, such as `Days_from_Baseline` (relative to the first visit *within the current cohort*) and `Time_since_Last_Visit_Days`.\n",
    "3.  Extract baseline clinical scores (e.g., `Baseline_CDR`, `Baseline_MMSE`) for each subject to be used as static input features.\n",
    "4.  Select the final set of columns (identifiers, time-varying features, static features, and the base target variable 'CDR') for the modeling dataset.\n",
    "5.  Create the primary target variable: `CDR_next_visit` (the CDR score at the subsequent visit). Rows corresponding to a subject's last visit (which have no next CDR) are dropped.\n",
    "6.  Perform a subject-level stratified train/validation/test split. Stratification is based on `Baseline_CDR` to ensure balanced representation of baseline cognitive status across splits.\n",
    "7.  Save the resulting `train_df`, `val_df`, and `test_df` DataFrames locally as Parquet files.\n",
    "8.  Log these data splits as versioned artifacts to Weights & Biases (W&B) for use in subsequent modeling notebooks.\n",
    "\n",
    "**Workflow:**\n",
    "1.  **Setup:** Import libraries, configure `sys.path`, load `config.json`, define input/output paths.\n",
    "2.  **W&B Initialization:** Start a new W&B run using `initialize_wandb_run`.\n",
    "3.  **Load Cohort Data:** Load `final_analysis_cohort.csv` (from NB02).\n",
    "4.  **Engineer Time Features:** Calculate `Days_from_Baseline` and `Time_since_Last_Visit_Days`.\n",
    "5.  **Prepare Static Features & Select Columns:** Extract baseline scores, define and filter feature lists.\n",
    "6.  **Create Target Variable:** Generate `CDR_next_visit`.\n",
    "7.  **Data Splitting:** Perform subject-level stratified train/validation/test split. Log split statistics.\n",
    "8.  **Save Splits & Log Artifacts:** Save DataFrames locally and log to W&B.\n",
    "9.  **Finalize W&B Run.**\n",
    "\n",
    "**Input:**\n",
    "* `config.json`: Main project configuration file.\n",
    "* `final_analysis_cohort.csv`: Output from Notebook 02 (path constructed using `config.json`).\n",
    "\n",
    "**Output:**\n",
    "* **Local Files (in notebook-specific output directory, e.g., `notebooks/outputs/03_Feature_Engineering_Splitting_OASIS2/`):**\n",
    "    * `cohort_train.parquet`\n",
    "    * `cohort_validation.parquet`\n",
    "    * `cohort_test.parquet`\n",
    "* **W&B Run:**\n",
    "    * Logged run configuration (including input paths, split ratios).\n",
    "    * Statistics on feature engineering and data splitting.\n",
    "    * Data splits (`train`, `validation`, `test`) logged as W&B Artifacts (e.g., `cohort-split-train-oasis2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "871fd443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: notebooks/03_Feature_Engineering_Splitting.ipynb\n",
    "# Purpose: Load the defined cohort, engineer time-based features,\n",
    "#          select features (incl. pre-computed MRI metrics) for baseline modeling,\n",
    "#          perform subject-level stratified split, and save the split datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc8821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import sys \n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8106d",
   "metadata": {},
   "source": [
    "## 1. Setup: Project Configuration, Paths, and Utilities\n",
    "\n",
    "This section initializes the notebook environment:\n",
    "* Determines the project's root directory and adds the `src` directory to the Python system path.\n",
    "* Imports necessary custom utility functions (primarily for W&B run initialization).\n",
    "* Loads the main project configuration from `config.json`.\n",
    "* Defines dataset identifiers and notebook-specific parameters.\n",
    "* Resolves the input path for the cohort data (from Notebook 02) and sets up the output directory for this notebook's generated data splits, using `config.json` for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "782260db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Project Setup, Configuration Loading, and Utility Imports ---\n",
    "print(\"--- Initializing Project Setup & Configuration for NB03 ---\")\n",
    "\n",
    "# Initialize\n",
    "PROJECT_ROOT = None\n",
    "base_config = {}\n",
    "\n",
    "try:\n",
    "    # Determine project root \n",
    "    current_notebook_path = Path.cwd() \n",
    "    potential_project_root = current_notebook_path.parent \n",
    "    if (potential_project_root / \"src\").is_dir() and (potential_project_root / \"config.json\").is_file():\n",
    "        PROJECT_ROOT = potential_project_root\n",
    "    else: \n",
    "        PROJECT_ROOT = current_notebook_path\n",
    "    \n",
    "    if not (PROJECT_ROOT / \"src\").is_dir() or not (PROJECT_ROOT / \"config.json\").is_file():\n",
    "        raise FileNotFoundError(f\"Could not reliably find 'src' or 'config.json'. PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "\n",
    "    if str(PROJECT_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "    print(f\"Added '{str(PROJECT_ROOT)}' to sys.path.\")\n",
    "\n",
    "    # Import custom utilities\n",
    "    from src.wandb_utils import initialize_wandb_run \n",
    "    print(\"Successfully imported 'initialize_wandb_run' from src.wandb_utils.\")\n",
    "\n",
    "except FileNotFoundError as e_path:\n",
    "    print(f\"CRITICAL ERROR in project setup (paths or src): {e_path}\")\n",
    "    # exit() \n",
    "except ImportError as e_imp:\n",
    "    print(f\"CRITICAL ERROR: Could not import custom utilities: {e_imp}\")\n",
    "    # exit()\n",
    "except Exception as e_general_setup:\n",
    "    print(f\"CRITICAL ERROR during initial setup: {e_general_setup}\")\n",
    "    # exit()\n",
    "\n",
    "# --- Load Main Project Configuration ---\n",
    "print(\"\\n--- Loading Main Project Configuration from config.json ---\")\n",
    "try:\n",
    "    if PROJECT_ROOT is None:\n",
    "        raise ValueError(\"PROJECT_ROOT was not successfully defined.\")\n",
    "    CONFIG_PATH_MAIN = PROJECT_ROOT / 'config.json'\n",
    "    with open(CONFIG_PATH_MAIN, 'r', encoding='utf-8') as f:\n",
    "        base_config = json.load(f)\n",
    "    print(f\"Main project config loaded from: {CONFIG_PATH_MAIN}\")\n",
    "except Exception as e_cfg:\n",
    "    print(f\"CRITICAL ERROR loading main config.json: {e_cfg}\")\n",
    "    # exit() \n",
    "\n",
    "# --- Define Dataset, Notebook Specifics, and Key Paths ---\n",
    "DATASET_IDENTIFIER = \"oasis2\" \n",
    "NOTEBOOK_MODULE_NAME = \"03_Feature_Engineering_Splitting\"\n",
    "# Key for this notebook's output subdir in config.json's locators\n",
    "NB03_OUTPUT_LOCATOR_KEY = \"feature_eng_subdir\"\n",
    "\n",
    "COHORT_CSV_PATH_NB03_INPUT = None # Input from NB02\n",
    "output_dir = None                 # Static output dir for this notebook's Parquet files\n",
    "\n",
    "try:\n",
    "    if not base_config:\n",
    "        raise ValueError(\"base_config is empty. Cannot define paths.\")\n",
    "\n",
    "    output_dir_base_from_config = PROJECT_ROOT / base_config['data']['output_dir_base']\n",
    "    dataset_locators = base_config.get(f\"pipeline_artefact_locators_{DATASET_IDENTIFIER}\", {})\n",
    "    if not dataset_locators:\n",
    "        raise KeyError(f\"pipeline_artefact_locators_{DATASET_IDENTIFIER} section not found in config.json.\")\n",
    "\n",
    "    # Path to final_analysis_cohort.csv (output from Notebook 02)\n",
    "    nb02_output_subdir_name = dataset_locators.get(\"cohort_def_subdir\")\n",
    "    nb02_final_cohort_fname = dataset_locators.get(\"final_cohort_fname\")\n",
    "    if not nb02_output_subdir_name or not nb02_final_cohort_fname:\n",
    "        raise KeyError(\"Missing 'cohort_def_subdir' or 'final_cohort_fname' in locators config.\")\n",
    "    COHORT_CSV_PATH_NB03_INPUT = output_dir_base_from_config / nb02_output_subdir_name / nb02_final_cohort_fname\n",
    "\n",
    "    # Define the main OUTPUT directory for THIS notebook's files (e.g., cohort_train.parquet)\n",
    "    notebook_output_folder_name = dataset_locators.get(\n",
    "        NB03_OUTPUT_LOCATOR_KEY, # e.g., \"feature_eng_subdir_nb03\"\n",
    "        f\"{NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER}_default_outputs\" # Fallback\n",
    "    )\n",
    "    output_dir = output_dir_base_from_config / notebook_output_folder_name\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nKey paths for Notebook 03 ({DATASET_IDENTIFIER}):\")\n",
    "    print(f\"  Input Cohort CSV (from NB02): {COHORT_CSV_PATH_NB03_INPUT}\")\n",
    "    print(f\"  Notebook Output Directory (for data splits): {output_dir}\")\n",
    "    \n",
    "    if not COHORT_CSV_PATH_NB03_INPUT.is_file():\n",
    "        raise FileNotFoundError(f\"CRITICAL: Input cohort CSV from NB02 not found: {COHORT_CSV_PATH_NB03_INPUT}. \"\n",
    "                                \"Ensure Notebook 02 ran successfully and config.json locators are correct.\")\n",
    "    print(\"All critical input paths for NB03 verified.\")\n",
    "\n",
    "except KeyError as e_key:\n",
    "    print(f\"CRITICAL ERROR: Missing key {e_key} in config.json or locators section.\")\n",
    "    # exit()\n",
    "except FileNotFoundError as e_fnf:\n",
    "    print(f\"CRITICAL ERROR: {e_fnf}\")\n",
    "    # exit()\n",
    "except Exception as e_paths_nb03:\n",
    "    print(f\"CRITICAL ERROR defining paths for NB03: {e_paths_nb03}\")\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef3e57",
   "metadata": {},
   "source": [
    "## 2. Initialize Weights & Biases Run\n",
    "\n",
    "A new W&B run is started for this Feature Engineering and Data Splitting notebook. The run will log:\n",
    "* The configuration parameters used (input data path, time feature strategy, split ratios).\n",
    "* Summary statistics about the engineered features and data splits.\n",
    "* The final train, validation, and test data splits as versioned W&B Artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a326ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize W&B Run for THIS Notebook Execution (NB03) ---\n",
    "print(\"\\n--- Initializing Weights & Biases Run for Notebook 03 ---\")\n",
    "\n",
    "# Define constants for splitting to be logged to W&B config\n",
    "TEST_SET_RATIO_CONFIG = 0.15\n",
    "VAL_SET_RATIO_CONFIG = 0.15\n",
    "RANDOM_STATE_CONFIG = 42\n",
    "\n",
    "nb03_run_config_log = {\n",
    "    \"notebook_name_code\": f\"{NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER}\",\n",
    "    \"dataset_source\": DATASET_IDENTIFIER,\n",
    "    \"input_cohort_data_path\": str(COHORT_CSV_PATH_NB03_INPUT),\n",
    "    \"output_dir_for_local_saves\": str(output_dir),\n",
    "    \"time_feature_source_planned\": \"'MR Delay' preferred, fallback 'Age'\", # Documenting strategy\n",
    "    \"split_test_ratio_target\": TEST_SET_RATIO_CONFIG, # Log planned ratios\n",
    "    \"split_validation_ratio_target\": VAL_SET_RATIO_CONFIG,\n",
    "    \"split_random_state\": RANDOM_STATE_CONFIG,\n",
    "    \"execution_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Actual feature lists used will be logged after they are defined.\n",
    "}\n",
    "\n",
    "nb_number_prefix_nb03 = NOTEBOOK_MODULE_NAME.split('_')[0] if '_' in NOTEBOOK_MODULE_NAME else \"NB\"\n",
    "job_specific_type_nb03 = f\"{nb_number_prefix_nb03}-FeatEngSplit-{DATASET_IDENTIFIER}\"\n",
    "custom_elements_for_name_nb03 = [nb_number_prefix_nb03, DATASET_IDENTIFIER.upper(), \"FeatEngSplit\"]\n",
    "\n",
    "run = initialize_wandb_run(\n",
    "    base_project_config=base_config,\n",
    "    job_group=\"DataProcessing\",\n",
    "    job_specific_type=job_specific_type_nb03,\n",
    "    run_specific_config=nb03_run_config_log,\n",
    "    custom_run_name_elements=custom_elements_for_name_nb03,\n",
    "    notes=f\"{DATASET_IDENTIFIER.upper()}: Feature engineering and train/validation/test data splitting.\"\n",
    ")\n",
    "\n",
    "if run:\n",
    "    print(f\"W&B run '{run.name}' (Job Type: '{run.job_type}') initialized. View at: {run.url}\")\n",
    "else:\n",
    "    print(\"Proceeding without W&B logging for this session (W&B run initialization failed).\")\n",
    "    # output_dir should still be defined for local saves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa9147",
   "metadata": {},
   "source": [
    "## 3. Load Defined Cohort Data\n",
    "\n",
    "Load the `final_analysis_cohort.csv` dataset which was generated and saved by Notebook 02. This dataset contains the subjects and their longitudinal visits that have met all initial inclusion criteria (baseline cognitive status, minimum number of visits, and MRI scan availability). Basic information about this cohort is printed, and the input dataset is logged as a \"used\" artifact in Weights & Biases for lineage tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffde150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Defined Cohort Data (Output from Notebook 02) ---\n",
    "# COHORT_CSV_PATH_NB03_INPUT should be defined in the setup cell\n",
    "print(f\"\\n--- Loading Defined Cohort Data from: {COHORT_CSV_PATH_NB03_INPUT} ---\")\n",
    "cohort_df = None # Initialize to ensure it's defined\n",
    "\n",
    "try:\n",
    "    if COHORT_CSV_PATH_NB03_INPUT is None or not COHORT_CSV_PATH_NB03_INPUT.is_file():\n",
    "         raise FileNotFoundError(f\"Defined cohort data file path not set or file not found: {COHORT_CSV_PATH_NB03_INPUT}. \"\n",
    "                                 \"Ensure Notebook 02 ran successfully and saved its output, \"\n",
    "                                 \"and config.json locators are correct.\")\n",
    "    \n",
    "    cohort_df = pd.read_csv(COHORT_CSV_PATH_NB03_INPUT)\n",
    "    print(f\"Defined cohort data loaded successfully. Initial Shape: {cohort_df.shape}\")\n",
    "\n",
    "    if cohort_df.empty:\n",
    "        print(f\"CRITICAL ERROR: Loaded cohort DataFrame from {COHORT_CSV_PATH_NB03_INPUT} is empty. Cannot proceed.\")\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit() # Or raise error\n",
    "\n",
    "    if run: # Log input stats and artifact if W&B run is active\n",
    "        # Log characteristics of the input cohort\n",
    "        num_input_subjects = cohort_df['Subject ID'].nunique() if 'Subject ID' in cohort_df.columns else 0\n",
    "        run.log({'feature_engineering_input/cohort_rows': cohort_df.shape[0],\n",
    "                 'feature_engineering_input/cohort_columns': cohort_df.shape[1],\n",
    "                 'feature_engineering_input/cohort_subjects': num_input_subjects\n",
    "                })\n",
    "        \n",
    "        # Log the input cohort CSV as a \"used\" artifact by this run for traceability\n",
    "        input_cohort_artifact_name = f\"input_cohort_{DATASET_IDENTIFIER}_NB03_from_NB02\" # Clear, specific name\n",
    "        input_cohort_artifact_description = (\n",
    "            f\"Final analysis cohort data (output of Notebook 02) used as input for Notebook 03 (Feature Engineering). \"\n",
    "            f\"Source file: {COHORT_CSV_PATH_NB03_INPUT.name}\"\n",
    "        )\n",
    "        input_cohort_wandb_artifact = wandb.Artifact(\n",
    "            input_cohort_artifact_name,\n",
    "            type=f\"processed_dataset_{DATASET_IDENTIFIER}\", # Matches type used by NB02 for its output artifact\n",
    "            description=input_cohort_artifact_description,\n",
    "            metadata={\"source_notebook\": \"02_Cohort_Definition\", \n",
    "                      \"shape_rows\": cohort_df.shape[0],\n",
    "                      \"shape_columns\": cohort_df.shape[1],\n",
    "                      \"num_subjects\": num_input_subjects,\n",
    "                      \"path_used_by_nb03\": str(COHORT_CSV_PATH_NB03_INPUT)}\n",
    "        )\n",
    "        # Add the actual file that was used\n",
    "        input_cohort_wandb_artifact.add_file(str(COHORT_CSV_PATH_NB03_INPUT), name=COHORT_CSV_PATH_NB03_INPUT.name)\n",
    "        run.use_artifact(input_cohort_wandb_artifact) # Log that this run *used* this artifact\n",
    "        print(f\"Input cohort artifact '{input_cohort_artifact_name}' (from NB02) logged as used by this W&B run.\")\n",
    "\n",
    "    print(\"\\nCohort DataFrame Head (first 5 rows):\")\n",
    "    #display(cohort_df.head()) # For better display\n",
    "    print(cohort_df.head())\n",
    "    print(\"\\nCohort DataFrame Info:\")\n",
    "    cohort_df.info(verbose=True, show_counts=True) # Provides detailed info including non-null counts\n",
    "\n",
    "except FileNotFoundError as e_fnf_cohort_nb03:\n",
    "    print(f\"CRITICAL ERROR: {e_fnf_cohort_nb03}\")\n",
    "    if run: run.finish(exit_code=1)\n",
    "    # exit()\n",
    "except Exception as e_load_cohort_nb03:\n",
    "    print(f\"CRITICAL ERROR occurred while loading the defined cohort data: {e_load_cohort_nb03}\")\n",
    "    if run: run.finish(exit_code=1)\n",
    "    # exit()\n",
    "\n",
    "# Ensure cohort_df is defined for subsequent cells, even if empty after an error (if not exiting)\n",
    "if cohort_df is None: \n",
    "    cohort_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0caa73e",
   "metadata": {},
   "source": [
    "## 4. Engineer Time-Based Features\n",
    "\n",
    "For effective longitudinal modeling, features representing the passage of time and intervals between visits are essential. This section calculates:\n",
    "\n",
    "* **`Days_from_Baseline`**: The number of days elapsed since the subject's first recorded visit *within the current analysis cohort*. This normalizes the timeline for each subject.\n",
    "* **`Time_since_Last_Visit_Days`**: The number of days that have passed since the subject's immediately preceding visit. For a subject's first visit in the cohort, this value is set to 0.\n",
    "\n",
    "The calculation prioritizes using the `MR Delay` column (assumed to be days from a consistent study baseline). If `MR Delay` is unreliable or largely missing, `Age` is used as a fallback to approximate these temporal features. The method employed for time feature generation is logged to W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61cbc614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Engineer Time-Based Features ---\n",
    "print(\"\\n--- Engineering Time-Based Features ---\")\n",
    "\n",
    "# Ensure cohort_df is not empty and is defined from the previous cell\n",
    "if 'cohort_df' not in locals() or cohort_df.empty:\n",
    "    print(\"CRITICAL ERROR: cohort_df is not defined or is empty. Cannot proceed with feature engineering.\")\n",
    "    if run: run.finish(exit_code=1)\n",
    "    # exit() # Or raise error\n",
    "else:\n",
    "    # Work on a copy to avoid SettingWithCopyWarning if cohort_df was a slice,\n",
    "    # and ensure original cohort_df (if needed later) is untouched by these specific calculations.\n",
    "    # The user's original script modified cohort_df in place, which is also acceptable if intended.\n",
    "    # For clarity of steps, let's use a new variable for this stage of processing.\n",
    "    df_with_time_features = cohort_df.sort_values(by=['Subject ID', 'Visit']).copy()\n",
    "    \n",
    "    time_feature_source_method = \"Not_Calculated\" # Default status\n",
    "\n",
    "    # Prioritize 'MR Delay' (days from study entry) if it exists, is numeric, and is mostly non-null\n",
    "    if 'MR Delay' in df_with_time_features.columns and \\\n",
    "       pd.api.types.is_numeric_dtype(df_with_time_features['MR Delay']) and \\\n",
    "       df_with_time_features['MR Delay'].notnull().sum() > (len(df_with_time_features) * 0.5): # More than 50% non-null\n",
    "        \n",
    "        print(\"  Using 'MR Delay' (days from study baseline) to calculate longitudinal time features.\")\n",
    "        # 'MR Delay' is assumed to be days from a consistent project/study baseline for each subject.\n",
    "        # To get Days_from_Baseline relative to their *first visit included in this cohort*:\n",
    "        df_with_time_features['Min_MR_Delay_In_Cohort'] = df_with_time_features.groupby('Subject ID')['MR Delay'].transform('min')\n",
    "        df_with_time_features['Days_from_Baseline'] = df_with_time_features['MR Delay'] - df_with_time_features['Min_MR_Delay_In_Cohort']\n",
    "        \n",
    "        # Time since last visit (for this subject, in this cohort)\n",
    "        df_with_time_features['Time_since_Last_Visit_Days'] = df_with_time_features.groupby('Subject ID')['Days_from_Baseline'].diff()\n",
    "        time_feature_source_method = 'MR_Delay_Col_Used'\n",
    "\n",
    "    elif 'Age' in df_with_time_features.columns and pd.api.types.is_numeric_dtype(df_with_time_features['Age']):\n",
    "        print(\"  Warning: 'MR Delay' not suitable or unavailable. Using 'Age' to approximate longitudinal time features.\")\n",
    "        # Calculate Days_from_Baseline based on age difference from first visit age in this cohort\n",
    "        df_with_time_features['Min_Age_In_Cohort'] = df_with_time_features.groupby('Subject ID')['Age'].transform('min')\n",
    "        df_with_time_features['Days_from_Baseline'] = (df_with_time_features['Age'] - df_with_time_features['Min_Age_In_Cohort']) * 365.25 # Approximate days\n",
    "        \n",
    "        # Time since last visit (for this subject, in this cohort) based on Age difference\n",
    "        df_with_time_features['Time_since_Last_Visit_Days'] = df_with_time_features.groupby('Subject ID')['Days_from_Baseline'].diff()\n",
    "        time_feature_source_method = 'Age_Approximation_Used'\n",
    "    else:\n",
    "        print(\"  CRITICAL ERROR: Neither 'MR Delay' nor 'Age' column is suitable for calculating time features. Stopping.\")\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit() \n",
    "        # If not exiting, ensure columns exist to prevent downstream errors, though they'll be all NaN\n",
    "        df_with_time_features['Days_from_Baseline'] = np.nan\n",
    "        df_with_time_features['Time_since_Last_Visit_Days'] = np.nan\n",
    "\n",
    "    # For the first visit of each subject, Time_since_Last_Visit_Days will be NaN; fill with 0.0.\n",
    "    if 'Time_since_Last_Visit_Days' in df_with_time_features.columns:\n",
    "        df_with_time_features['Time_since_Last_Visit_Days'] = df_with_time_features['Time_since_Last_Visit_Days'].fillna(0.0)\n",
    "\n",
    "    print(\"  Successfully calculated 'Days_from_Baseline' and 'Time_since_Last_Visit_Days'.\")\n",
    "\n",
    "    # Display example results for verification\n",
    "    print(\"\\n  Example Engineered Time Features (first 5 rows of processed data):\")\n",
    "    cols_to_show_time_example = ['Subject ID', 'Visit', \n",
    "                                 'MR Delay' if 'MR Delay' in df_with_time_features.columns else 'Age', \n",
    "                                 'Days_from_Baseline', 'Time_since_Last_Visit_Days']\n",
    "    # Ensure all columns in the example list actually exist in the DataFrame before trying to display\n",
    "    cols_to_show_time_example = [col for col in cols_to_show_time_example if col in df_with_time_features.columns]\n",
    "    if cols_to_show_time_example:\n",
    "        # display(df_with_time_features[cols_to_show_time_example].head())\n",
    "        print(df_with_time_features[cols_to_show_time_example].head())\n",
    "    else:\n",
    "        print(\"    Could not display example time features (required columns for example view are missing).\")\n",
    "\n",
    "    # Log basic statistics of the new time features to W&B\n",
    "    if 'Days_from_Baseline' in df_with_time_features.columns and \\\n",
    "       'Time_since_Last_Visit_Days' in df_with_time_features.columns and run:\n",
    "        \n",
    "        print(\"\\n  Engineered Time Feature Statistics:\")\n",
    "        desc_engineered_time_features = df_with_time_features[['Days_from_Baseline', 'Time_since_Last_Visit_Days']].describe()\n",
    "        # display(desc_engineered_time_features)\n",
    "        print(desc_engineered_time_features)\n",
    "        \n",
    "        # Log which method was used to create time features to W&B config\n",
    "        run.config.update({'feature_engineering_details/time_feature_source_method': time_feature_source_method}, allow_val_change=True)\n",
    "        \n",
    "        # Log statistics of these new features\n",
    "        for col_stat_name in desc_engineered_time_features.columns:\n",
    "            for idx_stat_name in desc_engineered_time_features.index: # e.g. 'mean', 'std', 'min', '25%'\n",
    "                 # Sanitize stat_name for W&B key (e.g., replace '%' with 'pct')\n",
    "                 safe_stat_name = idx_stat_name.replace('%','pct')\n",
    "                 run.log({f'stats_engineered_features/time/{col_stat_name}_{safe_stat_name}': desc_engineered_time_features.loc[idx_stat_name, col_stat_name]})\n",
    "        print(\"  Engineered time feature statistics logged to W&B.\")\n",
    "    elif not ('Days_from_Baseline' in df_with_time_features.columns and 'Time_since_Last_Visit_Days' in df_with_time_features.columns):\n",
    "        print(\"  Could not calculate or log statistics for engineered time features (columns not created).\")\n",
    "        if run: run.config.update({'feature_engineering_details/time_feature_source_method': 'Failed_or_Skipped'}, allow_val_change=True)\n",
    "\n",
    "# Replace original cohort_df with the processed one for subsequent steps in this notebook\n",
    "if 'df_with_time_features' in locals() and not df_with_time_features.empty:\n",
    "    cohort_df = df_with_time_features \n",
    "else:\n",
    "    print(\"Warning: Time feature engineering resulted in an empty or undefined DataFrame. Original cohort_df may be used or errors might occur.\")\n",
    "    # cohort_df remains as it was, potentially without the new time features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf27c068",
   "metadata": {},
   "source": [
    "## 5. Prepare Static Features and Select Final Columns for Modeling\n",
    "\n",
    "This step finalizes the feature set that will be used for building sequences:\n",
    "1.  **Extract Baseline Clinical Scores:** For each subject, their Clinical Dementia Rating (CDR) and Mini-Mental State Examination (MMSE) scores from their *first visit present in this cohort* are extracted. These serve as static (time-invariant) features, providing crucial baseline cognitive context for each subject's entire sequence of visits.\n",
    "2.  **Define Feature Categories:** Features are categorized into `identifiers`, `time_varying_features`, `static_features`, and the `target_base` ('CDR', used to derive the actual prediction target).\n",
    "3.  **Select and Filter:** These predefined lists of desired features are filtered against the columns actually available in the current DataFrame. Only existing features are retained for the modeling dataset.\n",
    "4.  **Create `feature_df`:** A new DataFrame is created containing only these selected columns.\n",
    "\n",
    "The final lists of time-varying and static features intended for model input are logged to this W&B run's configuration. This configuration can then be fetched by downstream notebooks (e.g., Notebook 04 for fitting preprocessors, Notebook 06/07 for training models) to ensure consistent feature usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ec08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare Static Features & Select Final Columns for Modeling Dataset ---\n",
    "print(\"\\n--- Preparing Static Features & Selecting Final Columns ---\")\n",
    "\n",
    "feature_df = pd.DataFrame() # Initialize to ensure it's defined\n",
    "\n",
    "# Ensure cohort_df (now potentially df_with_time_features) exists and is not empty\n",
    "if 'cohort_df' in locals() and not cohort_df.empty:\n",
    "    # Work on a copy to avoid modifying the DataFrame used for baseline feature extraction\n",
    "    df_for_final_selection = cohort_df.copy()\n",
    "\n",
    "    # --- Extract Baseline Clinical Scores (as static features for each subject) ---\n",
    "    # These are derived from the *first visit present in this cohort* for each subject.\n",
    "    # The DataFrame should already be sorted by Subject ID and Visit from previous cell.\n",
    "    print(\"  Extracting Baseline_CDR and Baseline_MMSE as static features...\")\n",
    "    if 'CDR' in df_for_final_selection.columns and 'Subject ID' in df_for_final_selection.columns:\n",
    "        df_for_final_selection['Baseline_CDR'] = df_for_final_selection.groupby('Subject ID')['CDR'].transform('first')\n",
    "    else:\n",
    "        print(\"  Warning: 'CDR' or 'Subject ID' column not found. 'Baseline_CDR' will be missing.\")\n",
    "        df_for_final_selection['Baseline_CDR'] = np.nan # Ensure column exists if expected\n",
    "\n",
    "    if 'MMSE' in df_for_final_selection.columns and 'Subject ID' in df_for_final_selection.columns:\n",
    "        df_for_final_selection['Baseline_MMSE'] = df_for_final_selection.groupby('Subject ID')['MMSE'].transform('first')\n",
    "        # Propagate the first valid Baseline_MMSE to all visits for that subject\n",
    "        df_for_final_selection['Baseline_MMSE'] = df_for_final_selection.groupby('Subject ID')['Baseline_MMSE'].ffill().bfill()\n",
    "    else:\n",
    "        print(\"  Warning: 'MMSE' or 'Subject ID' column not found. 'Baseline_MMSE' will be missing.\")\n",
    "        df_for_final_selection['Baseline_MMSE'] = np.nan # Ensure column exists\n",
    "    print(\"  Baseline clinical scores extracted and propagated.\")\n",
    "\n",
    "    # --- Define Feature Categories for the Model ---\n",
    "    # These are the *original* column names we aim to use or have engineered.\n",
    "    # They will be filtered by actual availability in df_for_final_selection.\n",
    "\n",
    "    # Days_from_Baseline measures the time elapsed relative to each subject's \n",
    "    # first visit that is part of the analysis cohort. MR Delay represents the \n",
    "    # number of days from a subject's first imaging session.\n",
    "    \n",
    "    # Time-varying features (dynamic per visit)\n",
    "    time_varying_features = [\n",
    "        'Age', 'MMSE', 'nWBV', \n",
    "        'Days_from_Baseline', 'Time_since_Last_Visit_Days'\n",
    "    ]\n",
    "    # Static features (constant per subject)\n",
    "    static_features = [\n",
    "        'M/F', # Original gender column for encoding later by OASISDataset\n",
    "        'EDUC', 'SES', \n",
    "        'Baseline_CDR', 'Baseline_MMSE', # Engineered static features\n",
    "        'eTIV', 'ASF'\n",
    "    ]\n",
    "    # Identifier columns crucial for data structure and linking\n",
    "    identifiers = ['Subject ID', 'Visit', 'MRI ID']\n",
    "    # Base column from which the actual target ('CDR_next_visit') will be derived\n",
    "    target_base_col = ['CDR']\n",
    "\n",
    "\n",
    "    # --- Filter Defined Feature Lists by Actual Availability in the DataFrame ---\n",
    "    available_cols_in_current_df = df_for_final_selection.columns.tolist()\n",
    "    \n",
    "    selected_identifiers = [f for f in identifiers if f in available_cols_in_current_df]\n",
    "    selected_time_varying = [f for f in time_varying_features if f in available_cols_in_current_df]\n",
    "    selected_static = [f for f in static_features if f in available_cols_in_current_df]\n",
    "    selected_target_base = [f for f in target_base_col if f in available_cols_in_current_df]\n",
    "\n",
    "    # Validate presence of essential columns\n",
    "    if not all(id_col in selected_identifiers for id_col in ['Subject ID', 'Visit']) or not selected_target_base:\n",
    "         print(\"CRITICAL ERROR: Essential identifier ('Subject ID', 'Visit') or base target ('CDR') columns \"\n",
    "               \"are missing from the DataFrame after feature selection. Check feature definitions and input data.\")\n",
    "         if run: run.finish(exit_code=1)\n",
    "         # exit()\n",
    "\n",
    "    # Combine all columns to keep for the modeling dataframe\n",
    "    # This feature_df will be used for target creation and then splitting.\n",
    "    final_columns_to_keep_for_feature_df = sorted(list(set(\n",
    "        selected_identifiers + \n",
    "        selected_time_varying + \n",
    "        selected_static + \n",
    "        selected_target_base\n",
    "    )))\n",
    "\n",
    "    print(f\"\\nSelected final columns for 'feature_df' (count: {len(final_columns_to_keep_for_feature_df)}):\")\n",
    "    # pprint(final_columns_to_keep_for_feature_df) # Use pprint for nice list printing if desired\n",
    "\n",
    "    # Create the feature DataFrame with only these selected columns\n",
    "    feature_df = df_for_final_selection[final_columns_to_keep_for_feature_df].copy()\n",
    "    print(f\"Shape of 'feature_df' before target variable creation: {feature_df.shape}\")\n",
    "\n",
    "    # --- Log Selected Feature Lists to W&B Config for Downstream Use ---\n",
    "    # This is the configuration that OASISDataset will expect to find when this run's\n",
    "    # config is fetched by Notebook 04 (Fit Preprocessors) or Notebooks 06/07 (Train Models).\n",
    "    if run:\n",
    "        # This structure with nested 'features' and 'preprocess' (though preprocess is more NB04)\n",
    "        # is what OASISDataset expects.\n",
    "        features_config_for_downstream = {\n",
    "            'time_varying': selected_time_varying, # List of time-varying features before encoding M/F\n",
    "            'static': selected_static             # List of static features before encoding M/F\n",
    "                                                  # M/F will be handled by OASISDataset if 'M/F_encoded' is in its static_feats_from_config\n",
    "        }\n",
    "        # Also log which columns were identified for scaling/imputation in NB04 for full context.\n",
    "        # This is done in NB04 itself after analyzing the training data.\n",
    "        # Here, we just log the feature lists for the model.\n",
    "        \n",
    "        # In NB03, the most important thing to log for OASISDataset is the\n",
    "        # set of feature names that will constitute the tabular input.\n",
    "        # OASISDataset also needs scaling_cols and imputation_cols, which NB04 determines.\n",
    "        # So, NB04 will log the definitive 'features' and 'preprocess' config that OASISDataset uses.\n",
    "        # What NB03 *can* log is the set of features it *prepared* and *selected*.\n",
    "        \n",
    "        wandb.config.update({\n",
    "            \"feature_selection/final_identifiers_kept\": selected_identifiers,\n",
    "            \"feature_selection/final_time_varying_kept\": selected_time_varying,\n",
    "            \"feature_selection/final_static_kept\": selected_static,\n",
    "            \"feature_selection/final_target_base_kept\": selected_target_base,\n",
    "            # For direct use by OASISDataset if NB04 is skipped OR for documentation:\n",
    "            \"features_prepared_in_nb03\": features_config_for_downstream \n",
    "        }, allow_val_change=True)\n",
    "        print(\"Selected feature categories (input to target creation & splitting) logged to W&B config.\")\n",
    "else:\n",
    "    print(\"Skipping static feature preparation and column selection as input DataFrame ('cohort_df') is empty.\")\n",
    "    feature_df = pd.DataFrame() # Ensure feature_df is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01121b5c",
   "metadata": {},
   "source": [
    "## 6. Create Target Variable (`CDR_next_visit`)\n",
    "\n",
    "Generate the primary target variable for the longitudinal prediction task. The goal is to predict the Clinical Dementia Rating (CDR) score of the *next available visit* for each subject. To achieve this:\n",
    "1.  The `feature_df` (containing selected features and the 'CDR' column) is sorted by `Subject ID` and `Visit`.\n",
    "2.  For each subject, the 'CDR' values are shifted by one position (`.shift(-1)`). This assigns the CDR of visit `k+1` as the `CDR_next_visit` for visit `k`.\n",
    "3.  Rows corresponding to a subject's last recorded visit will have a `NaN` value for `CDR_next_visit` (as there is no subsequent visit). These rows are dropped from the `feature_df` because they cannot be used for training or evaluating this specific prediction task.\n",
    "The number of rows before and after this operation is logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d1557ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Target Variable (CDR score at the next visit) ---\n",
    "print(\"\\n--- Creating Target Variable: 'CDR_next_visit' ---\")\n",
    "\n",
    "# Ensure feature_df exists and is not empty from the previous cell\n",
    "if 'feature_df' in locals() and not feature_df.empty:\n",
    "    # Ensure DataFrame is sorted by Subject ID and then by Visit for correct shifting\n",
    "    # This copy is important if feature_df is used elsewhere before this modification.\n",
    "    df_for_target_creation = feature_df.sort_values(by=['Subject ID', 'Visit']).copy()\n",
    "\n",
    "    if 'CDR' in df_for_target_creation.columns and 'Subject ID' in df_for_target_creation.columns:\n",
    "        # Shift CDR scores up within each subject's group to get the next visit's CDR\n",
    "        df_for_target_creation['CDR_next_visit'] = df_for_target_creation.groupby('Subject ID')['CDR'].shift(-1)\n",
    "\n",
    "        # Rows corresponding to the last visit of each subject will now have NaN for 'CDR_next_visit'\n",
    "        initial_rows_before_target_dropna = len(df_for_target_creation)\n",
    "        \n",
    "        # Drop rows where 'CDR_next_visit' is NaN (i.e., the last visit for each subject)\n",
    "        df_with_target = df_for_target_creation.dropna(subset=['CDR_next_visit']).copy()\n",
    "        # It's good practice to ensure the target is of a float type for regression models\n",
    "        df_with_target['CDR_next_visit'] = df_with_target['CDR_next_visit'].astype(float)\n",
    "        \n",
    "        rows_after_target_dropna = len(df_with_target)\n",
    "        rows_dropped_for_last_visit = initial_rows_before_target_dropna - rows_after_target_dropna\n",
    "\n",
    "        print(f\"Removed {rows_dropped_for_last_visit} rows (last visit of a subject or where next CDR was NaN).\")\n",
    "        print(f\"Shape of DataFrame after creating target 'CDR_next_visit' and dropping NaNs: {df_with_target.shape}\")\n",
    "\n",
    "        if run: # Log statistics related to target creation\n",
    "            run.log({\n",
    "                'target_creation/rows_before_dropna_for_target': initial_rows_before_target_dropna,\n",
    "                'target_creation/rows_dropped_for_missing_target': rows_dropped_for_last_visit,\n",
    "                'target_creation/final_rows_with_target': rows_after_target_dropna,\n",
    "                'target_creation/final_subjects_with_target': df_with_target['Subject ID'].nunique() if 'Subject ID' in df_with_target.columns else 0\n",
    "            })\n",
    "\n",
    "        if df_with_target.empty:\n",
    "            print(\"CRITICAL ERROR: No data remaining after creating target variable and dropping NaNs. \"\n",
    "                  \"This might happen if all subjects have only one visit after previous filters.\")\n",
    "            if run: run.finish(exit_code=1)\n",
    "            # exit()\n",
    "        \n",
    "        feature_df = df_with_target # Update feature_df to be this version with the target\n",
    "    else:\n",
    "        print(\"Warning: 'CDR' or 'Subject ID' column not found. Cannot create 'CDR_next_visit' target.\")\n",
    "        # feature_df remains as it was, subsequent steps will likely fail or produce empty results.\n",
    "else:\n",
    "    print(\"Skipping target variable creation as 'feature_df' is empty or not defined.\")\n",
    "    # feature_df remains an empty DataFrame if it was initialized as such."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29dee5",
   "metadata": {},
   "source": [
    "## 7. Perform Subject-Level Stratified Train/Validation/Test Split\n",
    "\n",
    "The `feature_df` is split into training, validation, and test sets. This split is performed at the **subject level** to prevent data leakage, ensuring that all visits from a single subject belong exclusively to one set (train, validation, or test).\n",
    "\n",
    "Stratification is based on the **`Baseline_CDR`** score of each subject. This helps to ensure that the distribution of baseline cognitive impairment levels is similar across the different data splits, which is important for robust model training and evaluation. The split ratios and random state are defined for reproducibility. Counts of subjects and visits in each split are logged to W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "013c8667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Perform Subject-Level Stratified Train/Validation/Test Split ---\n",
    "print(\"\\n--- Performing Stratified Train/Validation/Test Split by Subject ---\")\n",
    "\n",
    "# NOTE ON STRATIFICATION:\n",
    "# Currently, the subject-level split is stratified based on 'Baseline_CDR' to ensure \n",
    "# representation of initial cognitive states across train, validation, and test sets.\n",
    "# For future enhancements or if dealing with datasets where other baseline factors \n",
    "# (e.g., Sex, specific Age groups, Education levels, or future genetic markers like APOE for ADNI) \n",
    "# are known to have very strong confounding effects and imbalanced distributions, \n",
    "# more complex stratification could be considered. This might involve creating a \n",
    "# composite stratification key from multiple baseline variables. However, this also \n",
    "# increases the risk of having strata with too few samples, especially in smaller datasets.\n",
    "# For this phase, stratification by Baseline_CDR is deemed the most critical.\n",
    "\n",
    "# Initialize split DataFrames to ensure they are defined even if splitting fails or df is empty\n",
    "train_df = pd.DataFrame()\n",
    "val_df = pd.DataFrame()\n",
    "test_df = pd.DataFrame()\n",
    "\n",
    "if 'feature_df' in locals() and not feature_df.empty:\n",
    "    # Get unique subjects and their baseline CDR (from the first visit in feature_df) for stratification\n",
    "    # Ensure 'Visit' and 'Baseline_CDR' columns exist\n",
    "    if not all(col in feature_df.columns for col in ['Subject ID', 'Visit', 'Baseline_CDR']):\n",
    "        print(\"CRITICAL ERROR: 'Subject ID', 'Visit', or 'Baseline_CDR' missing from feature_df. Cannot stratify.\")\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "    else:\n",
    "        # Get the Baseline_CDR for each subject (should be constant per subject already due to transform('first'))\n",
    "        # We need one value per unique subject for stratification.\n",
    "        subject_stratification_info = feature_df.drop_duplicates(subset=['Subject ID'])[['Subject ID', 'Baseline_CDR']].copy()\n",
    "        subject_stratification_info.set_index('Subject ID', inplace=True)\n",
    "        \n",
    "        unique_subjects_for_split = subject_stratification_info.index.unique()\n",
    "        subject_baseline_cdr_labels = subject_stratification_info['Baseline_CDR']\n",
    "\n",
    "        if len(unique_subjects_for_split) < 10: # Arbitrary small number, adjust as needed\n",
    "            print(f\"Warning: Very small number of unique subjects ({len(unique_subjects_for_split)}) for splitting. \"\n",
    "                  \"Splits might be very small or unstable.\")\n",
    "\n",
    "        # Use pre-defined split ratios and random state (defined in setup, logged to W&B config)\n",
    "        # TEST_SET_RATIO_CONFIG, VAL_SET_RATIO_CONFIG, RANDOM_STATE_CONFIG\n",
    "        \n",
    "        # Calculate relative validation size for the second split (from (1 - test_ratio) of data)\n",
    "        if (1.0 - TEST_SET_RATIO_CONFIG) <= 0:\n",
    "             print(\"CRITICAL ERROR: TEST_SET_RATIO_CONFIG must be less than 1.0.\")\n",
    "             if run: run.finish(exit_code=1)\n",
    "             # exit()\n",
    "        else:\n",
    "            relative_val_size_for_split = VAL_SET_RATIO_CONFIG / (1.0 - TEST_SET_RATIO_CONFIG)\n",
    "\n",
    "            print(f\"Attempting to split {len(unique_subjects_for_split)} unique subjects.\")\n",
    "            print(f\"  Target Test Ratio: {TEST_SET_RATIO_CONFIG:.2%}\")\n",
    "            print(f\"  Target Validation Ratio (of original): {VAL_SET_RATIO_CONFIG:.2%}\")\n",
    "            print(f\"  Target Train Ratio (of original): {1.0 - TEST_SET_RATIO_CONFIG - VAL_SET_RATIO_CONFIG:.2%}\")\n",
    "            print(f\"  Stratifying by 'Baseline_CDR'. Random State: {RANDOM_STATE_CONFIG}\")\n",
    "\n",
    "            try:\n",
    "                # First split: separate out the test set\n",
    "                train_val_subjects, test_subjects, train_val_labels, _ = train_test_split(\n",
    "                    unique_subjects_for_split, subject_baseline_cdr_labels,\n",
    "                    test_size=TEST_SET_RATIO_CONFIG,\n",
    "                    random_state=RANDOM_STATE_CONFIG,\n",
    "                    stratify=subject_baseline_cdr_labels \n",
    "                )\n",
    "\n",
    "                # Second split: split the remaining (train_val_subjects) into train and validation\n",
    "                # Handle cases where train_val_subjects might be too small or lack diversity for stratification\n",
    "                if len(train_val_subjects) < 2 or (len(np.unique(train_val_labels)) < 2 and len(train_val_subjects) >=2) : # Check if stratification is possible\n",
    "                     print(\"  Warning: Train+Validation set too small or lacks label diversity for stratified validation split. \"\n",
    "                           \"Performing non-stratified split for Train/Validation from the remainder.\")\n",
    "                     if len(train_val_subjects) < 2: # Can't split further\n",
    "                          train_subjects = train_val_subjects\n",
    "                          val_subjects = np.array([]) # No validation subjects\n",
    "                     else: # Can do non-stratified split\n",
    "                          train_subjects, val_subjects = train_test_split(\n",
    "                               train_val_subjects,\n",
    "                               test_size=relative_val_size_for_split, # test_size here refers to proportion for val_subjects\n",
    "                               random_state=RANDOM_STATE_CONFIG\n",
    "                               # No stratify=train_val_labels here\n",
    "                          )\n",
    "                else: # Proceed with stratified split for train/validation\n",
    "                     train_subjects, val_subjects, _, _ = train_test_split(\n",
    "                         train_val_subjects, train_val_labels,\n",
    "                         test_size=relative_val_size_for_split, \n",
    "                         random_state=RANDOM_STATE_CONFIG,\n",
    "                         stratify=train_val_labels\n",
    "                     )\n",
    "\n",
    "                print(f\"\\nSplit completed:\")\n",
    "                print(f\"  Train subjects count: {len(train_subjects)}\")\n",
    "                print(f\"  Validation subjects count: {len(val_subjects)}\")\n",
    "                print(f\"  Test subjects count: {len(test_subjects)}\")\n",
    "                total_split_subjects = len(train_subjects) + len(val_subjects) + len(test_subjects)\n",
    "                print(f\"  Total subjects accounted for in splits: {total_split_subjects} (Expected: {len(unique_subjects_for_split)})\")\n",
    "                if total_split_subjects != len(unique_subjects_for_split):\n",
    "                    print(\"  WARNING: Discrepancy in total split subjects vs. unique subjects!\")\n",
    "\n",
    "\n",
    "                # --- Create the actual split DataFrames using the lists of subject IDs ---\n",
    "                train_df = feature_df[feature_df['Subject ID'].isin(train_subjects)].copy()\n",
    "                val_df = feature_df[feature_df['Subject ID'].isin(val_subjects)].copy()\n",
    "                test_df = feature_df[feature_df['Subject ID'].isin(test_subjects)].copy()\n",
    "\n",
    "                print(f\"\\nSplit DataFrames created:\")\n",
    "                print(f\"  Train DataFrame shape: {train_df.shape} ({train_df['Subject ID'].nunique()} subjects)\")\n",
    "                print(f\"  Validation DataFrame shape: {val_df.shape} ({val_df['Subject ID'].nunique()} subjects)\")\n",
    "                print(f\"  Test DataFrame shape: {test_df.shape} ({test_df['Subject ID'].nunique()} subjects)\")\n",
    "\n",
    "                # Log split details to W&B\n",
    "                if run:\n",
    "                    run.log({\n",
    "                        'split_counts/subjects_train': len(train_subjects),\n",
    "                        'split_counts/subjects_val': len(val_subjects),\n",
    "                        'split_counts/subjects_test': len(test_subjects),\n",
    "                        'split_counts/visits_train': len(train_df),\n",
    "                        'split_counts/visits_val': len(val_df),\n",
    "                        'split_counts/visits_test': len(test_df),\n",
    "                    })\n",
    "                    # Config for split ratios was already logged during W&B init for this notebook\n",
    "                    # run.config.update({\n",
    "                    #     'split_details/test_set_ratio_used': TEST_SET_RATIO_CONFIG,\n",
    "                    #     'split_details/validation_set_ratio_used': VAL_SET_RATIO_CONFIG,\n",
    "                    #     'split_details/stratify_by_column': 'Baseline_CDR',\n",
    "                    #     'split_details/random_state_used': RANDOM_STATE_CONFIG\n",
    "                    # }, allow_val_change=True) # Already logged\n",
    "\n",
    "                    # Verify stratification proportions in W&B\n",
    "                    print(\"  Logging stratification check to W&B...\")\n",
    "                    for split_name_log, df_log_split in [('Train', train_df), ('Validation', val_df), ('Test', test_df)]:\n",
    "                         if not df_log_split.empty and 'Subject ID' in df_log_split.columns and 'Baseline_CDR' in df_log_split.columns:\n",
    "                             baseline_dist_log = df_log_split.drop_duplicates(subset=['Subject ID'])['Baseline_CDR'].value_counts(normalize=True).sort_index()\n",
    "                             for cdr_val, prop in baseline_dist_log.items():\n",
    "                                 run.log({f'split_stratification_check/{split_name_log}_prop_baseline_cdr_{str(cdr_val).replace(\".\",\"p\")}': prop})\n",
    "                    print(\"  Stratification check logged.\")\n",
    "\n",
    "            except ValueError as e_split_value: # Typically from stratify if a class has too few members\n",
    "                print(f\"\\nCRITICAL ERROR during stratified split: {e_split_value}\")\n",
    "                print(\"  This often happens if a group for stratification (e.g., a Baseline_CDR value) \"\n",
    "                      \"has too few members (e.g., less than n_splits=2 for that group).\")\n",
    "                print(\"  Consider adjusting split ratios, ensuring enough samples per stratum, or using non-stratified split as a fallback if cohort is very small.\")\n",
    "                if run: run.finish(exit_code=1)\n",
    "                # exit()\n",
    "            except Exception as e_split_general:\n",
    "                 print(f\"An unexpected CRITICAL ERROR occurred during data splitting: {e_split_general}\")\n",
    "                 if run: run.finish(exit_code=1)\n",
    "                 # exit()\n",
    "else:\n",
    "    print(\"Skipping data splitting as input 'feature_df' is empty or not defined.\")\n",
    "    # Ensure train_df, val_df, test_df are empty DataFrames if not created\n",
    "    train_df = pd.DataFrame()\n",
    "    val_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7872c37",
   "metadata": {},
   "source": [
    "## 8. Save Split DataFrames and Log to W&B Artifacts\n",
    "\n",
    "The resulting train, validation, and test DataFrames, which now include engineered time features, selected static features, identifiers, and the `CDR_next_visit` target variable, are saved locally as efficient Parquet files in this notebook's output directory. These crucial data splits are also logged as versioned artifacts to Weights & Biases. This ensures that the exact datasets used for training, validation, and testing can be easily retrieved and reproduced in subsequent modeling and analysis stages (e.g., Notebook 04 for fitting preprocessors, Notebook 06/07 for model training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "865fae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save Split DataFrames Locally and Log as W&B Artifacts ---\n",
    "print(\"\\n--- Saving Split DataFrames and Logging to W&B Artifacts ---\")\n",
    "\n",
    "# Dictionary of DataFrames to process\n",
    "# This ensures train_df, val_df, test_df are defined (even if empty from a failed split)\n",
    "split_dataframes_to_save = {\n",
    "    \"train\": train_df if 'train_df' in locals() else pd.DataFrame(),\n",
    "    \"validation\": val_df if 'val_df' in locals() else pd.DataFrame(),\n",
    "    \"test\": test_df if 'test_df' in locals() else pd.DataFrame()\n",
    "}\n",
    "# Define a consistent W&B artifact type for these data splits\n",
    "wandb_artifact_type_for_splits = f\"data_split_{DATASET_IDENTIFIER}\" \n",
    "\n",
    "for split_name, df_to_save in split_dataframes_to_save.items():\n",
    "    if df_to_save is not None and not df_to_save.empty:\n",
    "        # Define local save path and W&B artifact name\n",
    "        local_file_name = f\"cohort_{split_name}_{DATASET_IDENTIFIER}.parquet\"\n",
    "        local_file_path = output_dir / local_file_name # output_dir is NB03's static output dir\n",
    "        \n",
    "        # W&B artifact name, e.g., \"cohort_split_train_oasis2\"\n",
    "        wandb_artifact_name_for_split = f\"cohort_split_{split_name}_{DATASET_IDENTIFIER}\"\n",
    "        \n",
    "        try:\n",
    "            # Save locally as Parquet\n",
    "            df_to_save.to_parquet(local_file_path, index=False)\n",
    "            print(f\"{split_name.capitalize()} DataFrame saved locally to: {local_file_path} (Shape: {df_to_save.shape})\")\n",
    "\n",
    "            # Log as W&B artifact\n",
    "            if run: # Check if W&B run is active\n",
    "                print(f\"  Logging {split_name} DataFrame as W&B artifact: '{wandb_artifact_name_for_split}'...\")\n",
    "                split_description = (\n",
    "                    f\"{split_name.capitalize()} data split for {DATASET_IDENTIFIER}. \"\n",
    "                    f\"Contains {df_to_save['Subject ID'].nunique() if 'Subject ID' in df_to_save.columns else 'N/A'} subjects, \"\n",
    "                    f\"{len(df_to_save)} visits. Features engineered, target 'CDR_next_visit' created.\"\n",
    "                )\n",
    "                split_metadata = {\n",
    "                    'dataset_identifier': DATASET_IDENTIFIER,\n",
    "                    'split_type': split_name,\n",
    "                    'num_rows': len(df_to_save),\n",
    "                    'num_columns': len(df_to_save.columns),\n",
    "                    'num_subjects': df_to_save['Subject ID'].nunique() if 'Subject ID' in df_to_save.columns else 'N/A',\n",
    "                    'columns_list': df_to_save.columns.tolist()\n",
    "                }\n",
    "                \n",
    "                artifact_to_log = wandb.Artifact(\n",
    "                    wandb_artifact_name_for_split, \n",
    "                    type=wandb_artifact_type_for_splits, \n",
    "                    description=split_description,\n",
    "                    metadata=split_metadata\n",
    "                )\n",
    "                artifact_to_log.add_file(str(local_file_path), name=local_file_name) # Add the saved parquet file\n",
    "                run.log_artifact(artifact_to_log, aliases=[\"latest\", f\"{split_name}_{time.strftime('%Y%m%d')}\"])\n",
    "                print(f\"  {split_name.capitalize()} data split artifact logged to W&B.\")\n",
    "\n",
    "        except Exception as e_save_split:\n",
    "            print(f\"Warning: Could not save or log {split_name} DataFrame. Error: {e_save_split}\")\n",
    "    else:\n",
    "        print(f\"Skipping saving/logging for empty or None '{split_name}' DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96cf72c",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "The split DataFrames (`cohort_train.parquet`, `cohort_validation.parquet`, `cohort_test.parquet`) containing engineered and selected features are now saved locally and logged as W&B artifacts. These are the primary inputs for Notebook 04 (Fitting Preprocessors) and subsequently for model training (Notebooks 06 & 07).\n",
    "\n",
    "The subsequent stages involve:\n",
    "1.  **Fitting Preprocessors (Notebook 04):** Fit imputers and scalers *only* on `cohort_train.parquet`.\n",
    "2.  **Data Loading Pipeline (`src/datasets.py`):** The `OASISDataset` class will load these Parquet files, apply the fitted preprocessors, handle sequence creation, padding, and batching.\n",
    "3.  **MRI Feature Extraction:** The `run_preprocessing.py` script processes raw MRIs. The `OASISDataset` (for hybrid models) will load these preprocessed MRIs.\n",
    "4.  **Model Training & Evaluation (Notebooks 06, 07):** Train models using the DataLoaders.\n",
    "5.  **Model Analysis (Notebook 08):** Perform interpretability and uncertainty analysis on trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5141d2",
   "metadata": {},
   "source": [
    "## 10. Finalize W&B Run\n",
    "\n",
    "Complete the execution of this notebook and finish the associated Weights & Biases run, ensuring all logs and artifacts are uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b120c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Finish W&B Run ---\n",
    "print(f\"\\n--- {NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER} complete. Finishing W&B run. ---\")\n",
    "if run: # Check if 'run' object exists and is an active run\n",
    "    try:\n",
    "        # Log final counts of the created DataFrames to summary, if they exist\n",
    "        if 'train_df' in locals() and not train_df.empty: run.summary['final_train_df_rows'] = len(train_df)\n",
    "        if 'val_df' in locals() and not val_df.empty: run.summary['final_val_df_rows'] = len(val_df)\n",
    "        if 'test_df' in locals() and not test_df.empty: run.summary['final_test_df_rows'] = len(test_df)\n",
    "        \n",
    "        run.finish()\n",
    "        run_name_to_print = run.name_synced if hasattr(run, 'name_synced') and run.name_synced else \\\n",
    "                            run.name if hasattr(run, 'name') and run.name else \\\n",
    "                            run.id if hasattr(run, 'id') else \"current run\"\n",
    "        print(f\"W&B run '{run_name_to_print}' finished successfully.\")\n",
    "    except Exception as e_finish_run:\n",
    "        print(f\"Error during wandb.finish(): {e_finish_run}\")\n",
    "else:\n",
    "    print(\"No active W&B run to finish for this session.\")\n",
    "\n",
    "print(f\"\\n--- Notebook {NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER} execution finished. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro_predcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
