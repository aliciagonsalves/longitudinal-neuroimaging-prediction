{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871fd443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: notebooks/03_Feature_Engineering_Splitting.ipynb\n",
    "# Purpose: Load the defined cohort, engineer time-based features,\n",
    "#          select features (incl. pre-computed MRI metrics) for baseline modeling,\n",
    "#          perform subject-level stratified split, and save the split datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc8821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782260db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config Loading ---\n",
    "print(\"--- Loading Configuration ---\")\n",
    "CONFIG_PATH = Path('../config.json') # Path relative to the notebook location\n",
    "try:\n",
    "    PROJECT_ROOT = CONFIG_PATH.parent.resolve()\n",
    "    print(f\"Project Root detected as: {PROJECT_ROOT}\")\n",
    "\n",
    "    with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    print(\"Configuration loaded successfully.\")\n",
    "\n",
    "    # Define key variables from config\n",
    "    OUTPUT_DIR_BASE = PROJECT_ROOT / config['data']['output_dir_base']\n",
    "    WANDB_PROJECT = config['wandb']['project_name']\n",
    "    WANDB_ENTITY = config['wandb'].get('entity', None)\n",
    "\n",
    "    # Define input path (output from Notebook 02)\n",
    "    NB02_OUTPUT_DIR = OUTPUT_DIR_BASE / \"02_Cohort_Definition\"\n",
    "    COHORT_CSV_PATH = NB02_OUTPUT_DIR / \"final_analysis_cohort.csv\" # Output from NB 02\n",
    "\n",
    "    # Define specific output dir for this notebook's results and create it\n",
    "    NOTEBOOK_NAME = \"03_Feature_Engineering_Splitting\"\n",
    "    output_dir = OUTPUT_DIR_BASE / NOTEBOOK_NAME\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Outputs will be saved to: {output_dir}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading config or setting up paths: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Initialize W&B Run ---\n",
    "print(\"\\n--- Initializing Weights & Biases Run ---\")\n",
    "run = None # Initialize run to None\n",
    "try:\n",
    "    run = wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        entity=WANDB_ENTITY,\n",
    "        job_type=\"feature-engineering-splitting\",\n",
    "        name=f\"{NOTEBOOK_NAME}-run-{time.strftime('%Y%m%d-%H%M')}\",\n",
    "        config={ # Log key config choices for this job\n",
    "            \"input_cohort_artifact\": f\"analysis_cohort-OASIS2...\", # Ideally get full name from NB02 run\n",
    "            \"input_cohort_path\": str(COHORT_CSV_PATH),\n",
    "            \"time_feature_source\": \"'MR Delay' preferred, fallback 'Age'\", # Documenting strategy\n",
    "            # Split ratios etc. here later via wandb.config.update()\n",
    "        }\n",
    "    )\n",
    "    print(f\"W&B run '{run.name}' initialized successfully. View at: {run.url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing W&B: {e}\")\n",
    "    print(\"Proceeding without W&B logging.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa9147",
   "metadata": {},
   "source": [
    "## Load Defined Cohort Data\n",
    "\n",
    "Load the `final_analysis_cohort.csv` file generated by Notebook 02. This contains the subjects and visits meeting our baseline CDR, minimum visit, and MRI availability criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffde150",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- Loading Defined Cohort Data from: {COHORT_CSV_PATH} ---\")\n",
    "try:\n",
    "    if not COHORT_CSV_PATH.is_file():\n",
    "         raise FileNotFoundError(f\"Cohort data file not found at {COHORT_CSV_PATH}. Ensure Notebook 02 ran successfully.\")\n",
    "    cohort_df = pd.read_csv(COHORT_CSV_PATH)\n",
    "    print(f\"Cohort data loaded successfully. Shape: {cohort_df.shape}\")\n",
    "    if run:\n",
    "        run.log({'feature_engineering/input_cohort_rows': len(cohort_df),\n",
    "                 'feature_engineering/input_cohort_subjects': cohort_df['Subject ID'].nunique()})\n",
    "    print(\"\\nCohort DataFrame Head:\")\n",
    "    print(cohort_df.head())\n",
    "    print(\"\\nCohort DataFrame Info:\")\n",
    "    cohort_df.info()\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    if run: run.finish()\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading the cohort data: {e}\")\n",
    "    if run: run.finish()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0caa73e",
   "metadata": {},
   "source": [
    "## Engineer Time Features\n",
    "\n",
    "Calculate time relative to each subject's baseline visit within this cohort and the time elapsed since the previous visit. We prioritize using the `MR Delay` column if available (assuming it's days from the first visit), otherwise, we approximate using the `Age` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cbc614",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Engineering Time-Based Features ---\")\n",
    "\n",
    "# Ensure data is sorted for time calculations\n",
    "cohort_df = cohort_df.sort_values(by=['Subject ID', 'Visit']).copy()\n",
    "\n",
    "# Prioritize 'MR Delay' if it exists and seems correct (days from baseline visit 1)\n",
    "if 'MR Delay' in cohort_df.columns and cohort_df['MR Delay'].isnull().sum() < len(cohort_df) * 0.5 : # Check if MR Delay mostly exists\n",
    "    print(\"Using 'MR Delay' to calculate time features.\")\n",
    "    # Ensure MR Delay is numeric\n",
    "    cohort_df['MR Delay'] = pd.to_numeric(cohort_df['MR Delay'], errors='coerce')\n",
    "    # Find the MR Delay of the *first included visit* in our cohort for each subject\n",
    "    cohort_df['BaselineVisitMRDelay'] = cohort_df.groupby('Subject ID')['MR Delay'].transform('min')\n",
    "    # Calculate Days_from_Baseline relative to the first visit *in the cohort*\n",
    "    cohort_df['Days_from_Baseline'] = cohort_df['MR Delay'] - cohort_df['BaselineVisitMRDelay']\n",
    "    # Calculate time since last visit using the diff on Days_from_Baseline\n",
    "    cohort_df['Time_since_Last_Visit_Days'] = cohort_df.groupby('Subject ID')['Days_from_Baseline'].diff()\n",
    "    time_feature_source = 'MR Delay'\n",
    "\n",
    "elif 'Age' in cohort_df.columns:\n",
    "    print(\"Warning: Using 'Age' column to approximate time features. Check if 'MR Delay' exists and is suitable.\")\n",
    "    # Calculate Days_from_Baseline based on age difference from first visit age\n",
    "    cohort_df['BaselineAge'] = cohort_df.groupby('Subject ID')['Age'].transform('min')\n",
    "    cohort_df['Days_from_Baseline'] = (cohort_df['Age'] - cohort_df['BaselineAge']) * 365.25\n",
    "    # Calculate time since last visit using the diff on Age\n",
    "    cohort_df['Time_since_Last_Visit_Days'] = cohort_df.groupby('Subject ID')['Age'].diff() * 365.25\n",
    "    time_feature_source = 'Age_approx'\n",
    "\n",
    "else:\n",
    "    print(\"Error: Neither 'MR Delay' nor 'Age' suitable for calculating time features.\")\n",
    "    if run: run.finish()\n",
    "    exit()\n",
    "\n",
    "# Fill NaN for the first visit of each subject with 0\n",
    "cohort_df['Time_since_Last_Visit_Days'].fillna(0, inplace=False) # Use assignment, not inplace\n",
    "# Replace the above line with:\n",
    "cohort_df['Time_since_Last_Visit_Days'] = cohort_df['Time_since_Last_Visit_Days'].fillna(0)\n",
    "\n",
    "print(\"Calculated 'Days_from_Baseline' and 'Time_since_Last_Visit_Days'.\")\n",
    "\n",
    "# Display results for verification\n",
    "print(\"\\nExample Time Features:\")\n",
    "print(cohort_df[['Subject ID', 'Visit', 'Age', 'MR Delay' if 'MR Delay' in cohort_df.columns else 'Age' , 'Days_from_Baseline', 'Time_since_Last_Visit_Days']].head())\n",
    "\n",
    "# Basic check on calculated time features\n",
    "print(\"\\nTime Feature Statistics:\")\n",
    "desc_time_features = cohort_df[['Days_from_Baseline', 'Time_since_Last_Visit_Days']].describe()\n",
    "print(desc_time_features)\n",
    "if run:\n",
    "    run.config.update({'feature_engineering/time_feature_source': time_feature_source})\n",
    "    for col in desc_time_features.columns:\n",
    "        for idx in desc_time_features.index:\n",
    "             run.log({f'stats/time_features/{col}_{idx}': desc_time_features.loc[idx, col]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf27c068",
   "metadata": {},
   "source": [
    "## Prepare Static Features and Select Final Columns\n",
    "\n",
    "Extract baseline features (CDR, MMSE) for each subject to use as static inputs. Select the final set of columns needed for the modeling dataset, including identifiers, time-varying features (clinical + time + pre-computed MRI like nWBV), static features, and the base target variable ('CDR')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ec08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Preparing Static Features & Selecting Final Columns ---\")\n",
    "\n",
    "# Extract Baseline CDR and MMSE using transform('first') on the sorted dataframe\n",
    "cohort_df['Baseline_CDR'] = cohort_df.groupby('Subject ID')['CDR'].transform('first')\n",
    "if 'MMSE' in cohort_df.columns:\n",
    "    cohort_df['Baseline_MMSE'] = cohort_df.groupby('Subject ID')['MMSE'].transform('first')\n",
    "    print(\"Extracted Baseline CDR and Baseline MMSE.\")\n",
    "    # Handle potential NaNs if the first visit MMSE was missing\n",
    "    cohort_df['Baseline_MMSE'] = cohort_df.groupby('Subject ID')['Baseline_MMSE'].ffill().bfill() # Simple forward/backward fill within subject\n",
    "else:\n",
    "    cohort_df['Baseline_MMSE'] = np.nan\n",
    "    print(\"Extracted Baseline CDR. MMSE column not found or not used.\")\n",
    "\n",
    "# Define columns to keep\n",
    "# Focus on features available in the cohort_df for the *initial* model (using nWBV etc.)\n",
    "time_varying_features = [\n",
    "    'Age',      # Age at current visit\n",
    "    'MMSE',     # MMSE at current visit (handle missing via imputation later)\n",
    "    'nWBV',     # Pre-computed MRI feature (handle missing via imputation later)\n",
    "    'Days_from_Baseline',         # Engineered time feature\n",
    "    'Time_since_Last_Visit_Days'  # Engineered time feature\n",
    "]\n",
    "static_features = [\n",
    "    'M/F',      # Sex (needs encoding later)\n",
    "    'EDUC',     # Education (handle missing via imputation later?)\n",
    "    'SES',      # SES (handle missing via imputation later)\n",
    "    'Baseline_CDR', # Extracted baseline feature\n",
    "    'Baseline_MMSE',# Extracted baseline feature (handle missing via imputation later)\n",
    "    'eTIV',     # Often static per subject, treat as static? Or check variability. Let's assume static for now.\n",
    "    'ASF',      # Often static per subject, treat as static? Let's assume static for now.\n",
    "]\n",
    "identifiers = ['Subject ID', 'Visit', 'MRI ID'] # Keep MRI ID for potential future use/linking\n",
    "target_base = ['CDR'] # Base column for creating the target prediction\n",
    "\n",
    "# Filter lists based on actual columns present\n",
    "available_columns = cohort_df.columns.tolist()\n",
    "time_varying_features = [f for f in time_varying_features if f in available_columns]\n",
    "static_features = [f for f in static_features if f in available_columns]\n",
    "identifiers = [f for f in identifiers if f in available_columns]\n",
    "target_base = [f for f in target_base if f in available_columns]\n",
    "\n",
    "# Check if essential columns are present\n",
    "if 'Subject ID' not in identifiers or 'Visit' not in identifiers or not target_base:\n",
    "     print(\"Error: Essential identifier or target columns ('Subject ID', 'Visit', 'CDR') are missing!\")\n",
    "     if run: run.finish()\n",
    "     exit()\n",
    "\n",
    "columns_to_keep = sorted(list(set(identifiers + time_varying_features + static_features + target_base)))\n",
    "\n",
    "print(f\"\\nSelected features for modeling dataset:\")\n",
    "print(columns_to_keep)\n",
    "\n",
    "# Create the feature DataFrame\n",
    "feature_df = cohort_df[columns_to_keep].copy()\n",
    "print(f\"\\nShape of feature DataFrame before target creation: {feature_df.shape}\")\n",
    "\n",
    "# Log selected features\n",
    "if run:\n",
    "    wandb.config.update({\n",
    "        \"features/identifiers\": identifiers,\n",
    "        \"features/time_varying\": time_varying_features,\n",
    "        \"features/static\": static_features,\n",
    "        \"features/target_base\": target_base\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01121b5c",
   "metadata": {},
   "source": [
    "## Create Target Variable (Next Visit CDR)\n",
    "\n",
    "Generate the target variable for our sequence prediction task. This will be the CDR score from the *next* available visit for each subject. Visits that are the last recorded visit for a subject will not have a target and will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1557ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Creating Target Variable (CDR at Next Visit) ---\")\n",
    "\n",
    "# Ensure sorting before shifting\n",
    "feature_df = feature_df.sort_values(by=['Subject ID', 'Visit']).copy()\n",
    "\n",
    "# Shift CDR scores up within each subject group to get the next visit's score\n",
    "feature_df['CDR_next_visit'] = feature_df.groupby('Subject ID')['CDR'].shift(-1)\n",
    "\n",
    "# Handle the last visit for each subject - they will have NaN for the target\n",
    "initial_rows_target = len(feature_df)\n",
    "feature_df.dropna(subset=['CDR_next_visit'], inplace=True)\n",
    "rows_after_target_dropna = len(feature_df)\n",
    "rows_dropped_last_visit = initial_rows_target - rows_after_target_dropna\n",
    "\n",
    "print(f\"Removed {rows_dropped_last_visit} rows corresponding to the last visit of each subject (no future CDR available).\")\n",
    "print(f\"Shape of final modeling DataFrame: {feature_df.shape}\")\n",
    "\n",
    "if run:\n",
    "    run.log({\n",
    "        'feature_engineering/rows_before_target_dropna': initial_rows_target,\n",
    "        'feature_engineering/rows_dropped_for_last_visit': rows_dropped_last_visit,\n",
    "        'feature_engineering/final_rows_with_target': rows_after_target_dropna\n",
    "    })\n",
    "\n",
    "if feature_df.empty:\n",
    "    print(\"Error: No data remaining after creating target variable. Check cohort definition and data.\")\n",
    "    if run: run.finish()\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f18eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Target Variable (Next Visit CDR) ---\n",
    "print(\"\\n--- Creating Target Variable (CDR at Next Visit) ---\")\n",
    "\n",
    "# Shift CDR scores up within each subject group to get the next visit's score\n",
    "feature_df['CDR_next_visit'] = feature_df.groupby('Subject ID')['CDR'].shift(-1)\n",
    "\n",
    "# Handle the last visit for each subject - they will have NaN for the target\n",
    "initial_rows = len(feature_df)\n",
    "feature_df.dropna(subset=['CDR_next_visit'], inplace=True)\n",
    "rows_after_target_dropna = len(feature_df)\n",
    "rows_dropped_last_visit = initial_rows - rows_after_target_dropna\n",
    "\n",
    "print(f\"Removed {rows_dropped_last_visit} rows corresponding to the last visit of each subject (no future CDR available).\")\n",
    "print(f\"Shape of final modeling DataFrame: {feature_df.shape}\")\n",
    "\n",
    "if run:\n",
    "    run.log({\n",
    "        'feature_engineering/rows_before_target_dropna': initial_rows,\n",
    "        'feature_engineering/rows_dropped_for_last_visit': rows_dropped_last_visit,\n",
    "        'feature_engineering/final_rows_with_target': rows_after_target_dropna\n",
    "    })\n",
    "\n",
    "if feature_df.empty:\n",
    "    print(\"Error: No data remaining after creating target variable. Check cohort definition and data.\")\n",
    "    if run: run.finish()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29dee5",
   "metadata": {},
   "source": [
    "## Perform Subject-Level Stratified Train/Validation/Test Split\n",
    "\n",
    "Split the data into training, validation, and test sets. This split MUST be done at the **subject level** to prevent data leakage (i.e., all visits from one subject belong to the same set). We will stratify the split based on the **Baseline CDR** (0 vs 0.5) to ensure similar group representation across sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013c8667",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Performing Stratified Train/Validation/Test Split by Subject ---\")\n",
    "\n",
    "# Get unique subjects and their baseline CDR for stratification\n",
    "# Ensure we take the value associated with the MINIMUM visit per subject in this df\n",
    "subject_baseline_cdr = feature_df.loc[feature_df.groupby('Subject ID')['Visit'].idxmin()][['Subject ID', 'Baseline_CDR']].copy()\n",
    "subject_baseline_cdr.set_index('Subject ID', inplace=True)\n",
    "\n",
    "unique_subjects = subject_baseline_cdr.index.unique()\n",
    "subject_labels = subject_baseline_cdr['Baseline_CDR']\n",
    "\n",
    "if len(unique_subjects) < 50: # Check if cohort size is very small\n",
    "    print(f\"Warning: Very small number of subjects ({len(unique_subjects)}) for splitting.\")\n",
    "\n",
    "# Define split ratios (ensure they sum to <= 1)\n",
    "TEST_SET_RATIO = 0.15\n",
    "VAL_SET_RATIO = 0.15 # Proportion of the *original* data for validation\n",
    "RANDOM_STATE = 42\n",
    "# Set a fixed random state (integer) for the train/test splitting functions.\n",
    "# Using the same state ensures that the *exact same subjects* are assigned\n",
    "# to the train, validation, and test sets every time this code is executed.\n",
    "# This is essential for reproducible results and fair comparison between experiments.\n",
    "\n",
    "# Calculate relative validation size needed for the second split\n",
    "if (1.0 - TEST_SET_RATIO) <= 0:\n",
    "     print(\"Error: Test set ratio must be less than 1.\")\n",
    "     exit()\n",
    "relative_val_size = VAL_SET_RATIO / (1.0 - TEST_SET_RATIO)\n",
    "\n",
    "print(f\"Splitting {len(unique_subjects)} subjects.\")\n",
    "print(f\"Target split ratios -> Test: {TEST_SET_RATIO:.1%}, Validation: {VAL_SET_RATIO:.1%}, Train: {1.0 - TEST_SET_RATIO - VAL_SET_RATIO:.1%}\")\n",
    "\n",
    "# Split subjects into Train and Temp (Val+Test)\n",
    "try:\n",
    "    train_subjects, temp_subjects, train_labels, temp_labels = train_test_split(\n",
    "        unique_subjects, subject_labels,\n",
    "        test_size=TEST_SET_RATIO, # Reserve test set first\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=subject_labels\n",
    "    )\n",
    "\n",
    "    # Split Temp into Validation and Test\n",
    "    # Handle cases where temp set might be too small for stratification\n",
    "    if len(temp_subjects) < 2 or len(np.unique(temp_labels)) < 2:\n",
    "         print(\"Warning: Temp set (Val+Test) too small or lacks diversity for stratification. Performing non-stratified split for Val/Test.\")\n",
    "         # Fallback to non-stratified split if necessary\n",
    "         if len(temp_subjects) < 2:\n",
    "              # Handle edge case: put all temp into val? or test? Or error? Let's put into val for now.\n",
    "              val_subjects = temp_subjects\n",
    "              test_subjects = np.array([]) # No test subjects\n",
    "         else:\n",
    "              val_subjects, test_subjects = train_test_split(\n",
    "                   temp_subjects,\n",
    "                   test_size=TEST_SET_RATIO / (TEST_SET_RATIO + VAL_SET_RATIO), # Adjust test size relative to temp set\n",
    "                   random_state=RANDOM_STATE\n",
    "              )\n",
    "    else:\n",
    "         # Proceed with stratified split\n",
    "         val_subjects, test_subjects, _, _ = train_test_split(\n",
    "             temp_subjects, temp_labels,\n",
    "             test_size=TEST_SET_RATIO / (TEST_SET_RATIO + VAL_SET_RATIO), # Adjust test size relative to temp set\n",
    "             random_state=RANDOM_STATE,\n",
    "             stratify=temp_labels\n",
    "         )\n",
    "\n",
    "    print(f\"\\nSplit complete:\")\n",
    "    print(f\"  Train subjects: {len(train_subjects)}\")\n",
    "    print(f\"  Validation subjects: {len(val_subjects)}\")\n",
    "    print(f\"  Test subjects: {len(test_subjects)}\")\n",
    "    print(f\"  Total subjects accounted for: {len(train_subjects) + len(val_subjects) + len(test_subjects)}\")\n",
    "\n",
    "    # --- Create the split DataFrames ---\n",
    "    train_df = feature_df[feature_df['Subject ID'].isin(train_subjects)].copy()\n",
    "    val_df = feature_df[feature_df['Subject ID'].isin(val_subjects)].copy()\n",
    "    test_df = feature_df[feature_df['Subject ID'].isin(test_subjects)].copy()\n",
    "\n",
    "    print(f\"\\nDataFrames created:\")\n",
    "    print(f\"  Train DF shape: {train_df.shape}\")\n",
    "    print(f\"  Validation DF shape: {val_df.shape}\")\n",
    "    print(f\"  Test DF shape: {test_df.shape}\")\n",
    "\n",
    "    # Log split details\n",
    "    if run:\n",
    "        run.log({\n",
    "            'split/num_subjects_train': len(train_subjects),\n",
    "            'split/num_subjects_val': len(val_subjects),\n",
    "            'split/num_subjects_test': len(test_subjects),\n",
    "            'split/num_visits_train': len(train_df),\n",
    "            'split/num_visits_val': len(val_df),\n",
    "            'split/num_visits_test': len(test_df),\n",
    "        })\n",
    "        wandb.config.update({\n",
    "            'split/test_set_ratio': TEST_SET_RATIO,\n",
    "            'split/validation_set_ratio': VAL_SET_RATIO,\n",
    "            'split/stratify_by': 'Baseline_CDR',\n",
    "            'split/random_state': RANDOM_STATE\n",
    "        })\n",
    "\n",
    "        # Verify stratification (optional but recommended log)\n",
    "        for split_name, df_split in [('train', train_df), ('validation', val_df), ('test', test_df)]:\n",
    "             if not df_split.empty:\n",
    "                 baseline_dist = df_split.drop_duplicates(subset=['Subject ID'])['Baseline_CDR'].value_counts(normalize=True)\n",
    "                 for cdr_val, prop in baseline_dist.items():\n",
    "                     run.log({f'split_check/{split_name}_prop_cdr_{cdr_val}': prop})\n",
    "        print(\"Logged split counts and stratification check to W&B.\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"\\nError during stratified split: {e}\")\n",
    "    print(\"This can happen if a group (e.g., CDR=0.5) has too few members (<= n_splits) for the requested splits.\")\n",
    "    print(\"Consider adjusting split ratios or reviewing the cohort definition.\")\n",
    "    if run: run.finish()\n",
    "    exit()\n",
    "except Exception as e:\n",
    "     print(f\"An unexpected error occurred during splitting: {e}\")\n",
    "     if run: run.finish()\n",
    "     exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7872c37",
   "metadata": {},
   "source": [
    "## Save Split DataFrames and Log Artifacts\n",
    "\n",
    "Save the resulting train, validation, and test DataFrames locally as efficient Parquet files. Log these files as versioned artifacts in Weights & Biases for reproducibility and easy access in later modeling stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865fae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Saving Split DataFrames and Logging Artifacts ---\")\n",
    "\n",
    "# Ensure we have DataFrames to save, even if empty after split error handling\n",
    "split_files = {\n",
    "    \"train\": train_df if 'train_df' in locals() else pd.DataFrame(),\n",
    "    \"validation\": val_df if 'val_df' in locals() else pd.DataFrame(),\n",
    "    \"test\": test_df if 'test_df' in locals() else pd.DataFrame()\n",
    "}\n",
    "artifact_type = \"split-dataframe\" # Use a descriptive type\n",
    "\n",
    "for split_name, df_split in split_files.items():\n",
    "    if df_split is not None and not df_split.empty:\n",
    "        # Define local save path\n",
    "        file_path = output_dir / f\"cohort_{split_name}.parquet\"\n",
    "        artifact_name = f\"cohort-split-{split_name}\" # e.g., cohort-split-train\n",
    "\n",
    "        try:\n",
    "            # Save locally\n",
    "            df_split.to_parquet(file_path, index=False)\n",
    "            print(f\"{split_name.capitalize()} DataFrame saved locally to {file_path} (Shape: {df_split.shape})\")\n",
    "\n",
    "            # Log as W&B artifact\n",
    "            if run:\n",
    "                print(f\"Logging {split_name} DataFrame as W&B artifact: {artifact_name}...\")\n",
    "                description = (f\"{split_name.capitalize()} split of the analysis cohort \"\n",
    "                               f\"({df_split['Subject ID'].nunique()} subjects, {len(df_split)} visits). \"\n",
    "                               f\"Features engineered, ready for sequence creation/scaling/imputation.\")\n",
    "                artifact = wandb.Artifact(artifact_name, type=artifact_type, description=description,\n",
    "                                         metadata={'split': split_name,\n",
    "                                                   'num_rows': len(df_split),\n",
    "                                                   'num_subjects': df_split['Subject ID'].nunique(),\n",
    "                                                   'columns': df_split.columns.tolist()})\n",
    "                artifact.add_file(str(file_path))\n",
    "                run.log_artifact(artifact)\n",
    "                print(f\"{split_name.capitalize()} artifact logged.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save or log {split_name} DataFrame. Error: {e}\")\n",
    "    else:\n",
    "        print(f\"Skipping saving/logging for empty or None {split_name} DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96cf72c",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The split DataFrames (`cohort_train.parquet`, `cohort_validation.parquet`, `cohort_test.parquet`) containing engineered and selected features are now saved locally and logged as W&B artifacts.\n",
    "\n",
    "The subsequent stages involve:\n",
    "\n",
    "1.  **Data Loading Pipeline:** Create PyTorch/TensorFlow `Dataset` and `DataLoader` classes to:\n",
    "    * Load data from the saved parquet files.\n",
    "    * Create sequences of visits for each subject.\n",
    "    * Handle padding and masking for variable sequence lengths.\n",
    "    * **Fit Imputers/Scalers:** Fit `SimpleImputer` (for SES, MMSE, nWBV etc. if needed) and `StandardScaler` **only on the training data (`cohort_train.parquet`)**.\n",
    "    * **Apply Imputation/Scaling:** Transform train, validation, and test data using the *fitted* imputers/scalers.\n",
    "    * Encode categorical features (e.g., 'M/F').\n",
    "    * Prepare batches for the model.\n",
    "2.  **MRI Feature Extraction:** Implement the pipeline to preprocess raw T1w images and extract features using the baseline 3D CNN. This is a separate, significant task that needs to align with the subject/scan IDs in these splits.\n",
    "3.  **Model Implementation:** Define the sequence model (LSTM/GRU) architecture, incorporating the combined clinical/demographic/time/MRI features and dropout.\n",
    "4.  **Training & Evaluation:** Implement the training loop, loss function, optimizer, evaluation metrics, and MC Dropout for uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b120c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Finish W&B Run ---\n",
    "print(\"\\n--- Feature Engineering and Splitting complete. Finishing W&B run. ---\")\n",
    "if run:\n",
    "    run.finish()\n",
    "    print(\"W&B run finished.\")\n",
    "else:\n",
    "    print(\"No active W&B run to finish.\")\n",
    "\n",
    "print(\"\\nScript execution finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro_pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
