{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f98e754",
   "metadata": {},
   "source": [
    "# Notebook 07: Train Hybrid CNN+LSTM Model (OASIS-2)\n",
    "\n",
    "**Project Phase:** 1 (Model Training - Hybrid)\n",
    "**Dataset:** OASIS-2 Longitudinal MRI & Clinical Data\n",
    "\n",
    "**Purpose:**\n",
    "This notebook trains the `ModularLateFusionLSTM` hybrid model (defined in `src/models.py`) to predict the next visit's CDR score. It utilizes both longitudinal tabular clinical/demographic features and features extracted from 3D T1w MRI scans via an internal 3D CNN. This model explores the benefit of incorporating neuroimaging data.\n",
    "\n",
    "**Workflow:**\n",
    "1.  **Setup:** Load `config.json`, define training hyperparameters (`HP`), and resolve paths for data splits.\n",
    "2.  **Consume Preprocessor Artifacts & Fetch NB04 Config:** Download fitted preprocessors (Scaler, Imputer) from W&B Artifacts (logged by NB04). Fetch the authoritative `features` and `preprocess` configurations from the Notebook 04 W&B run that produced these preprocessors.\n",
    "3.  **W&B Initialization:** Start a new W&B run for this training experiment, logging HPs and source configurations. Create a local output directory for model checkpoints.\n",
    "4.  **Setup Device:** Set PyTorch device (CPU, CUDA, MPS).\n",
    "5.  **Load Data & Create DataLoaders:** Instantiate `OASISDataset` for train/validation (with `include_mri=True`), passing downloaded preprocessor paths and fetched NB04 config. Create `DataLoader`s using `pad_collate_fn`.\n",
    "6.  **Define Model, Loss, Optimizer:** Instantiate `ModularLateFusionLSTM`, `MSELoss`, and `Adam` optimizer.\n",
    "7.  **Train Model:** Call the `train_model` utility, which handles epochs, training/validation (using `evaluate_model` with `model_type_flag=\"hybrid\"`), W&B logging, checkpointing, and early stopping.\n",
    "8.  **Evaluate Best Model on Test Set:** Load the best checkpoint, create test `DataLoader`, use `evaluate_model` for test metrics, and log to W&B summary.\n",
    "9.  **Finalize W&B Run.**\n",
    "\n",
    "**Input:**\n",
    "* `config.json`: Main project configuration file.\n",
    "* W&B Artifact Names for training, validation, and test data splits (from Notebook 03).\n",
    "* W&B Artifact Names for fitted preprocessors (Scaler & Imputer - from Notebook 04).\n",
    "* Directory containing preprocessed MRI scans.\n",
    "* `src/` modules.\n",
    "\n",
    "**Output:**\n",
    "* **Local Files (in run-specific directory, e.g., `notebooks/outputs/07_Train_Hybrid_Model_OASIS2/<run_name>/`):**\n",
    "    * Trained `ModularLateFusionLSTM` model checkpoints.\n",
    "* **W&B Run:**\n",
    "    * Logged HPs (including modality dropout rate, source NB04 config).\n",
    "    * Metrics, best model artifact (`Training-HybridCNNLSTM-OASIS2-checkpoint:best`), periodic checkpoints, test metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b77082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: notebooks/07_Train_Hybrid_Model.ipynb\n",
    "# Purpose: Train the Hybrid (CNN+LSTM) model to predict next CDR score\n",
    "#          using tabular clinical features and 3D MRI scan data.\n",
    "#          Loads data processing configurations from the relevant NB04 W&B run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f6080",
   "metadata": {},
   "source": [
    "## Setup: Imports, Paths, Config, Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc231bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "import sys "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a352f",
   "metadata": {},
   "source": [
    "## 1. Setup: Project Configuration, Paths, Utilities, and Hyperparameters\n",
    "\n",
    "This section initializes the notebook environment:\n",
    "* Determines `PROJECT_ROOT` and adds `src/` to `sys.path`.\n",
    "* Imports all necessary custom utilities from `src/` modules.\n",
    "* Loads the main project configuration (`base_config`) from `config.json`.\n",
    "* Defines dataset and notebook-specific identifiers.\n",
    "* **Uses `get_dataset_paths` to resolve paths for input data splits (train, validation, test from Notebook 03) and the general MRI data directory.** Paths to preprocessor `.joblib` files will be obtained later by downloading W&B artifacts.\n",
    "* Defines initial training hyperparameters (`HP`) for the `ModularLateFusionLSTM` model. Key parameters like `tabular_input_size` and CNN configurations will be confirmed or updated after fetching the definitive configuration from Notebook 04.\n",
    "* Defines the locator key from `config.json` for this notebook's local output directory (for model checkpoints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0832f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Project Setup, Configuration Loading, Utility Imports & Hyperparameters ---\n",
    "print(\"--- Initializing Project Setup, Configuration, and HPs for NB07 (Hybrid Model) ---\")\n",
    "\n",
    "# Initialize\n",
    "PROJECT_ROOT = None\n",
    "base_config = {} \n",
    "\n",
    "try:\n",
    "    current_notebook_path = Path.cwd() \n",
    "    potential_project_root = current_notebook_path.parent \n",
    "    if (potential_project_root / \"src\").is_dir() and (potential_project_root / \"config.json\").is_file():\n",
    "        PROJECT_ROOT = potential_project_root\n",
    "    else: \n",
    "        PROJECT_ROOT = current_notebook_path\n",
    "    if not (PROJECT_ROOT / \"src\").is_dir() or not (PROJECT_ROOT / \"config.json\").is_file():\n",
    "        raise FileNotFoundError(f\"Could not find 'src' or 'config.json'. PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "    if str(PROJECT_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    print(f\"PROJECT_ROOT: {PROJECT_ROOT}, added to sys.path.\")\n",
    "\n",
    "    # Import all necessary custom utilities\n",
    "    from src.wandb_utils import initialize_wandb_run, load_model_from_wandb_artifact\n",
    "    from src.paths_utils import get_dataset_paths, get_notebook_run_output_dir\n",
    "    from src.datasets import OASISDataset, pad_collate_fn\n",
    "    from src.models import ModularLateFusionLSTM # Import the HYBRID model\n",
    "    from src.training_utils import train_model\n",
    "    from src.evaluation_utils import evaluate_model\n",
    "    print(\"Successfully imported all required custom utilities and classes.\")\n",
    "\n",
    "except Exception as e_setup:\n",
    "    print(f\"CRITICAL ERROR during initial setup or imports: {e_setup}\")\n",
    "    # exit()\n",
    "\n",
    "# --- Load Main Project Configuration ---\n",
    "print(\"\\n--- Loading Main Project Configuration ---\")\n",
    "try:\n",
    "    if PROJECT_ROOT is None: raise ValueError(\"PROJECT_ROOT not set.\")\n",
    "    CONFIG_PATH_MAIN = PROJECT_ROOT / 'config.json'\n",
    "    with open(CONFIG_PATH_MAIN, 'r', encoding='utf-8') as f:\n",
    "        base_config = json.load(f)\n",
    "    print(f\"Main project config loaded from: {CONFIG_PATH_MAIN}\")\n",
    "except Exception as e_cfg:\n",
    "    print(f\"CRITICAL ERROR loading main config.json: {e_cfg}\")\n",
    "    # exit() \n",
    "\n",
    "# --- Define Dataset, Notebook Specifics, and Resolve Data Split Paths ---\n",
    "DATASET_IDENTIFIER = \"oasis2\" \n",
    "NOTEBOOK_MODULE_NAME = \"07_Train_Hybrid_Model\"\n",
    "# Key from config.json locators for this notebook's output directory\n",
    "NB07_OUTPUT_LOCATOR_KEY = base_config.get(f\"pipeline_artefact_locators_{DATASET_IDENTIFIER}\", {})\\\n",
    "                                     .get(\"train_hybrid_subdir_nb07_key\", \"train_hybrid_subdir_nb07\")\n",
    "                                     # Example key in config: \"train_hybrid_subdir_nb07\": \"07_Train_Hybrid_Outputs\"\n",
    "\n",
    "TRAIN_DATA_PATH = None\n",
    "VAL_DATA_PATH = None\n",
    "TEST_DATA_PATH = None\n",
    "MRI_DATA_DIR = None # For OASISDataset\n",
    "\n",
    "try:\n",
    "    if not base_config: raise ValueError(\"base_config is empty.\")\n",
    "    \n",
    "    train_stage_paths = get_dataset_paths(\n",
    "        PROJECT_ROOT, base_config, DATASET_IDENTIFIER, stage=\"training\"\n",
    "    )\n",
    "    TRAIN_DATA_PATH = train_stage_paths.get('train_data_parquet')\n",
    "    VAL_DATA_PATH = train_stage_paths.get('val_data_parquet')\n",
    "    MRI_DATA_DIR = train_stage_paths.get('mri_data_dir') # MRI dir is needed for hybrid\n",
    "    \n",
    "    test_stage_paths = get_dataset_paths(\n",
    "        PROJECT_ROOT, base_config, DATASET_IDENTIFIER, stage=\"testing\"\n",
    "    )\n",
    "    TEST_DATA_PATH = test_stage_paths.get('test_data_parquet')\n",
    "    \n",
    "    if not all([TRAIN_DATA_PATH, VAL_DATA_PATH, TEST_DATA_PATH, MRI_DATA_DIR]):\n",
    "        raise ValueError(\"Failed to resolve one or more critical data/MRI paths via get_dataset_stage_paths.\")\n",
    "\n",
    "    print(f\"\\nKey data input paths for Notebook 07 ({DATASET_IDENTIFIER}):\")\n",
    "    print(f\"  Training Data Parquet (from NB03): {TRAIN_DATA_PATH}\")\n",
    "    print(f\"  Validation Data Parquet (from NB03): {VAL_DATA_PATH}\")\n",
    "    print(f\"  Test Data Parquet (from NB03): {TEST_DATA_PATH}\")\n",
    "    print(f\"  MRI Data Directory: {MRI_DATA_DIR}\")\n",
    "    \n",
    "    for p_name, p_obj in [(\"Training Data\", TRAIN_DATA_PATH), (\"Validation Data\", VAL_DATA_PATH), (\"Test Data\", TEST_DATA_PATH)]:\n",
    "        if not p_obj.is_file(): raise FileNotFoundError(f\"CRITICAL: {p_name} parquet file not found at: {p_obj}\")\n",
    "    if not MRI_DATA_DIR.is_dir(): raise FileNotFoundError(f\"CRITICAL: MRI Data Directory not found at: {MRI_DATA_DIR}\")\n",
    "    print(\"All critical input data and MRI paths for NB07 verified.\")\n",
    "\n",
    "except (KeyError, ValueError, FileNotFoundError) as e_paths_nb07:\n",
    "    print(f\"CRITICAL ERROR during path setup for NB07: {e_paths_nb07}\")\n",
    "    # exit()\n",
    "except Exception as e_general_nb07_setup:\n",
    "    print(f\"CRITICAL ERROR during setup for NB07: {e_general_nb07_setup}\")\n",
    "    # exit()\n",
    "\n",
    "# --- Define Training Hyperparameters (HP) for Hybrid Model ---\n",
    "print(\"\\n--- Defining Training Hyperparameters (HP) for Hybrid Model ---\")\n",
    "HP = {\n",
    "    'model_type': 'HybridCNNLSTM', \n",
    "    'dataset_identifier': DATASET_IDENTIFIER,\n",
    "    'batch_size': 1, # Might need to be smaller than baseline due to MRI memory\n",
    "    'learning_rate': 1e-4,     \n",
    "    'epochs': 2, # Reduced for local testing, increase for full runs\n",
    "    \n",
    "    # LSTM parameters (can be shared or specific if fetched from NB04 config later)\n",
    "    'lstm_hidden_size': 128, # General LSTM hidden size, can be overridden for mri/tabular streams\n",
    "    'mri_lstm_hidden_size': 128, # Specific for MRI stream\n",
    "    'tabular_lstm_hidden_size': 128, # Specific for Tabular stream\n",
    "    'num_lstm_layers': 2,      \n",
    "    'lstm_dropout_prob': 0.3,\n",
    "    'modality_dropout_rate': 0.0, # Set to 0.0 for no MD, or e.g., 0.1, 0.2 for MD\n",
    "    \n",
    "    'dataloader_num_workers': 0, \n",
    "    'patience': 10,            \n",
    "    'save_checkpoint_every_n_epochs': 5, \n",
    "    'random_seed': 42,\n",
    "    \n",
    "    # These will be updated/confirmed from NB04's config:\n",
    "    'tabular_input_size': None, \n",
    "    'cnn_input_channels': None, # Will come from cnn_model_params in NB04 config\n",
    "    'cnn_output_features': None # Will come from cnn_model_params in NB04 config\n",
    "}\n",
    "# Seed for reproducibility\n",
    "np.random.seed(HP['random_seed'])\n",
    "torch.manual_seed(HP['random_seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(HP['random_seed'])\n",
    "print(\"Training hyperparameters (HP) for Hybrid model defined. Key sizes will be set after fetching NB04 config.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ffb4c",
   "metadata": {},
   "source": [
    "## 2. Initialize W&B Run, Fetch Preprocessor Artifacts & Definitive Configuration from Notebook 04\n",
    "\n",
    "This step is identical in principle to Notebook 06:\n",
    "1.  **Initialize W&B Run for Notebook 07:** A new W&B run is started for this `ModularLateFusionLSTM` training experiment using `initialize_wandb_run`.\n",
    "2.  **Define Output Directory:** A unique local directory for this run's model checkpoints is created.\n",
    "3.  **Consume Preprocessor Artifacts:** Fitted `StandardScaler` and `SimpleImputer` W&B Artifacts (from Notebook 04) are downloaded via `run.use_artifact()`.\n",
    "4.  **Fetch Authoritative NB04 Configuration:** The W&B Run that produced these preprocessors is identified using `artifact.logged_by()`. Its configuration, containing the definitive `features` (time-varying, static), `preprocess` (imputation/scaling columns, strategies), `cnn_model_params` (for MRI input shape and CNN output size), and `preprocessing_config` (for MRI file suffix) is fetched. This becomes `config_for_dataset`.\n",
    "5.  **Update `HP`:** Key hyperparameters like `HP['tabular_input_size']`, `HP['cnn_input_channels']`, and `HP['cnn_output_features']` are dynamically set or confirmed based on the fetched `config_for_dataset`.\n",
    "6.  All source information is logged to the current Notebook 07 W&B run's configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2416120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize W&B Run, Fetch Preprocessor Artifacts & NB04 Config ---\n",
    "print(\"\\n--- Initializing W&B Run for NB07 & Fetching NB04 Preprocessing Setup ---\")\n",
    "\n",
    "run = None\n",
    "run_output_dir_for_checkpoints = None\n",
    "SCALER_PATH_FROM_ARTIFACT = None\n",
    "IMPUTER_PATH_FROM_ARTIFACT = None\n",
    "config_for_dataset = {} \n",
    "source_nb04_run_id_logged = \"N/A\"\n",
    "\n",
    "# --- 1. Initialize W&B Run for THIS Notebook 07 execution ---\n",
    "temp_hp_for_init = HP.copy() # Log initial HP; it will be updated with input_size etc.\n",
    "\n",
    "nb_number_prefix_nb07 = NOTEBOOK_MODULE_NAME.split('_')[0] if '_' in NOTEBOOK_MODULE_NAME else \"NB\"\n",
    "job_specific_type_nb07 = f\"{nb_number_prefix_nb07}-HybridCNNLSTM-{DATASET_IDENTIFIER}\"\n",
    "mod_drop_suffix = f\"MD{HP.get('modality_dropout_rate',0.0):.1f}\" if HP.get('modality_dropout_rate',0.0) > 0 else \"NoMD\"\n",
    "\n",
    "custom_elements_for_name_nb07 = [\n",
    "    nb_number_prefix_nb07, DATASET_IDENTIFIER.upper(), \"Hybrid\", mod_drop_suffix,\n",
    "    f\"h{HP.get('mri_lstm_hidden_size', HP['lstm_hidden_size'])}\", # Use specific or general\n",
    "    f\"nl{HP['num_lstm_layers']}\", f\"dp{HP['lstm_dropout_prob']:.1f}\",\n",
    "    f\"lr{HP['learning_rate']:.0e}\", f\"bs{HP['batch_size']}\"\n",
    "]\n",
    "run = initialize_wandb_run(\n",
    "    base_project_config=base_config,\n",
    "    job_group=\"Training\",\n",
    "    job_specific_type=job_specific_type_nb07,\n",
    "    run_specific_config=temp_hp_for_init, \n",
    "    custom_run_name_elements=custom_elements_for_name_nb07,\n",
    "    notes=f\"Training ModularLateFusionLSTM on {DATASET_IDENTIFIER.upper()} (Modality Dropout: {HP.get('modality_dropout_rate',0.0)}).\"\n",
    ")\n",
    "\n",
    "# --- 2. Define Run-Specific Output Directory for Local Checkpoints ---\n",
    "if run:\n",
    "    print(f\"W&B run '{run.name}' (Job Type: '{run.job_type}') initialized. View at: {run.url}\")\n",
    "    run_output_dir_for_checkpoints = get_notebook_run_output_dir(\n",
    "        PROJECT_ROOT, base_config, NB07_OUTPUT_LOCATOR_KEY, run, DATASET_IDENTIFIER # Use NB07's locator key\n",
    "    )\n",
    "    print(f\"Model checkpoints for this run will be saved locally to: {run_output_dir_for_checkpoints}\")\n",
    "    run.config.update({\"run_outputs/local_checkpoint_dir\": str(run_output_dir_for_checkpoints)}, allow_val_change=True)\n",
    "else:\n",
    "    print(\"W&B run initialization failed. Proceeding with local fallback for outputs.\")\n",
    "    default_nb_locator_nb07 = f\"{NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER}_local_outputs\"\n",
    "    run_output_dir_for_checkpoints = get_notebook_run_output_dir(\n",
    "        PROJECT_ROOT, base_config, NB07_OUTPUT_LOCATOR_KEY if base_config else default_nb_locator_nb07, \n",
    "        None, DATASET_IDENTIFIER\n",
    "    )\n",
    "    print(f\"Model checkpoints (local fallback) will be saved to: {run_output_dir_for_checkpoints}\")\n",
    "\n",
    "if not isinstance(run_output_dir_for_checkpoints, Path):\n",
    "    run_output_dir_for_checkpoints = Path(run_output_dir_for_checkpoints)\n",
    "\n",
    "# --- 3. Fetch Preprocessor Artifacts & NB04 Config (requires active NB07 W&B run) ---\n",
    "if run: \n",
    "    try:\n",
    "        # Define expected preprocessor artifact names (must match what NB04 logged)\n",
    "        cfg_preprocessing = base_config.get('preprocessing_config', {}) # Main project config\n",
    "        cfg_locators = base_config.get(f\"pipeline_artefact_locators_{DATASET_IDENTIFIER}\", {}) # Main project config\n",
    "\n",
    "        scaling_strategy_name = cfg_preprocessing.get('scaling_strategy', 'standard_scaler')\n",
    "        imputation_strategy_name = cfg_preprocessing.get('imputation_strategy', 'median')\n",
    "\n",
    "        SCALER_ARTIFACT_LOGICAL_NAME = f\"scaler_{scaling_strategy_name.lower().replace('scaler','').replace('_','')}_{DATASET_IDENTIFIER}\"\n",
    "        IMPUTER_ARTIFACT_LOGICAL_NAME = f\"simple_imputer_{imputation_strategy_name.lower()}_{DATASET_IDENTIFIER}\"\n",
    "        \n",
    "        PREPROCESSOR_ARTIFACT_TYPE = f\"preprocessor_{DATASET_IDENTIFIER}\"\n",
    "        PREPROCESSOR_ARTIFACT_VERSION = \"latest\"\n",
    "\n",
    "        # --- Download SCALER Artifact ---\n",
    "        scaler_artifact_full_wandb_path = f\"{base_config['wandb']['entity']}/{base_config['wandb']['project_name']}/{SCALER_ARTIFACT_LOGICAL_NAME}:{PREPROCESSOR_ARTIFACT_VERSION}\"\n",
    "        print(f\"  Attempting to use Scaler artifact: {scaler_artifact_full_wandb_path}\")\n",
    "        scaler_artifact = run.use_artifact(scaler_artifact_full_wandb_path, type=PREPROCESSOR_ARTIFACT_TYPE)\n",
    "        scaler_artifact_dir = Path(scaler_artifact.download())\n",
    "        scaler_filename_in_artifact = cfg_locators.get('scaler_fname_pattern', '{scaling_strategy}_{dataset_identifier}.joblib').format(\n",
    "            scaling_strategy=scaling_strategy_name.lower(), dataset_identifier=DATASET_IDENTIFIER)\n",
    "        SCALER_PATH_FROM_ARTIFACT = scaler_artifact_dir / scaler_filename_in_artifact\n",
    "        if not SCALER_PATH_FROM_ARTIFACT.is_file(): # Fallback scan\n",
    "            found_scalers = list(scaler_artifact_dir.glob(\"*.joblib\"))\n",
    "            if found_scalers: SCALER_PATH_FROM_ARTIFACT = found_scalers[0]\n",
    "            else: raise FileNotFoundError(f\"Scaler .joblib ('{scaler_filename_in_artifact}') not found in artifact {scaler_artifact.name}\")\n",
    "        print(f\"  Scaler artifact '{scaler_artifact.name}' downloaded. Local path: {SCALER_PATH_FROM_ARTIFACT}\")\n",
    "\n",
    "        # --- Download IMPUTER Artifact ---\n",
    "        imputer_artifact_full_wandb_path = f\"{base_config['wandb']['entity']}/{base_config['wandb']['project_name']}/{IMPUTER_ARTIFACT_LOGICAL_NAME}:{PREPROCESSOR_ARTIFACT_VERSION}\"\n",
    "        print(f\"  Attempting to use Imputer artifact: {imputer_artifact_full_wandb_path}\")\n",
    "        imputer_artifact = run.use_artifact(imputer_artifact_full_wandb_path, type=PREPROCESSOR_ARTIFACT_TYPE)\n",
    "        imputer_artifact_dir = Path(imputer_artifact.download())\n",
    "        imputer_fname_pattern = cfg_locators.get('imputer_fname_pattern', 'simple_imputer_{imputation_strategy}_{dataset_identifier}.joblib')\n",
    "        imputer_filename_in_artifact = imputer_fname_pattern.format(\n",
    "            imputation_strategy=imputation_strategy_name.lower(), dataset_identifier=DATASET_IDENTIFIER)\n",
    "        IMPUTER_PATH_FROM_ARTIFACT = imputer_artifact_dir / imputer_filename_in_artifact\n",
    "        if not IMPUTER_PATH_FROM_ARTIFACT.is_file(): # Fallback scan\n",
    "            found_imputers = list(imputer_artifact_dir.glob(\"*.joblib\"))\n",
    "            if found_imputers: IMPUTER_PATH_FROM_ARTIFACT = found_imputers[0]\n",
    "            else: raise FileNotFoundError(f\"Imputer .joblib ('{imputer_filename_in_artifact}') not found in artifact {imputer_artifact.name}\")\n",
    "        print(f\"  Imputer artifact '{imputer_artifact.name}' downloaded. Local path: {IMPUTER_PATH_FROM_ARTIFACT}\")\n",
    "\n",
    "        # --- Fetch Full Configuration from the NB04 Run that Produced these Preprocessors ---\n",
    "        nb04_producer_run = imputer_artifact.logged_by()  # Check if the run is correct  \n",
    "        if nb04_producer_run:\n",
    "            source_nb04_run_id_logged = nb04_producer_run.id\n",
    "            nb04_run_name = nb04_producer_run.name\n",
    "            print(f\"  Fetching config from NB04 producer run: '{nb04_run_name}' (ID: {source_nb04_run_id_logged})\")\n",
    "            config_from_nb04_run = dict(nb04_producer_run.config)\n",
    "            if 'features' not in config_from_nb04_run or 'preprocess' not in config_from_nb04_run \\\n",
    "                or 'cnn_model_params' not in config_from_nb04_run \\\n",
    "                or 'preprocessing_config' not in config_from_nb04_run: # For MRI suffix\n",
    "                raise ValueError(\"Fetched NB04 config missing critical sections: 'features', 'preprocess', 'cnn_model_params', or 'preprocessing_config'.\")\n",
    "            config_for_dataset = config_from_nb04_run \n",
    "            print(\"  Definitive config for OASISDataset fetched from NB04 run.\")\n",
    "\n",
    "            # --- Update HP with input_size and CNN params from fetched config_for_dataset ---\n",
    "            time_varying_cfg = config_for_dataset.get('features', {}).get('time_varying', [])\n",
    "            static_cfg = config_for_dataset.get('features', {}).get('static', [])\n",
    "            HP['tabular_input_size'] = len(time_varying_cfg) + len(static_cfg) # Specific for hybrid model\n",
    "            \n",
    "            cnn_params_from_nb04_config = config_for_dataset.get('cnn_model_params', {})\n",
    "            HP['cnn_input_channels'] = cnn_params_from_nb04_config.get('input_shape', [1,0,0,0])[0] # Default from Simple3DCNN\n",
    "            HP['cnn_output_features'] = cnn_params_from_nb04_config.get('output_features', 128)   # Default from Simple3DCNN\n",
    "            \n",
    "            print(f\"  Updated HP: 'tabular_input_size' to {HP['tabular_input_size']}\")\n",
    "            print(f\"  Updated HP: 'cnn_input_channels' to {HP['cnn_input_channels']}\")\n",
    "            print(f\"  Updated HP: 'cnn_output_features' to {HP['cnn_output_features']}\")\n",
    "            if HP['tabular_input_size'] == 0 or HP['cnn_input_channels'] == 0:\n",
    "                print(\"  WARNING: Calculated tabular_input_size or cnn_input_channels is 0. Check NB04 config.\")\n",
    "\n",
    "            # Update current NB07 run's config with all this provenance and final HPs\n",
    "            current_run_config_update = {\n",
    "                \"source_config_from_nb04/producer_run_id\": source_nb04_run_id_logged,\n",
    "                \"source_config_from_nb04/producer_run_name\": nb04_run_name,\n",
    "                \"source_config_from_nb04/scaler_artifact_used\": scaler_artifact.name,\n",
    "                \"source_config_from_nb04/imputer_artifact_used\": imputer_artifact.name,\n",
    "                \"dataset_config_used/features\": config_for_dataset.get('features',{}),\n",
    "                \"dataset_config_used/preprocess\": config_for_dataset.get('preprocess',{}),\n",
    "                \"dataset_config_used/cnn_model_params\": config_for_dataset.get('cnn_model_params',{}),\n",
    "                \"dataset_config_used/preprocessing_config_mri\": config_for_dataset.get('preprocessing_config',{}),\n",
    "            }\n",
    "            current_run_config_update.update(HP) # Add all HPs to the run config\n",
    "            run.config.update(current_run_config_update, allow_val_change=True)\n",
    "            print(\"  NB07 W&B run config updated with source NB04 info, final dataset config, and all HPs.\")\n",
    "        else:\n",
    "            raise ConnectionError(\"Could not retrieve producer run from preprocessor artifact.\")\n",
    "    except Exception as e_fetch_nb04_all:\n",
    "        print(f\"CRITICAL ERROR fetching preprocessor artifacts or config from NB04: {e_fetch_nb04_all}\")\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "else: # W&B run for NB07 failed to initialize\n",
    "    print(\"CRITICAL ERROR: W&B run for NB07 not initialized. Cannot fetch artifacts or proceed.\")\n",
    "\n",
    "# Final check\n",
    "if not all([SCALER_PATH_FROM_ARTIFACT, IMPUTER_PATH_FROM_ARTIFACT, \n",
    "            config_for_dataset.get('features'), HP.get('tabular_input_size') is not None,\n",
    "            HP.get('cnn_input_channels') is not None, HP.get('cnn_output_features') is not None]):\n",
    "    print(\"CRITICAL ERROR: Essential components for model training are not properly set up after NB04 config/artifact fetch.\")\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6e34e",
   "metadata": {},
   "source": [
    "## 4. Setup Device\n",
    "\n",
    "Detect and configure the appropriate PyTorch device (CUDA GPU, MPS for Apple Silicon, or CPU) for model training and data handling. The selected device is logged to the current W&B run's configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe6e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup Device (GPU/CPU/MPS) ---\n",
    "print(\"\\n--- Setting up PyTorch Device ---\")\n",
    "\n",
    "# This logic prioritizes MPS if available, then CUDA, then CPU.\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU).\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA and MPS not available. Using CPU.\")\n",
    "\n",
    "# Log the determined device to the W&B run config for this training session\n",
    "if run: # 'run' is the W&B run object for this NB07 execution\n",
    "    run.config.update({'training_details/device_used': str(device)}, allow_val_change=True)\n",
    "    print(f\"Device '{str(device)}' logged to W&B run config.\")\n",
    "else:\n",
    "    print(f\"Device set to '{str(device)}' (W&B run not active for logging).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd2717",
   "metadata": {},
   "source": [
    "## 5. Load Data & Create DataLoaders for Hybrid Model\n",
    "\n",
    "This section instantiates the custom `OASISDataset` for the training and validation data splits (`cohort_train.parquet` and `cohort_validation.parquet`). Key configurations for the dataset are critical:\n",
    "\n",
    "* **Preprocessor Paths:** The paths to the *fitted* `StandardScaler` and `SimpleImputer` (obtained by downloading W&B Artifacts logged by Notebook 04) are provided to `OASISDataset`.\n",
    "* **Feature & Preprocessing Configuration:** The `config_for_dataset` dictionary (fetched from the W&B configuration of the Notebook 04 run that produced the preprocessors) is passed to `OASISDataset`. This ensures it uses the authoritative lists of time-varying/static features and applies the corresponding imputation/scaling.\n",
    "* **MRI Data:** For this hybrid model, `include_mri` is explicitly set to `True`, and the path to the preprocessed MRI scans (`MRI_DATA_DIR`) is provided.\n",
    "\n",
    "The instantiated `OASISDataset` objects are then wrapped in PyTorch `DataLoader`s, which handle batching and use the custom `pad_collate_fn` to manage variable sequence lengths and the dual-modality input. The model input sizes (tabular and CNN-derived) are also confirmed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d4383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate Datasets and DataLoaders for Hybrid Model Training ---\n",
    "print(\"\\n--- Loading Data and Creating DataLoaders for Hybrid Model Training ---\")\n",
    "\n",
    "# Initialize dataset and loader variables\n",
    "train_dataset: OASISDataset | None = None\n",
    "val_dataset: OASISDataset | None = None\n",
    "train_loader: DataLoader | None = None\n",
    "val_loader: DataLoader | None = None\n",
    "\n",
    "# --- Prerequisite Variable Check ---\n",
    "# These are expected from preceding cells:\n",
    "#   TRAIN_DATA_PATH, VAL_DATA_PATH (Paths to .parquet files)\n",
    "#   SCALER_PATH_FROM_ARTIFACT, IMPUTER_PATH_FROM_ARTIFACT (Paths to downloaded .joblib files)\n",
    "#   config_for_dataset (Authoritative config from NB04 run for OASISDataset)\n",
    "#   MRI_DATA_DIR (Path to preprocessed MRI scans)\n",
    "#   HP (Hyperparameter dictionary for batch_size, num_workers, and model input sizes)\n",
    "#   pad_collate_fn, OASISDataset (Imported classes)\n",
    "\n",
    "required_vars_for_hybrid_loading = {\n",
    "    'TRAIN_DATA_PATH': TRAIN_DATA_PATH, 'VAL_DATA_PATH': VAL_DATA_PATH,\n",
    "    'SCALER_PATH_FROM_ARTIFACT': SCALER_PATH_FROM_ARTIFACT, \n",
    "    'IMPUTER_PATH_FROM_ARTIFACT': IMPUTER_PATH_FROM_ARTIFACT,\n",
    "    'config_for_dataset': config_for_dataset, \n",
    "    'MRI_DATA_DIR': MRI_DATA_DIR, 'HP': HP\n",
    "}\n",
    "\n",
    "proceed_with_hybrid_loading = True\n",
    "for var_name, var_value in required_vars_for_hybrid_loading.items():\n",
    "    if var_value is None:\n",
    "        print(f\"  CRITICAL ERROR: Prerequisite variable '{var_name}' for DataLoaders is None.\")\n",
    "        proceed_with_hybrid_loading = False\n",
    "    if var_name == 'config_for_dataset' and \\\n",
    "       (not var_value or 'features' not in var_value or 'preprocess' not in var_value or \\\n",
    "        'cnn_model_params' not in var_value or 'preprocessing_config' not in var_value): # MRI config also needed\n",
    "        print(f\"  CRITICAL ERROR: 'config_for_dataset' is empty or missing essential sections \"\n",
    "              \"('features', 'preprocess', 'cnn_model_params', 'preprocessing_config').\")\n",
    "        proceed_with_hybrid_loading = False\n",
    "    if var_name == 'HP' and (not var_value or HP.get('tabular_input_size') is None or \\\n",
    "                             HP.get('cnn_input_channels') is None or HP.get('cnn_output_features') is None):\n",
    "        print(f\"  CRITICAL ERROR: 'HP' dictionary missing key model input sizes \"\n",
    "              \"('tabular_input_size', 'cnn_input_channels', 'cnn_output_features'). \"\n",
    "              \"Ensure these were set after fetching NB04 config.\")\n",
    "        proceed_with_hybrid_loading = False\n",
    "\n",
    "if not proceed_with_hybrid_loading:\n",
    "    print(\"Halting DataLoader creation due to missing or invalid prerequisites.\")\n",
    "    if run: run.finish(exit_code=1)\n",
    "    # exit()\n",
    "else:\n",
    "    try:\n",
    "        print(f\"\\nInstantiating training dataset for {DATASET_IDENTIFIER.upper()} (Hybrid - MRI Included)...\")\n",
    "        train_dataset = OASISDataset(\n",
    "            data_parquet_path=TRAIN_DATA_PATH,\n",
    "            scaler_path=SCALER_PATH_FROM_ARTIFACT,\n",
    "            imputer_path=IMPUTER_PATH_FROM_ARTIFACT,\n",
    "            config=config_for_dataset, # Authoritative config from NB04 W&B run\n",
    "            mri_data_dir=MRI_DATA_DIR,\n",
    "            include_mri=True # Explicitly True for hybrid model\n",
    "        )\n",
    "        num_train_subjects = len(train_dataset)\n",
    "        print(f\"  Train dataset (hybrid) created. Number of subjects (sequences): {num_train_subjects}\")\n",
    "\n",
    "        print(f\"\\nInstantiating validation dataset for {DATASET_IDENTIFIER.upper()} (Hybrid - MRI Included)...\")\n",
    "        val_dataset = OASISDataset(\n",
    "            data_parquet_path=VAL_DATA_PATH,\n",
    "            scaler_path=SCALER_PATH_FROM_ARTIFACT, \n",
    "            imputer_path=IMPUTER_PATH_FROM_ARTIFACT,\n",
    "            config=config_for_dataset, \n",
    "            mri_data_dir=MRI_DATA_DIR,\n",
    "            include_mri=True \n",
    "        )\n",
    "        num_val_subjects = len(val_dataset)\n",
    "        print(f\"  Validation dataset (hybrid) created. Number of subjects (sequences): {num_val_subjects}\")\n",
    "\n",
    "        # --- Create DataLoaders ---\n",
    "        print(\"\\nCreating DataLoaders for Hybrid Model...\")\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=HP['batch_size'], \n",
    "            shuffle=True, \n",
    "            collate_fn=pad_collate_fn, # Handles 5-item tuples for hybrid\n",
    "            num_workers=HP.get('dataloader_num_workers', 0),\n",
    "            persistent_workers=(HP.get('dataloader_num_workers',0) > 0) if HP.get('dataloader_num_workers',0) > 0 else False\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=HP['batch_size'], \n",
    "            shuffle=False, \n",
    "            collate_fn=pad_collate_fn, # Handles 5-item tuples for hybrid\n",
    "            num_workers=HP.get('dataloader_num_workers', 0),\n",
    "            persistent_workers=(HP.get('dataloader_num_workers',0) > 0) if HP.get('dataloader_num_workers',0) > 0 else False\n",
    "        )\n",
    "        print(\"Train and Validation DataLoaders for hybrid model created.\")\n",
    "        print(f\"  Number of batches in train_loader: ~{len(train_loader)}\")\n",
    "        print(f\"  Number of batches in val_loader: ~{len(val_loader)}\")\n",
    "\n",
    "        # Log dataset info to W&B for this training run\n",
    "        if run:\n",
    "            run.log({\n",
    "                f'dataset_info_{DATASET_IDENTIFIER}/train_subjects_hybrid': num_train_subjects,\n",
    "                f'dataset_info_{DATASET_IDENTIFIER}/val_subjects_hybrid': num_val_subjects,\n",
    "                f'dataset_info_{DATASET_IDENTIFIER}/tabular_input_size_for_model': HP['tabular_input_size'],\n",
    "                f'dataset_info_{DATASET_IDENTIFIER}/cnn_output_features_as_mri_lstm_input': HP['cnn_output_features'],\n",
    "                f'dataset_info_{DATASET_IDENTIFIER}/batch_size_hybrid': HP['batch_size']\n",
    "            })\n",
    "        \n",
    "        print(f\"\\nInput sizes for ModularLateFusionLSTM:\")\n",
    "        print(f\"  Tabular Input Size (num_tabular_features): {HP['tabular_input_size']}\")\n",
    "        print(f\"  CNN Input Channels: {HP['cnn_input_channels']}\")\n",
    "        print(f\"  CNN Output Features (becomes MRI LSTM input_size): {HP['cnn_output_features']}\")\n",
    "        if HP['tabular_input_size'] == 0 or HP['cnn_input_channels'] == 0 or HP['cnn_output_features'] == 0 :\n",
    "            print(\"WARNING: One of the key input sizes for the model is 0. Model instantiation might fail or behave unexpectedly.\")\n",
    "\n",
    "    except FileNotFoundError as e_fnf_ds_nb07:\n",
    "        print(f\"CRITICAL ERROR during OASISDataset instantiation in NB07: File not found - {e_fnf_ds_nb07}\")\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "    except KeyError as e_key_ds_nb07:\n",
    "        print(f\"CRITICAL ERROR during OASISDataset instantiation in NB07: Missing key in 'config_for_dataset' - {e_key_ds_nb07}\")\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "    except Exception as e_ds_init_nb07:\n",
    "        print(f\"An unexpected CRITICAL ERROR occurred during OASISDataset or DataLoader instantiation in NB07: {e_ds_init_nb07}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "\n",
    "# Final check for subsequent cells\n",
    "if train_loader is None or val_loader is None:\n",
    "    print(\"CRITICAL ERROR: DataLoaders were not successfully created. Cannot proceed with model training.\")\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6010fa1",
   "metadata": {},
   "source": [
    "## 6. Define Hybrid Model, Loss Function, and Optimizer\n",
    "\n",
    "This section sets up the components for training the `ModularLateFusionLSTM` model:\n",
    "1.  **Model Instantiation:** The `ModularLateFusionLSTM` (imported from `src/models.py`) is instantiated. It uses hyperparameters from the `HP` dictionary, including the `tabular_input_size` and CNN parameters (`cnn_input_channels`, `cnn_output_features`) which were dynamically determined from the Notebook 04 configuration. It also uses LSTM specific parameters like hidden sizes, number of layers, dropout, and the `modality_dropout_rate`. The model is then moved to the configured PyTorch `device`.\n",
    "2.  **Loss Function:** `nn.MSELoss` is used for this regression task.\n",
    "3.  **Optimizer:** `optim.Adam` is selected.\n",
    "4.  **W&B Model Watching:** `wandb.watch()` is called to monitor model gradients and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Hybrid Model, Loss Function, and Optimizer ---\n",
    "print(\"\\n--- Defining Hybrid Model (ModularLateFusionLSTM), Loss Function, and Optimizer ---\")\n",
    "\n",
    "# These variables are expected from previous cells:\n",
    "# HP (dictionary with all necessary HPs for ModularLateFusionLSTM)\n",
    "# device (torch.device)\n",
    "# ModularLateFusionLSTM (imported model class)\n",
    "# run (active W&B run object for NB07, or None)\n",
    "\n",
    "model: ModularLateFusionLSTM | None = None # Type hint for clarity\n",
    "criterion: nn.Module | None = None\n",
    "optimizer: optim.Optimizer | None = None\n",
    "\n",
    "# Ensure HP dictionary and critical model parameters are available\n",
    "required_hp_keys_hybrid = [\n",
    "    'tabular_input_size', 'cnn_input_channels', 'cnn_output_features',\n",
    "    'mri_lstm_hidden_size', 'tabular_lstm_hidden_size', 'num_lstm_layers',\n",
    "    'lstm_dropout_prob', 'learning_rate' \n",
    "    # 'modality_dropout_rate' is optional (defaults to 0.0 in model __init__)\n",
    "]\n",
    "if 'HP' not in locals() or not isinstance(HP, dict) or \\\n",
    "   not all(HP.get(key) is not None for key in required_hp_keys_hybrid): # Check if all required HPs are set\n",
    "    missing_keys_str = [key for key in required_hp_keys_hybrid if HP.get(key) is None]\n",
    "    print(f\"CRITICAL ERROR: HP dictionary is not defined or missing one or more critical keys for ModularLateFusionLSTM: {missing_keys_str}\")\n",
    "    print(\"Ensure these were correctly set after fetching NB04 config and defining HPs for NB07.\")\n",
    "    if run: run.finish(exit_code=1)\n",
    "    # exit() \n",
    "else:\n",
    "    try:\n",
    "        # 1. Instantiate the ModularLateFusionLSTM model\n",
    "        print(f\"  Instantiating ModularLateFusionLSTM with:\")\n",
    "        print(f\"    Tabular Input Size: {HP['tabular_input_size']}\")\n",
    "        print(f\"    CNN Input Channels: {HP['cnn_input_channels']}\")\n",
    "        print(f\"    CNN Output Features (MRI LSTM Input): {HP['cnn_output_features']}\")\n",
    "        print(f\"    MRI LSTM Hidden Size: {HP.get('mri_lstm_hidden_size', HP.get('lstm_hidden_size', 128))}\") # Fallback\n",
    "        print(f\"    Tabular LSTM Hidden Size: {HP.get('tabular_lstm_hidden_size', HP.get('lstm_hidden_size', 128))}\")\n",
    "        print(f\"    Num LSTM Layers: {HP['num_lstm_layers']}\")\n",
    "        print(f\"    LSTM Dropout: {HP['lstm_dropout_prob']}\")\n",
    "        print(f\"    Modality Dropout Rate: {HP.get('modality_dropout_rate', 0.0)}\")\n",
    "        \n",
    "        model = ModularLateFusionLSTM(\n",
    "            cnn_input_channels=HP['cnn_input_channels'],\n",
    "            cnn_output_features=HP['cnn_output_features'],\n",
    "            tabular_input_size=HP['tabular_input_size'],\n",
    "            mri_lstm_hidden_size=HP.get('mri_lstm_hidden_size', HP.get('lstm_hidden_size')), # Use .get for safety\n",
    "            tabular_lstm_hidden_size=HP.get('tabular_lstm_hidden_size', HP.get('lstm_hidden_size')),\n",
    "            num_lstm_layers=HP['num_lstm_layers'],\n",
    "            lstm_dropout_prob=HP['lstm_dropout_prob'],\n",
    "            modality_dropout_rate=HP.get('modality_dropout_rate', 0.0), # Defaults to 0.0 if not in HP\n",
    "            num_classes=1 # For regression of CDR score\n",
    "        )\n",
    "        model.to(device) # Move model to the selected device\n",
    "        print(f\"  ModularLateFusionLSTM model instantiated and moved to device: {device}.\")\n",
    "        # print(model) # Optional: print model architecture\n",
    "\n",
    "        # 2. Define the Loss Function (Criterion)\n",
    "        criterion = nn.MSELoss()\n",
    "        print(f\"  Loss function set to: {type(criterion).__name__}\")\n",
    "\n",
    "        # 3. Define the Optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr=HP['learning_rate'])\n",
    "        print(f\"  Optimizer set to: {type(optimizer).__name__} with Learning Rate = {HP['learning_rate']}\")\n",
    "\n",
    "        # 4. Optional: Watch model with W&B\n",
    "        if run:\n",
    "            try:\n",
    "                wandb.watch(model, criterion=criterion, log='all', log_freq=100) \n",
    "                print(\"  W&B model watching enabled (gradients, parameters, etc.).\")\n",
    "            except Exception as e_watch_hybrid:\n",
    "                print(f\"  Warning: wandb.watch() for hybrid model failed. Error: {e_watch_hybrid}\")\n",
    "\n",
    "    except KeyError as e_key_model_hybrid_init:\n",
    "        print(f\"CRITICAL ERROR: Missing a key in HP dictionary required for ModularLateFusionLSTM: {e_key_model_hybrid_init}\")\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "    except Exception as e_model_setup_hybrid_generic:\n",
    "        print(f\"CRITICAL ERROR during hybrid model, loss, or optimizer setup: {e_model_setup_hybrid_generic}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "\n",
    "# Final check for subsequent cells\n",
    "if model is None or criterion is None or optimizer is None:\n",
    "    print(\"CRITICAL ERROR: Hybrid model, criterion, or optimizer was not successfully initialized. Cannot proceed.\")\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d2a69d",
   "metadata": {},
   "source": [
    "## 7. Train Hybrid Model using `train_model` Utility\n",
    "\n",
    "The core training and validation loop for the `ModularLateFusionLSTM` is executed by calling the `train_model` utility function from `src/training_utils.py`. This utility is passed the instantiated hybrid model, data loaders, criterion, optimizer, and other training parameters.\n",
    "\n",
    "**Crucially, `model_type_flag=\"hybrid\"` is passed to `train_model`.** This flag ensures that the utility correctly unpacks the 5-item batches (tabular sequences, MRI sequences, lengths, targets, masks) produced by `pad_collate_fn` for hybrid data and calls the hybrid model's `forward` method with the appropriate arguments.\n",
    "\n",
    "The `train_model` utility handles epoch iteration, metric calculation (via `evaluate_model`), W&B logging, best and periodic model checkpointing (saving `.pth` files locally and as W&B Artifacts), and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b457ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train Hybrid Model using the train_model Utility from src/training_utils.py ---\n",
    "print(\"\\n--- Starting Hybrid Model Training using 'train_model' utility ---\")\n",
    "\n",
    "# Prerequisites from previous cells:\n",
    "# model (ModularLateFusionLSTM instance), train_loader, val_loader, criterion, optimizer, \n",
    "# device, HP, run (W&B object), run_output_dir_for_checkpoints, evaluate_model (imported function)\n",
    "\n",
    "path_to_best_checkpoint_local_hybrid: Path | None = None \n",
    "best_validation_loss_achieved_hybrid: float = float('inf')\n",
    "\n",
    "# Check if all necessary components for calling train_model are ready\n",
    "training_prerequisites_met_hybrid = True\n",
    "required_vars_for_hybrid_training_call = [\n",
    "    'model', 'train_loader', 'val_loader', 'criterion', 'optimizer', 'device', 'HP',\n",
    "    'run_output_dir_for_checkpoints', 'evaluate_model'\n",
    "]\n",
    "for var_name_check in required_vars_for_hybrid_training_call:\n",
    "    if var_name_check not in locals() or locals()[var_name_check] is None:\n",
    "        if var_name_check == 'run' and ('run' not in locals() or locals()['run'] is None):\n",
    "            print(\"  Note: W&B run object is None. Training will proceed with logging disabled within train_model.\")\n",
    "        else:\n",
    "            print(f\"  CRITICAL ERROR: Prerequisite variable '{var_name_check}' for training hybrid model is not defined or is None.\")\n",
    "            training_prerequisites_met_hybrid = False\n",
    "\n",
    "if not training_prerequisites_met_hybrid:\n",
    "    print(\"Halting hybrid model training due to missing or invalid prerequisites for 'train_model' utility.\")\n",
    "    if run: run.finish(exit_code=1)\n",
    "    # exit()\n",
    "else:\n",
    "    try:\n",
    "        print(f\"  Calling 'train_model' for {HP['epochs']} epochs. Checkpoints: {run_output_dir_for_checkpoints}\")\n",
    "        \n",
    "        path_to_best_checkpoint_local_hybrid, best_validation_loss_achieved_hybrid = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            num_epochs=HP['epochs'],\n",
    "            wandb_run=run, \n",
    "            checkpoint_dir=run_output_dir_for_checkpoints, \n",
    "            evaluate_fn=evaluate_model, \n",
    "            model_type_flag=\"hybrid\",\n",
    "            hp_dict=HP, \n",
    "            best_val_loss_init=float('inf') \n",
    "        )\n",
    "\n",
    "        print(f\"\\n--- Hybrid Model Training Complete (via 'train_model' utility) ---\")\n",
    "        print(f\"Best validation loss achieved during training: {best_validation_loss_achieved_hybrid:.4f}\")\n",
    "        if path_to_best_checkpoint_local_hybrid and path_to_best_checkpoint_local_hybrid.exists():\n",
    "            print(f\"Best hybrid model checkpoint saved locally at: {path_to_best_checkpoint_local_hybrid}\")\n",
    "            if run:\n",
    "                run.summary['best_model_local_path_hybrid'] = str(path_to_best_checkpoint_local_hybrid)\n",
    "                run.summary['best_val_loss_final_hybrid'] = best_validation_loss_achieved_hybrid\n",
    "        elif path_to_best_checkpoint_local_hybrid:\n",
    "            print(f\"Warning: train_model returned best checkpoint path, but file not found: {path_to_best_checkpoint_local_hybrid}\")\n",
    "        else:\n",
    "            print(\"No best model checkpoint was saved for hybrid model (e.g., no improvement or training error).\")\n",
    "\n",
    "    except Exception as e_train_hybrid:\n",
    "        print(f\"CRITICAL ERROR occurred during hybrid model training: {e_train_hybrid}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba99f6f",
   "metadata": {},
   "source": [
    "## 8. Evaluate Best Hybrid Model on Test Set\n",
    "\n",
    "Following training, the best `ModularLateFusionLSTM` model (based on the lowest validation loss) is loaded from its saved checkpoint. This model is then evaluated on the held-out test set (`cohort_test.parquet`) to assess its generalization performance.\n",
    "\n",
    "The `evaluate_model` utility is used with `model_name_for_batch_unpack=\"hybrid\"` to ensure correct data handling. Test metrics (Loss, MSE, MAE, R²) are calculated and logged to this W&B training run's summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83753016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate Best Hybrid Model on Test Set ---\n",
    "print(\"\\n--- Evaluating Best Hybrid Model on Test Set ---\")\n",
    "\n",
    "# Ensure prerequisites for test evaluation are available\n",
    "# path_to_best_checkpoint_local_hybrid, HP, config_for_dataset, device, TEST_DATA_PATH,\n",
    "# SCALER_PATH_FROM_ARTIFACT, IMPUTER_PATH_FROM_ARTIFACT, MRI_DATA_DIR, criterion\n",
    "\n",
    "if 'path_to_best_checkpoint_local_hybrid' in locals() and \\\n",
    "   path_to_best_checkpoint_local_hybrid is not None and \\\n",
    "   path_to_best_checkpoint_local_hybrid.is_file() and \\\n",
    "   'HP' in locals() and HP and \\\n",
    "   HP.get('tabular_input_size') is not None and \\\n",
    "   HP.get('cnn_input_channels') is not None and \\\n",
    "   HP.get('cnn_output_features') is not None and \\\n",
    "   'config_for_dataset' in locals() and config_for_dataset and \\\n",
    "   'TEST_DATA_PATH' in locals() and TEST_DATA_PATH.is_file() and \\\n",
    "   'SCALER_PATH_FROM_ARTIFACT' in locals() and SCALER_PATH_FROM_ARTIFACT.is_file() and \\\n",
    "   'IMPUTER_PATH_FROM_ARTIFACT' in locals() and IMPUTER_PATH_FROM_ARTIFACT.is_file() and \\\n",
    "   'MRI_DATA_DIR' in locals() and MRI_DATA_DIR.is_dir() and \\\n",
    "   'criterion' in locals() and criterion is not None and \\\n",
    "   'device' in locals() and device is not None:\n",
    "\n",
    "    print(f\"Loading best hybrid model for test evaluation from: {path_to_best_checkpoint_local_hybrid}\")\n",
    "\n",
    "    try:\n",
    "        # 1. Instantiate a new ModularLateFusionLSTM model with the same HPs\n",
    "        test_model_hybrid = ModularLateFusionLSTM(\n",
    "            cnn_input_channels=HP['cnn_input_channels'],\n",
    "            cnn_output_features=HP['cnn_output_features'],\n",
    "            tabular_input_size=HP['tabular_input_size'],\n",
    "            mri_lstm_hidden_size=HP.get('mri_lstm_hidden_size', HP['lstm_hidden_size']),\n",
    "            tabular_lstm_hidden_size=HP.get('tabular_lstm_hidden_size', HP['lstm_hidden_size']),\n",
    "            num_lstm_layers=HP['num_lstm_layers'],\n",
    "            lstm_dropout_prob=HP['lstm_dropout_prob'],\n",
    "            modality_dropout_rate=0.0, # Crucial: Set to 0 for deterministic test evaluation\n",
    "            num_classes=1\n",
    "        )\n",
    "        \n",
    "        # Load the saved state dictionary of the best model\n",
    "        # The checkpoint saved by train_model utility contains only model.state_dict()\n",
    "        test_model_hybrid.load_state_dict(torch.load(path_to_best_checkpoint_local_hybrid, map_location=device))\n",
    "        test_model_hybrid.to(device)\n",
    "        test_model_hybrid.eval() \n",
    "        print(\"  Best hybrid model loaded and set to evaluation mode.\")\n",
    "\n",
    "        # 2. Create DataLoader for the Test Set (with MRI data)\n",
    "        print(f\"  Instantiating test dataset from: {TEST_DATA_PATH} (Hybrid - MRI Included)...\")\n",
    "        test_dataset_hybrid = OASISDataset(\n",
    "            data_parquet_path=TEST_DATA_PATH,\n",
    "            scaler_path=SCALER_PATH_FROM_ARTIFACT,\n",
    "            imputer_path=IMPUTER_PATH_FROM_ARTIFACT,\n",
    "            config=config_for_dataset, # Use the same authoritative config from NB04\n",
    "            mri_data_dir=MRI_DATA_DIR,\n",
    "            include_mri=True # Explicitly True for the hybrid model\n",
    "        )\n",
    "        num_test_subjects_hybrid = len(test_dataset_hybrid)\n",
    "        print(f\"  Test dataset (hybrid) created with {num_test_subjects_hybrid} subjects.\")\n",
    "\n",
    "        test_loader_hybrid = DataLoader(\n",
    "            test_dataset_hybrid,\n",
    "            batch_size=HP['batch_size'], \n",
    "            shuffle=False, \n",
    "            collate_fn=pad_collate_fn, # Will yield 5-item tuples\n",
    "            num_workers=HP.get('dataloader_num_workers', 0)\n",
    "        )\n",
    "        print(f\"  Test DataLoader (hybrid) created. Number of batches: ~{len(test_loader_hybrid)}\")\n",
    "\n",
    "        # 3. Perform Evaluation using the utility function\n",
    "        if 'criterion' not in locals() or criterion is None: \n",
    "            criterion = nn.MSELoss() # Ensure criterion is defined\n",
    "            print(\"  Re-initialized criterion to MSELoss for test evaluation.\")\n",
    "\n",
    "        print(f\"  Evaluating best hybrid model on {num_test_subjects_hybrid} test subjects...\")\n",
    "        test_set_metrics_dict_hybrid = evaluate_model(\n",
    "            test_model_hybrid, \n",
    "            test_loader_hybrid, \n",
    "            criterion, \n",
    "            device, \n",
    "            model_name_for_batch_unpack=\"hybrid\"\n",
    "        )\n",
    "\n",
    "        print(f\"\\n--- Test Set Performance of Best Hybrid Model ---\")\n",
    "        if 'best_validation_loss_achieved_hybrid' in locals():\n",
    "             print(f\"  (Model achieved best validation loss: {best_validation_loss_achieved_hybrid:.4f})\")\n",
    "        \n",
    "        print(f\"  Test Loss (Criterion): {test_set_metrics_dict_hybrid.get('loss', float('nan')):.4f}\")\n",
    "        print(f\"  Test MSE (from preds): {test_set_metrics_dict_hybrid.get('mse', float('nan')):.4f}\")\n",
    "        print(f\"  Test MAE:  {test_set_metrics_dict_hybrid.get('mae', float('nan')):.4f}\")\n",
    "        print(f\"  Test R2:   {test_set_metrics_dict_hybrid.get('r2', float('nan')):.4f}\")\n",
    "\n",
    "        # 4. Log Test Metrics to W&B Summary\n",
    "        if run:\n",
    "            run.summary[\"test_set_hybrid/loss_criterion\"] = test_set_metrics_dict_hybrid.get('loss')\n",
    "            run.summary[\"test_set_hybrid/mse_from_preds\"] = test_set_metrics_dict_hybrid.get('mse')\n",
    "            run.summary[\"test_set_hybrid/mae\"] = test_set_metrics_dict_hybrid.get('mae')\n",
    "            run.summary[\"test_set_hybrid/r2\"] = test_set_metrics_dict_hybrid.get('r2')\n",
    "            if 'best_validation_loss_achieved_hybrid' in locals():\n",
    "                run.summary[\"test_set_hybrid/checkpoint_achieved_best_val_loss\"] = best_validation_loss_achieved_hybrid\n",
    "            run.summary[\"test_set_hybrid/num_test_subjects\"] = num_test_subjects_hybrid\n",
    "            run.summary[\"test_set_hybrid/num_test_visits\"] = len(test_dataset_hybrid.data_df) if hasattr(test_dataset_hybrid, 'data_df') else 'N/A'\n",
    "            print(\"  Hybrid model test metrics logged to W&B run summary.\")\n",
    "            \n",
    "    except Exception as e_test_eval_hybrid:\n",
    "        print(f\"CRITICAL ERROR during hybrid model test set evaluation: {e_test_eval_hybrid}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if run: run.summary[\"test_set_hybrid/status\"] = f\"EvaluationError: {str(e_test_eval_hybrid)[:100]}\"\n",
    "\n",
    "else:\n",
    "    print(\"Skipping hybrid model test set evaluation: Best model checkpoint not found/valid, \"\n",
    "          \"or other prerequisite variables are missing.\")\n",
    "    if run: run.summary[\"test_set_hybrid/status\"] = \"SkippedMissingPrerequisites\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10847ae7",
   "metadata": {},
   "source": [
    "## 9. Finalize W&B Run\n",
    "\n",
    "Complete the execution of this hybrid model training notebook and finish the associated Weights & Biases run. This ensures all queued logs, metrics, configurations, and model artifacts (including the best model checkpoint and any periodic checkpoints) are fully uploaded and synchronized with the W&B platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411b7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Finish W&B Run for Notebook 07 (Hybrid Model Training) ---\n",
    "print(f\"\\n--- {NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER} (Hybrid Training) complete. Finishing W&B run. ---\")\n",
    "\n",
    "if run: # Check if 'run' object exists and is an active W&B run\n",
    "    try:\n",
    "        # Ensure final best_val_loss is in summary (train_model utility also logs this if W&B active)\n",
    "        if 'best_validation_loss_achieved_hybrid' in locals() and 'best_val_loss_final_hybrid' not in run.summary : # Check if already logged by train_model more directly\n",
    "            run.summary['best_val_loss_final_hybrid'] = best_validation_loss_achieved_hybrid\n",
    "        \n",
    "        run.finish()\n",
    "        run_name_to_print = run.name_synced if hasattr(run, 'name_synced') and run.name_synced else \\\n",
    "                            run.name if hasattr(run, 'name') and run.name else \\\n",
    "                            run.id if hasattr(run, 'id') else \"current NB07 run\"\n",
    "        print(f\"W&B run '{run_name_to_print}' finished successfully.\")\n",
    "    except Exception as e_finish_run_nb07:\n",
    "        print(f\"Error during wandb.finish() for Notebook 07: {e_finish_run_nb07}\")\n",
    "        print(\"The run may not have finalized correctly on the W&B server.\")\n",
    "else:\n",
    "    print(\"No active W&B run to finish for this training session.\")\n",
    "\n",
    "print(f\"\\n--- Notebook {NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER} execution finished. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro_predcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
