{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: notebooks/04_Fit_Preprocessors.ipynb\n",
    "# Purpose: Load the TRAINING split data, fit data scalers (StandardScaler)\n",
    "#          and imputers (SimpleImputer) based ONLY on this training data,\n",
    "#          and save these fitted objects for later use in the Dataset class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Notebook 04: Fit Preprocessors\n",
    "\n",
    "**Purpose:** This notebook prepares the data preprocessing components required for training the model. It performs the following critical steps:\n",
    "\n",
    "1.  **Load Training Data:** Loads *only* the training data split (`cohort_train.parquet`) created in Notebook 03. This is crucial to prevent data leakage from validation/test sets into the preprocessing steps.\n",
    "2.  **Identify Preprocessing Columns:** Analyzes the training data to determine which columns require imputation (due to missing values) and which numerical columns should be scaled.\n",
    "3.  **Log Configuration:** Logs the identified column lists and the chosen preprocessing strategies (e.g., median imputation, standard scaling) to the Weights & Biases (W&B) configuration for this run. This configuration will be loaded by subsequent notebooks (like NB05, NB06) to ensure consistent feature handling.\n",
    "4.  **Fit Preprocessors:** Initializes and fits the chosen imputer (`SimpleImputer`) and scaler (`StandardScaler`) using **only** the training data.\n",
    "5.  **Save & Log Preprocessors:** Saves the *fitted* imputer and scaler objects locally as `.joblib` files and logs them as versioned artifacts to W&B. This allows later notebooks/scripts to load these exact fitted objects to apply the *same* imputation and scaling transformations consistently to training, validation, and test data.\n",
    "\n",
    "**Input:**\n",
    "* `cohort_train.parquet` (Output from NB 03)\n",
    "* `config.json` (For base paths and W&B details)\n",
    "\n",
    "**Output:**\n",
    "* `simple_imputer_median.joblib` (Saved locally and as W&B artifact)\n",
    "* `standard_scaler.joblib` (Saved locally and as W&B artifact)\n",
    "* Updated W&B Run Configuration (Includes `preprocess` and `features` lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "import joblib # For saving sklearn objects\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config Loading ---\n",
    "print(\"--- Loading Configuration ---\")\n",
    "CONFIG_PATH = Path('../config.json') # Path relative to the notebook location\n",
    "try:\n",
    "    PROJECT_ROOT = CONFIG_PATH.parent.resolve()\n",
    "    print(f\"Project Root detected as: {PROJECT_ROOT}\")\n",
    "    with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    print(\"Configuration loaded successfully.\")\n",
    "\n",
    "    # Define key variables from config\n",
    "    OUTPUT_DIR_BASE = PROJECT_ROOT / config['data']['output_dir_base']\n",
    "    WANDB_PROJECT = config['wandb']['project_name']\n",
    "    WANDB_ENTITY = config['wandb'].get('entity', None)\n",
    "\n",
    "    # Define input path (output from Notebook 03)\n",
    "    NB03_OUTPUT_DIR = OUTPUT_DIR_BASE / \"03_Feature_Engineering_Splitting\"\n",
    "    TRAIN_DATA_PATH = NB03_OUTPUT_DIR / \"cohort_train.parquet\" # Load ONLY training data\n",
    "\n",
    "    # Define specific output dir for this notebook's results (preprocessors) and create it\n",
    "    NOTEBOOK_NAME = \"04_Fit_Preprocessors\"\n",
    "    output_dir = OUTPUT_DIR_BASE / NOTEBOOK_NAME\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Fitted preprocessor objects will be saved to: {output_dir}\")\n",
    "\n",
    "    # Get planned features from config if logged, otherwise define defaults\n",
    "    # This assumes NB 03 logged these to config - might need manual definition otherwise\n",
    "    # For robustness, define them here based on NB 03 plan, checking columns later\n",
    "    planned_time_varying = ['Age', 'MMSE', 'nWBV', 'Days_from_Baseline', 'Time_since_Last_Visit_Days']\n",
    "    planned_static = ['M/F', 'EDUC', 'SES', 'Baseline_CDR', 'Baseline_MMSE', 'eTIV', 'ASF']\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading config or setting up paths: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize W&B Run ---\n",
    "print(\"\\n--- Initializing Weights & Biases Run ---\")\n",
    "run = None # Initialize run to None\n",
    "try:\n",
    "    run = wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        entity=WANDB_ENTITY,\n",
    "        job_type=\"fit-preprocessors\", # New job type\n",
    "        name=f\"{NOTEBOOK_NAME}-run-{time.strftime('%Y%m%d-%H%M')}\",\n",
    "        config={ # Log key config choices for this job\n",
    "            \"train_data_artifact\": f\"cohort-split-train:latest\", # Link to input artifact\n",
    "            \"train_data_path\": str(TRAIN_DATA_PATH),\n",
    "            \"output_dir\": str(output_dir),\n",
    "            # Imputation/Scaling strategies will be added\n",
    "        }\n",
    "    )\n",
    "    print(f\"W&B run '{run.name}' initialized successfully. View at: {run.url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing W&B: {e}\")\n",
    "    print(\"Proceeding without W&B logging.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Load Training Data Split\n",
    "\n",
    "Load the training dataset (`cohort_train.parquet`) generated in the previous notebook (NB 03). All subsequent fitting steps will use **only** this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- Loading TRAINING Data from: {TRAIN_DATA_PATH} ---\")\n",
    "try:\n",
    "    if not TRAIN_DATA_PATH.is_file():\n",
    "         raise FileNotFoundError(f\"Training data file not found at {TRAIN_DATA_PATH}.\")\n",
    "    train_df = pd.read_parquet(TRAIN_DATA_PATH)\n",
    "    print(f\"Training data loaded successfully. Shape: {train_df.shape}\")\n",
    "    if run: run.log({'fit_preprocessors/input_train_rows': len(train_df)})\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    if run: run.finish()\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading the training data: {e}\")\n",
    "    if run: run.finish()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Identify Features for Preprocessing\n",
    "\n",
    "Analyze the loaded training data (`train_df`) to:\n",
    "1.  Identify columns with missing values that require imputation (e.g., SES, MMSE, nWBV).\n",
    "2.  Identify numerical columns suitable for scaling (excluding identifiers, targets, and potentially already encoded categoricals).\n",
    "Define the strategies to be used (e.g., median imputation, standard scaling) and log these choices to W&B config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Defining Features for Imputation & Scaling ---\")\n",
    "\n",
    "# Identify columns actually present in the loaded data\n",
    "available_cols = train_df.columns.tolist()\n",
    "\n",
    "# Define columns needing imputation (based on previous EDA and plan)\n",
    "# Check missing values specifically in train_df now\n",
    "missing_in_train = train_df.isnull().sum()\n",
    "print(\"\\nMissing values in Training Set:\")\n",
    "print(missing_in_train[missing_in_train > 0])\n",
    "\n",
    "imputation_cols = []\n",
    "if 'SES' in available_cols and missing_in_train.get('SES', 0) > 0:\n",
    "    imputation_cols.append('SES')\n",
    "if 'MMSE' in available_cols and missing_in_train.get('MMSE', 0) > 0:\n",
    "    imputation_cols.append('MMSE')\n",
    "if 'nWBV' in available_cols and missing_in_train.get('nWBV', 0) > 0:\n",
    "    imputation_cols.append('nWBV')\n",
    "# Add EDUC, Baseline_MMSE etc. if they show missing values and need imputation\n",
    "if 'EDUC' in available_cols and missing_in_train.get('EDUC', 0) > 0:\n",
    "     imputation_cols.append('EDUC')\n",
    "if 'Baseline_MMSE' in available_cols and missing_in_train.get('Baseline_MMSE', 0) > 0:\n",
    "     imputation_cols.append('Baseline_MMSE')\n",
    "\n",
    "print(f\"\\nColumns selected for imputation: {imputation_cols}\")\n",
    "\n",
    "# Define numerical columns needing scaling (identifiers, target, categorical excluded)\n",
    "# Combine planned time-varying and static, filter based on availability & numeric type\n",
    "potential_scaling_cols = planned_time_varying + planned_static\n",
    "scaling_cols = [\n",
    "    col for col in potential_scaling_cols\n",
    "    if col in available_cols and pd.api.types.is_numeric_dtype(train_df[col]) and col not in ['M/F'] # Exclude Sex explicitly if numeric coding used\n",
    "]\n",
    "# Remove target base if present, just to be safe (target handled separately)\n",
    "if 'CDR' in scaling_cols: scaling_cols.remove('CDR')\n",
    "\n",
    "print(f\"Columns selected for scaling: {scaling_cols}\")\n",
    "\n",
    "print(\"\\n--- Finalizing and Logging Feature Lists for Model Input ---\")\n",
    "\n",
    "# Define final lists based on columns intended for the model AFTER preprocessing\n",
    "# Time-varying: Typically the scaled numeric time-varying columns\n",
    "final_time_varying = [col for col in planned_time_varying if col in scaling_cols]\n",
    "# Add any other processed time-varying features if they exist\n",
    "\n",
    "# Static: Scaled numeric static columns + encoded categorical static columns\n",
    "final_static_numeric = [col for col in planned_static if col in scaling_cols]\n",
    "final_static_categorical = []\n",
    "if 'M/F' in available_cols: # If M/F is present and needs encoding\n",
    "    final_static_categorical.append('M/F_encoded') # Assuming you create this name later\n",
    "\n",
    "# Baseline CDR might not be scaled, but should be included\n",
    "if 'Baseline_CDR' in available_cols and 'Baseline_CDR' not in scaling_cols:\n",
    "     final_static_numeric.append('Baseline_CDR') # Add if present but not scaled\n",
    "\n",
    "final_static = sorted(list(set(final_static_numeric + final_static_categorical))) # Ensure unique and sort\n",
    "\n",
    "print(f\"Final Time-Varying Features planned: {final_time_varying}\")\n",
    "print(f\"Final Static Features planned: {final_static}\")\n",
    "\n",
    "if run:\n",
    "    wandb.config.update({\n",
    "        'preprocess': {\n",
    "            'imputation_cols': imputation_cols,\n",
    "            'scaling_cols': scaling_cols,\n",
    "            'imputation_strategy': 'median',\n",
    "            'scaling_strategy': 'StandardScaler'\n",
    "        },\n",
    "        'features': { # Log the final lists expected by the model\n",
    "            'time_varying': final_time_varying,\n",
    "            'static': final_static\n",
    "        }\n",
    "    }, allow_val_change=True)\n",
    "    print(\"Final preprocess and features configuration logged to W&B.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Fit and Save Imputer(s)\n",
    "\n",
    "Based on the columns identified above, initialize and fit the chosen imputer (e.g., `SimpleImputer(strategy='median')`) using **only** the training data. Save the *fitted* imputer object locally (e.g., using `joblib`) and log it as a versioned artifact to W&B. This ensures the exact imputation learned from the training data can be applied consistently later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Fitting and Saving Imputer(s) ---\")\n",
    "\n",
    "imputer = None\n",
    "imputer_path = output_dir / \"simple_imputer_median.joblib\"\n",
    "\n",
    "if imputation_cols:\n",
    "    try:\n",
    "        print(f\"Fitting Median Imputer for columns: {imputation_cols}\")\n",
    "        # Use median strategy - generally robust to outliers\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        imputer.fit(train_df[imputation_cols])\n",
    "        print(\"Imputer fitted successfully.\")\n",
    "\n",
    "        # Save the fitted imputer\n",
    "        joblib.dump(imputer, imputer_path)\n",
    "        print(f\"Fitted imputer saved locally to: {imputer_path}\")\n",
    "\n",
    "        # Log artifact to W&B\n",
    "        if run:\n",
    "            print(\"Logging imputer as W&B artifact...\")\n",
    "            artifact_name = 'simple_imputer_median'\n",
    "            artifact_type = 'preprocessor'\n",
    "            description = f\"SimpleImputer(strategy='median') fitted on training data columns: {imputation_cols}\"\n",
    "            imputer_artifact = wandb.Artifact(artifact_name, type=artifact_type, description=description)\n",
    "            imputer_artifact.add_file(str(imputer_path))\n",
    "            run.log_artifact(imputer_artifact)\n",
    "            print(\"Imputer artifact logged.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting or saving imputer: {e}\")\n",
    "        imputer = None # Ensure imputer is None if saving failed\n",
    "else:\n",
    "    print(\"No columns identified for imputation based on missing values in training set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Fit and Save Scaler(s)\n",
    "\n",
    "Initialize and fit the chosen scaler (e.g., `StandardScaler`) on the specified numerical columns using **only** the training data (potentially after imputation if the same columns needed both). Save the *fitted* scaler object locally and log it as a versioned artifact to W&B. This captures the mean and standard deviation from the training data for consistent scaling later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Fitting and Saving Scaler(s) ---\")\n",
    "\n",
    "scaler = None\n",
    "scaler_path = output_dir / \"standard_scaler.joblib\"\n",
    "\n",
    "if scaling_cols:\n",
    "    try:\n",
    "        print(f\"Fitting StandardScaler for columns: {scaling_cols}\")\n",
    "        scaler = StandardScaler()\n",
    "        # Fit scaler ONLY on the training data\n",
    "        scaler.fit(train_df[scaling_cols])\n",
    "        print(\"Scaler fitted successfully.\")\n",
    "\n",
    "        # Save the fitted scaler\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        print(f\"Fitted scaler saved locally to: {scaler_path}\")\n",
    "\n",
    "        # Log artifact to W&B\n",
    "        if run:\n",
    "            print(\"Logging scaler as W&B artifact...\")\n",
    "            artifact_name = 'standard_scaler'\n",
    "            artifact_type = 'preprocessor'\n",
    "            description = f\"StandardScaler fitted on training data columns: {scaling_cols}\"\n",
    "            scaler_artifact = wandb.Artifact(artifact_name, type=artifact_type, description=description)\n",
    "            scaler_artifact.add_file(str(scaler_path))\n",
    "            run.log_artifact(scaler_artifact)\n",
    "            print(\"Scaler artifact logged.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting or saving scaler: {e}\")\n",
    "        scaler = None # Ensure scaler is None if saving failed\n",
    "else:\n",
    "    print(\"No columns identified for scaling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Finalize Run\n",
    "\n",
    "Finish the Weights & Biases run associated with fitting and saving the preprocessors. The saved `.joblib` files and W&B artifacts are now ready for use in the data loading pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Finish W&B Run ---\n",
    "print(\"\\n--- Preprocessor Fitting complete. Finishing W&B run. ---\")\n",
    "if run:\n",
    "    run.finish()\n",
    "    print(\"W&B run finished.\")\n",
    "else:\n",
    "    print(\"No active W&B run to finish.\")\n",
    "\n",
    "print(\"\\nScript execution finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro_pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
