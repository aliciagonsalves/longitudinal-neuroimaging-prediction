{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f48903b",
   "metadata": {},
   "source": [
    "# Notebook 04: Fit and Save Data Preprocessors (OASIS-2)\n",
    "\n",
    "**Project Phase:** 1 (Data Processing - Preprocessing)\n",
    "**Dataset:** OASIS-2 Longitudinal MRI & Clinical Data\n",
    "\n",
    "**Purpose:**\n",
    "This notebook is a critical step in preparing data for model training. It focuses on fitting data preprocessors (imputer and scaler) using *only* the training dataset split to prevent data leakage. The objectives are:\n",
    "\n",
    "1.  **Use Training Data Artifact:** Consume the versioned `cohort_train_oasis2` W&B Artifact (produced by Notebook 03). This provides the exact training data split.\n",
    "2.  **Fetch Feature Configuration from Producer Run:** From the input training data artifact, identify the W&B run (Notebook 03) that produced it and fetch the definitive lists of `time_varying_features` and `static_features` from that run's configuration. This ensures consistency.\n",
    "3.  **Identify Preprocessing Columns:** Based on the loaded training data and the fetched feature lists, determine which specific columns require missing value imputation and which numerical columns are candidates for scaling.\n",
    "4.  **Log Definitive Preprocessing Configuration to W&B:** Log the chosen imputation/scaling strategies, and the *exact lists* of columns that will be imputed and scaled, along with the final time-varying and static feature lists that `OASISDataset` should use. This W&B run's (Notebook 04's) configuration becomes the **source of truth** for data preprocessing details for `OASISDataset`.\n",
    "5.  **Fit Preprocessors:** Initialize and fit `sklearn.impute.SimpleImputer` and `sklearn.preprocessing.StandardScaler` instances exclusively on the identified columns of the training data.\n",
    "6.  **Save & Log Preprocessors:** Save these *fitted* preprocessor objects locally (e.g., as `.joblib` files) in a structured output directory. Log these fitted objects as versioned W&B Artifacts.\n",
    "\n",
    "**Workflow:**\n",
    "1.  **Setup:** Import libraries, configure `sys.path`, load `config.json`.\n",
    "2.  **W&B Initialization:** Start a new W&B run for this preprocessor fitting task using `initialize_wandb_run`.\n",
    "3.  **Load Training Data via W&B Artifact:** Use `run.use_artifact()` to get `cohort_train_oasis2`, download it, and load `cohort_train.parquet`.\n",
    "4.  **Fetch Feature Config from NB03 Producer Run:** Get the run that produced the training data artifact using `artifact.logged_by()` and extract feature lists from its config.\n",
    "5.  **Determine Imputation & Scaling Columns:** Based on the fetched feature lists and analysis of the `train_df`.\n",
    "6.  **Log Final Preprocessing & Feature Config to Current W&B Run:** Update this Notebook 04 W&B run's config.\n",
    "7.  **Fit, Save, & Log Imputer Artifact.**\n",
    "8.  **Fit, Save, & Log Scaler Artifact.**\n",
    "9.  **Finalize W&B Run.**\n",
    "\n",
    "**Input:**\n",
    "* `config.json`: Main project configuration file.\n",
    "* **W&B Artifact Name for Training Data Split:** e.g., `\"cohort_split_train_oasis2:latest\"` (produced by Notebook 03).\n",
    "\n",
    "**Output:**\n",
    "* **Local Files (in designated preprocessors output directory defined by `config.json` and `paths_utils.py`):**\n",
    "    * `simple_imputer_median_oasis2.joblib` (or similar, based on strategy and dataset)\n",
    "    * `standard_scaler_oasis2.joblib` (or similar)\n",
    "* **W&B Run (for this Notebook 04 execution):**\n",
    "    * Logged run configuration containing:\n",
    "        * Name of the input training data W&B Artifact used.\n",
    "        * ID of the source Notebook 03 run that defined the features.\n",
    "        * **The definitive `features` (time-varying, static) and `preprocess` (imputation_cols, scaling_cols, imputation_strategy, scaling_strategy) dictionaries that all downstream `OASISDataset` instances will fetch and use from *this run's config*.**\n",
    "    * Fitted `SimpleImputer` and `StandardScaler` objects logged as new W&B Artifacts (e.g., `imputer-median-oasis2:latest`, `scaler-standard-oasis2:latest`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a3440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: notebooks/04_Fit_Preprocessors.ipynb\n",
    "# Purpose: Load the TRAINING split data, fit data scalers (StandardScaler)\n",
    "#          and imputers (SimpleImputer) based ONLY on this training data,\n",
    "#          and save these fitted objects for later use in the Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc42992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8767e289",
   "metadata": {},
   "source": [
    "## 1. Setup: Project Configuration, Paths, and Utilities\n",
    "\n",
    "This section initializes the notebook environment:\n",
    "* Determines the project's root directory and adds the `src` directory to `sys.path`.\n",
    "* Imports custom utilities for W&B run initialization and path resolution.\n",
    "* Loads the main project configuration (`config.json`).\n",
    "* Defines dataset and notebook-specific identifiers.\n",
    "* **Uses `get_dataset_paths` to resolve the input path for `cohort_train.parquet` (from Notebook 03) and the output directory where fitted preprocessors will be saved.** These paths are derived from `config.json` for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d4a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Project Setup, Configuration Loading, and Utility Imports ---\n",
    "print(\"--- Initializing Project Setup & Configuration for NB04 ---\")\n",
    "\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = None\n",
    "base_config = {}\n",
    "try:\n",
    "    current_notebook_path = Path.cwd() \n",
    "    potential_project_root = current_notebook_path.parent \n",
    "    if (potential_project_root / \"src\").is_dir() and (potential_project_root / \"config.json\").is_file():\n",
    "        PROJECT_ROOT = potential_project_root\n",
    "    else: \n",
    "        PROJECT_ROOT = current_notebook_path\n",
    "    if not (PROJECT_ROOT / \"src\").is_dir() or not (PROJECT_ROOT / \"config.json\").is_file():\n",
    "        raise FileNotFoundError(f\"Could not find 'src' or 'config.json'. PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "    if str(PROJECT_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    print(f\"PROJECT_ROOT: {PROJECT_ROOT}, added to sys.path.\")\n",
    "\n",
    "    from src.wandb_utils import initialize_wandb_run\n",
    "    # get_dataset_paths is used here mainly to determine where to SAVE preprocessors.\n",
    "    # The INPUT training data path will come from the downloaded W&B artifact.\n",
    "    from src.paths_utils import get_dataset_paths \n",
    "    print(\"Successfully imported custom utilities.\")\n",
    "except Exception as e_setup:\n",
    "    print(f\"CRITICAL ERROR during initial setup: {e_setup}\")\n",
    "    # exit()\n",
    "\n",
    "# --- Load Main Project Configuration ---\n",
    "print(\"\\n--- Loading Main Project Configuration ---\")\n",
    "try:\n",
    "    if PROJECT_ROOT is None: raise ValueError(\"PROJECT_ROOT not set.\")\n",
    "    CONFIG_PATH_MAIN = PROJECT_ROOT / 'config.json'\n",
    "    with open(CONFIG_PATH_MAIN, 'r', encoding='utf-8') as f:\n",
    "        base_config = json.load(f)\n",
    "    print(f\"Main project config loaded from: {CONFIG_PATH_MAIN}\")\n",
    "except Exception as e_cfg:\n",
    "    print(f\"CRITICAL ERROR loading main config.json: {e_cfg}\")\n",
    "    # exit() \n",
    "\n",
    "# --- Define Dataset, Notebook Specifics ---\n",
    "DATASET_IDENTIFIER = \"oasis2\" \n",
    "NOTEBOOK_MODULE_NAME = \"04_Fit_Preprocessors\"\n",
    "# Key from config.json locators for this notebook's *output preprocessor files*\n",
    "NB04_PREPROCESSORS_LOCATOR_KEY = \"preprocessors_subdir\" # Matches key in get_dataset_stage_paths\n",
    "\n",
    "# Path where preprocessor .joblib files will be saved.\n",
    "# This uses get_dataset_paths to find the preprocessors_subdir defined in config.\n",
    "output_dir_for_preprocessors = None\n",
    "try:\n",
    "    if not base_config: raise ValueError(\"base_config is empty.\")\n",
    "    # We call get_dataset_stage_paths primarily to get the preprocessor output paths.\n",
    "    # The 'stage' argument doesn't really change these particular paths, but let's use 'training'.\n",
    "    # This utility will resolve 'scaler_path' and 'imputer_path' fully.\n",
    "    # We'll take their parent as the output directory for this notebook.\n",
    "    pipeline_paths_for_nb04 = get_dataset_paths(\n",
    "        PROJECT_ROOT, base_config, DATASET_IDENTIFIER, stage=\"training\" \n",
    "    )\n",
    "    SCALER_SAVE_PATH = pipeline_paths_for_nb04.get('scaler_path')\n",
    "    IMPUTER_SAVE_PATH = pipeline_paths_for_nb04.get('imputer_path')\n",
    "\n",
    "    if not SCALER_SAVE_PATH or not IMPUTER_SAVE_PATH:\n",
    "        raise ValueError(\"Could not resolve scaler_path or imputer_path from paths utility.\")\n",
    "    \n",
    "    output_dir_for_preprocessors = SCALER_SAVE_PATH.parent # e.g., .../04_Fit_Preprocessors_oasis2/\n",
    "    output_dir_for_preprocessors.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nKey paths for Notebook 04 ({DATASET_IDENTIFIER}):\")\n",
    "    print(f\"  Output Directory for Preprocessor .joblib files: {output_dir_for_preprocessors}\")\n",
    "    print(f\"  Scaler will be saved as: {SCALER_SAVE_PATH.name}\")\n",
    "    print(f\"  Imputer will be saved as: {IMPUTER_SAVE_PATH.name}\")\n",
    "\n",
    "except Exception as e_paths_nb04:\n",
    "    print(f\"CRITICAL ERROR during path setup for NB04: {e_paths_nb04}\")\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7888ac9a",
   "metadata": {},
   "source": [
    "## 3. Initialize Weights & Biases Run for Notebook 04\n",
    "\n",
    "A new W&B run is initiated for this specific \"Fit Preprocessors\" task. This run will log:\n",
    "\n",
    "* **Input Artifact Consumed:**\n",
    "    * The name and version of the input **training data W&B Artifact** (e.g., `cohort_split_train_oasis2:latest`) that this notebook consumes. This artifact is the direct output of a previous Notebook 03 execution and contains the `cohort_train.parquet` file.\n",
    "* **Source Feature Configuration (from Producer Run):**\n",
    "    * The W&B Run ID and name of the specific Notebook 03 execution that **produced** the consumed training data artifact. This producer run's configuration (containing the definitive `features_prepared_in_nb03` lists) is fetched automatically via artifact lineage to ensure this notebook uses the correct feature definitions.\n",
    "* **Determined Preprocessing Details:**\n",
    "    * The exact lists of feature columns identified from the loaded training data for **imputation**.\n",
    "    * The exact lists of feature columns identified for **scaling**.\n",
    "    * The chosen strategies for imputation (e.g., 'median') and scaling (e.g., 'StandardScaler'), typically sourced from `config.json`.\n",
    "* **Definitive Configuration for `OASISDataset` (Critically Important):**\n",
    "    * The final, definitive **`features` dictionary** (detailing `time_varying` and `static` features, reflecting the lists fetched from the NB03 producer run and considering any encoding handled by `OASISDataset`).\n",
    "    * The final, definitive **`preprocess` dictionary** (detailing `imputation_cols`, `scaling_cols`, and the chosen `imputation_strategy` and `scaling_strategy`).\n",
    "    * These two dictionaries, logged to **this Notebook 04 W&B run's configuration**, will serve as the **authoritative source of truth** for the `OASISDataset` class in all subsequent data loading stages (i.e., in Notebooks 05, 06, 07, and 08).\n",
    "* **Fitted Preprocessor Artifacts:**\n",
    "    * The *fitted* imputer object (e.g., `SimpleImputer`) saved as a versioned W&B Artifact.\n",
    "    * The *fitted* scaler object (e.g., `StandardScaler`) saved as a versioned W&B Artifact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aeeb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize W&B Run for THIS Notebook Execution (NB04) ---\n",
    "print(\"\\n--- Initializing Weights & Biases Run for Notebook 04 ---\")\n",
    "\n",
    "# --- Define W&B Artifact Name for Input Training Data (Output from NB03) ---\n",
    "# This should match the artifact_name used in NB03 when logging cohort_split_train\n",
    "INPUT_TRAIN_DATA_ARTIFACT_NAME = f\"cohort_split_train_{DATASET_IDENTIFIER}\" \n",
    "INPUT_TRAIN_DATA_ARTIFACT_TYPE = f\"data_split_{DATASET_IDENTIFIER}\" # Matches type in NB03\n",
    "INPUT_TRAIN_DATA_ARTIFACT_VERSION = \"latest\" # Or a specific version like \"v0\"\n",
    "\n",
    "nb04_run_config_log = {\n",
    "    \"notebook_name_code\": f\"{NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER}\",\n",
    "    \"dataset_source\": DATASET_IDENTIFIER,\n",
    "    \"input_train_data_artifact\": f\"{INPUT_TRAIN_DATA_ARTIFACT_NAME}:{INPUT_TRAIN_DATA_ARTIFACT_VERSION}\",\n",
    "    \"output_dir_for_local_preprocessors\": str(output_dir_for_preprocessors),\n",
    "    \"scaler_save_filename\": SCALER_SAVE_PATH.name, # Log the intended save names\n",
    "    \"imputer_save_filename\": IMPUTER_SAVE_PATH.name,\n",
    "    \"execution_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    # Feature lists, imputation/scaling cols, and strategies will be logged after determination\n",
    "}\n",
    "\n",
    "nb_number_prefix_nb04 = NOTEBOOK_MODULE_NAME.split('_')[0] if '_' in NOTEBOOK_MODULE_NAME else \"NB\"\n",
    "job_specific_type_nb04 = f\"{nb_number_prefix_nb04}-FitPreprocessors-{DATASET_IDENTIFIER}\"\n",
    "custom_elements_for_name_nb04 = [nb_number_prefix_nb04, DATASET_IDENTIFIER.upper(), \"FitPreproc\"]\n",
    "\n",
    "run = initialize_wandb_run(\n",
    "    base_project_config=base_config,\n",
    "    job_group=\"DataProcessing\",\n",
    "    job_specific_type=job_specific_type_nb04,\n",
    "    run_specific_config=nb04_run_config_log,\n",
    "    custom_run_name_elements=custom_elements_for_name_nb04,\n",
    "    notes=f\"{DATASET_IDENTIFIER.upper()}: Fitting and saving data preprocessors based on training data artifact.\"\n",
    ")\n",
    "\n",
    "if run:\n",
    "    print(f\"W&B run '{run.name}' (Job Type: '{run.job_type}') initialized. View at: {run.url}\")\n",
    "else:\n",
    "    print(\"Proceeding without W&B logging for this session (W&B run initialization failed).\")\n",
    "    # output_dir_for_preprocessors should still be defined for local saves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab63ac6",
   "metadata": {},
   "source": [
    "## 4. Load Training Data Artifact & Fetch Feature Configuration from Producer Run\n",
    "\n",
    "This notebook consumes the training data split (`cohort_train_oasis2`) logged as an artifact by Notebook 03.\n",
    "1.  The specified W&B artifact is downloaded.\n",
    "2.  The `cohort_train.parquet` file is loaded from the downloaded artifact directory.\n",
    "3.  Crucially, we then identify the W&B Run (from Notebook 03) that *produced* this training data artifact.\n",
    "4.  The configuration of that producer run is fetched to retrieve the definitive lists of `time_varying_features` and `static_features` that were selected and prepared in Notebook 03. This ensures that preprocessors in this notebook are fitted using the exact feature set intended for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Training Data from W&B Artifact and Fetch Producer Run Config ---\n",
    "print(f\"\\n--- Loading Training Data Artifact & Fetching NB03 Feature Config ---\")\n",
    "train_df = None\n",
    "source_time_varying_features = []\n",
    "source_static_features = []\n",
    "source_nb03_run_id_for_config = \"N/A\" # For logging\n",
    "\n",
    "try:\n",
    "    if run is None: # If W&B init failed earlier\n",
    "        raise ConnectionError(\"W&B run not initialized. Cannot use W&B artifacts.\")\n",
    "\n",
    "    print(f\"Using input training data artifact: {INPUT_TRAIN_DATA_ARTIFACT_NAME}:{INPUT_TRAIN_DATA_ARTIFACT_VERSION}\")\n",
    "    # Use the artifact\n",
    "    train_data_artifact = run.use_artifact(\n",
    "        f\"{INPUT_TRAIN_DATA_ARTIFACT_NAME}:{INPUT_TRAIN_DATA_ARTIFACT_VERSION}\", \n",
    "        type=INPUT_TRAIN_DATA_ARTIFACT_TYPE\n",
    "    )\n",
    "    train_data_artifact_dir = Path(train_data_artifact.download())\n",
    "    # Construct path to the parquet file within the artifact directory\n",
    "    # The filename within the artifact was defined in NB03 when artifact.add_file() was called.\n",
    "    # It should match locators.get(\"train_data_fname\", \"cohort_train.parquet\") from NB03's config.\n",
    "    # Let's assume a consistent naming or get it from artifact metadata if logged by NB03.\n",
    "    # For now, assume it's the default name from pipeline_artefact_locators.\n",
    "    locators = base_config.get(f\"pipeline_artefact_locators_{DATASET_IDENTIFIER}\", {})\n",
    "    train_fname_in_artifact = locators.get(\"train_data_fname\", f\"cohort_train_{DATASET_IDENTIFIER}.parquet\") # Match NB03 save\n",
    "    \n",
    "    TRAIN_DATA_PATH_FROM_ARTIFACT = train_data_artifact_dir / train_fname_in_artifact\n",
    "    \n",
    "    if not TRAIN_DATA_PATH_FROM_ARTIFACT.is_file():\n",
    "        raise FileNotFoundError(f\"Training data parquet file '{train_fname_in_artifact}' not found in downloaded artifact at {train_data_artifact_dir}\")\n",
    "\n",
    "    train_df = pd.read_parquet(TRAIN_DATA_PATH_FROM_ARTIFACT)\n",
    "    print(f\"Training data loaded successfully from artifact. Shape: {train_df.shape}\")\n",
    "    run.log({'fit_preprocessors/input_train_rows_from_artifact': train_df.shape[0],\n",
    "             'fit_preprocessors/input_train_cols_from_artifact': train_df.shape[1]})\n",
    "\n",
    "    # --- Fetch Feature Configuration from the NB03 Run that Produced this Artifact ---\n",
    "    nb03_producer_run = train_data_artifact.logged_by()\n",
    "    if nb03_producer_run:\n",
    "        source_nb03_run_id_for_config = nb03_producer_run.id\n",
    "        print(f\"Fetching feature config from producer NB03 run: {nb03_producer_run.name} (ID: {source_nb03_run_id_for_config})\")\n",
    "        \n",
    "        nb03_run_config = dict(nb03_producer_run.config) # Convert to dict\n",
    "        # NB03 logged its feature selection under \"features_prepared_in_nb03\"\n",
    "        fetched_features_from_nb03 = nb03_run_config.get(\"features_prepared_in_nb03\", {})\n",
    "        \n",
    "        if not fetched_features_from_nb03 or \\\n",
    "           'time_varying' not in fetched_features_from_nb03 or \\\n",
    "           'static' not in fetched_features_from_nb03:\n",
    "            raise ValueError(\"Key 'features_prepared_in_nb03' or its subkeys ('time_varying', 'static') \"\n",
    "                             \"not found or incomplete in the config of the NB03 run that produced the input artifact.\")\n",
    "            \n",
    "        source_time_varying_features = fetched_features_from_nb03.get(\"time_varying\", [])\n",
    "        source_static_features = fetched_features_from_nb03.get(\"static\", [])\n",
    "        \n",
    "        print(f\"  Successfully fetched feature lists from producer NB03 run's config.\")\n",
    "        print(f\"    Source Time-Varying features: {source_time_varying_features}\")\n",
    "        print(f\"    Source Static features: {source_static_features}\")\n",
    "        \n",
    "        # Log which NB03 run's config was used\n",
    "        run.config.update({\n",
    "            \"source_config/nb03_producer_run_id\": source_nb03_run_id_for_config,\n",
    "            \"source_config/nb03_producer_run_name\": nb03_producer_run.name,\n",
    "            \"source_config/initial_time_varying_features\": source_time_varying_features,\n",
    "            \"source_config/initial_static_features\": source_static_features\n",
    "        }, allow_val_change=True)\n",
    "    else:\n",
    "        raise ConnectionError(\"Could not retrieve the W&B run that produced the input training data artifact. \"\n",
    "                              \"Feature lists cannot be fetched.\")\n",
    "\n",
    "except Exception as e_load_artifact_or_config:\n",
    "    print(f\"CRITICAL ERROR loading training data artifact or fetching NB03 config: {e_load_artifact_or_config}\")\n",
    "    if run: run.finish(exit_code=1)\n",
    "    # exit()\n",
    "\n",
    "if train_df is None or train_df.empty:\n",
    "    print(\"CRITICAL ERROR: train_df is not loaded or empty after artifact processing. Cannot continue.\")\n",
    "    # exit()\n",
    "    # Ensure these are lists for downstream code even if loading fails and we don't exit\n",
    "    if 'source_time_varying_features' not in locals(): source_time_varying_features = []\n",
    "    if 'source_static_features' not in locals(): source_static_features = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14359a63",
   "metadata": {},
   "source": [
    "## 5. Identify Columns for Imputation & Scaling and Log Final Configuration\n",
    "\n",
    "Based on the loaded `train_df` and the `source_feature_lists` (time-varying and static) fetched from the Notebook 03 W&B run configuration:\n",
    "\n",
    "1.  **Identify Imputation Columns:** Columns from the `source_feature_lists` that exhibit missing values (`NaN`) in the current `train_df` are selected for imputation. The imputation strategy (e.g., 'median') is noted.\n",
    "2.  **Identify Scaling Columns:** Numerical columns from the `source_feature_lists` that are present in `train_df` are selected for scaling (e.g., using `StandardScaler`). Categorical columns (like 'M/F' which will be encoded later) and potentially some pre-scaled or identifier-like numerical columns are excluded.\n",
    "3.  **Log Definitive Configuration to W&B:** The *exact* lists of `imputation_cols`, `scaling_cols`, the chosen `imputation_strategy`, `scaling_strategy`, and the definitive `features` (time-varying and static lists that `OASISDataset` should use, reflecting the original selection from NB03 and considering M/F encoding) are logged to the configuration of this **current Notebook 04 W&B run**. This logged configuration becomes the **authoritative source** for all subsequent `OASISDataset` instantiations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c661df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Identify Actual Columns for Imputation & Scaling from Training Data ---\n",
    "# Also, define and log the final feature and preprocessing config for OASISDataset.\n",
    "print(\"\\n--- Determining Actual Columns for Imputation & Scaling from Training Data ---\")\n",
    "print(\"   (Using feature lists fetched from Notebook 03's W&B Run config)\")\n",
    "\n",
    "# Initialize lists to store final column names for preprocessing\n",
    "imputation_cols_to_fit = []\n",
    "scaling_cols_to_fit = []\n",
    "\n",
    "# These are the feature lists that OASISDataset should ultimately use.\n",
    "# They are based on source_time_varying_features and source_static_features from NB03,\n",
    "# filtered by actual presence in train_df (already done when NB03 created them),\n",
    "# and considering M/F encoding.\n",
    "final_model_time_varying_features_list = [] \n",
    "final_model_static_features_list = [] \n",
    "\n",
    "if 'train_df' in locals() and not train_df.empty and \\\n",
    "   'source_time_varying_features' in locals() and \\\n",
    "   'source_static_features' in locals():\n",
    "\n",
    "    available_cols_in_train_df = train_df.columns.tolist()\n",
    "    candidate_model_features = [\n",
    "        f for f in (source_time_varying_features + source_static_features) \n",
    "        if f in available_cols_in_train_df\n",
    "    ]\n",
    "\n",
    "    # --- Determine Imputation Columns ---\n",
    "    missing_in_train_df_subset = train_df[candidate_model_features].isnull().sum()\n",
    "    imputation_cols_based_on_train_nans = missing_in_train_df_subset[missing_in_train_df_subset > 0].index.tolist()\n",
    "    print(f\"Columns with NaNs in current train_df: {imputation_cols_based_on_train_nans}\")\n",
    "\n",
    "    # --- Define columns that are known to sometimes have sparse NaNs AND will be scaled ---\n",
    "    # These should be proactively imputed even if the current train_df split has no NaNs for them.\n",
    "    # Ensure these are part of the overall feature set defined by NB03.\n",
    "    proactively_impute_these_if_scaled = ['MMSE', 'SES', 'nWBV', 'EDUC', 'Baseline_MMSE'] # Add any others\n",
    "    \n",
    "    imputation_cols_to_fit = list(set(imputation_cols_based_on_train_nans + \\\n",
    "                                   [col for col in proactively_impute_these_if_scaled if col in candidate_model_features]))\n",
    "    imputation_cols_to_fit = sorted([col for col in imputation_cols_to_fit if col in available_cols_in_train_df]) # Final check\n",
    "    \n",
    "    print(f\"Final columns selected for imputation: {imputation_cols_to_fit}\")\n",
    "\n",
    "    # --- Determine Scaling Columns ---\n",
    "    potential_cols_for_scaling_from_source = [\n",
    "        f for f in candidate_model_features \n",
    "        if pd.api.types.is_numeric_dtype(train_df[f])\n",
    "    ]\n",
    "    cols_to_exclude_from_scaling = ['M/F'] \n",
    "    if base_config.get(\"preprocessing_config\",{}).get(\"scale_baseline_cdr\", False) is False:\n",
    "        if 'Baseline_CDR' not in cols_to_exclude_from_scaling:\n",
    "             cols_to_exclude_from_scaling.append('Baseline_CDR')\n",
    "    scaling_cols_to_fit = [col for col in potential_cols_for_scaling_from_source if col not in cols_to_exclude_from_scaling]\n",
    "    print(f\"Final columns selected for scaling: {scaling_cols_to_fit}\")\n",
    "\n",
    "    # --- Determine Final Feature Lists for OASISDataset ---\n",
    "    final_model_time_varying_features_list = [f for f in source_time_varying_features if f in available_cols_in_train_df]\n",
    "    temp_static_from_source_available = [f for f in source_static_features if f in available_cols_in_train_df]\n",
    "    final_model_static_features_list = []\n",
    "    if 'M/F' in temp_static_from_source_available and base_config.get('preprocessing_config',{}).get('encode_m_f_in_dataset_class', True):\n",
    "        if 'M/F_encoded' not in final_model_static_features_list: \n",
    "            final_model_static_features_list.append('M/F_encoded')\n",
    "    for feat in temp_static_from_source_available:\n",
    "        if feat != 'M/F': \n",
    "            final_model_static_features_list.append(feat)\n",
    "    if 'M/F_encoded' in source_static_features and 'M/F_encoded' in available_cols_in_train_df and \\\n",
    "       'M/F_encoded' not in final_model_static_features_list:\n",
    "        final_model_static_features_list.append('M/F_encoded')\n",
    "    final_model_static_features_list = sorted(list(set(final_model_static_features_list)))\n",
    "\n",
    "    print(f\"\\nDefinitive Feature Lists for OASISDataset Configuration (to be logged to this NB04 run):\")\n",
    "    print(f\"  Time-Varying Features for Model: {final_model_time_varying_features_list}\")\n",
    "    print(f\"  Static Features for Model: {final_model_static_features_list}\")\n",
    "\n",
    "    # --- Log this definitive configuration to the current W&B run (NB04 run) ---\n",
    "    if run:\n",
    "        imputation_strategy_logged = base_config.get('preprocessing_config',{}).get('imputation_strategy', 'median')\n",
    "        scaling_strategy_logged = base_config.get('preprocessing_config',{}).get('scaling_strategy', 'standard_scaler') \n",
    "        \n",
    "        config_for_oasis_dataset_to_log = {\n",
    "            'preprocess': {\n",
    "                'imputation_cols': imputation_cols_to_fit,\n",
    "                'scaling_cols': scaling_cols_to_fit,       \n",
    "                'imputation_strategy': imputation_strategy_logged,\n",
    "                'scaling_strategy': scaling_strategy_logged \n",
    "            },\n",
    "            'features': { \n",
    "                'time_varying': final_model_time_varying_features_list,\n",
    "                'static': final_model_static_features_list \n",
    "            },\n",
    "            'cnn_model_params': base_config.get('cnn_model_params', {}),\n",
    "            'preprocessing_config': base_config.get('preprocessing_config', {})\n",
    "        }\n",
    "        run.config.update(config_for_oasis_dataset_to_log, allow_val_change=True)\n",
    "        print(\"\\nDefinitive 'features' and 'preprocess' configuration for OASISDataset logged to this W&B run's config.\")\n",
    "else:\n",
    "    print(\"Skipping identification of preprocessing columns as train_df is empty or source feature lists are not defined.\")\n",
    "    # Ensure lists are empty if not populated, to avoid NameErrors if run continues (though it shouldn't)\n",
    "    imputation_cols_to_fit = []\n",
    "    scaling_cols_to_fit = []\n",
    "    final_model_time_varying_features_list = []\n",
    "    final_model_static_features_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86045b1d",
   "metadata": {},
   "source": [
    "## 6. Fit and Save Imputer\n",
    "\n",
    "Based on the `imputation_cols_to_fit` identified from the training data, an imputer (e.g., `SimpleImputer` with a 'median' strategy) is initialized and fitted. This fitted imputer learns the imputation values (e.g., medians) *only from the training data*. The fitted object is then saved locally as a `.joblib` file and logged as a versioned artifact to W&B. This allows the exact same imputation to be applied consistently to validation and test sets later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fit and Save Imputer ---\n",
    "print(\"\\n--- Fitting and Saving Imputer ---\")\n",
    "\n",
    "imputer_fitted = None # Initialize to ensure variable is defined\n",
    "\n",
    "# Ensure imputation_cols_to_fit is defined from the previous cell,\n",
    "# train_df is loaded, and base_config is available.\n",
    "# IMPUTER_SAVE_PATH and DATASET_IDENTIFIER should also be defined from setup cells.\n",
    "if 'imputation_cols_to_fit' in locals() and imputation_cols_to_fit and \\\n",
    "   'train_df' in locals() and not train_df.empty and \\\n",
    "   'IMPUTER_SAVE_PATH' in locals() and IMPUTER_SAVE_PATH is not None:\n",
    "    \n",
    "    # Determine imputation strategy from base_config (preprocessing_config section)\n",
    "    imputation_strategy_to_use = base_config.get('preprocessing_config', {})\\\n",
    "                                           .get('imputation_strategy', 'median') # Default to 'median'\n",
    "    print(f\"  Using imputation strategy: '{imputation_strategy_to_use}' for columns: {imputation_cols_to_fit}\")\n",
    "\n",
    "    try:\n",
    "        # Initialize SimpleImputer with the chosen strategy\n",
    "        imputer_fitted = SimpleImputer(strategy=imputation_strategy_to_use)\n",
    "        \n",
    "        # Fit imputer ONLY on the specified columns of the training data\n",
    "        # Ensure all columns in imputation_cols_to_fit actually exist in train_df \n",
    "        # (though this should be guaranteed by how imputation_cols_to_fit was created)\n",
    "        valid_imputation_cols = [col for col in imputation_cols_to_fit if col in train_df.columns]\n",
    "        if not valid_imputation_cols:\n",
    "            print(\"  Warning: No valid columns found in train_df for imputation, though imputation_cols_to_fit was not empty. Skipping imputer fitting.\")\n",
    "        else:\n",
    "            imputer_fitted.fit(train_df[valid_imputation_cols])\n",
    "            print(f\"  Imputer (strategy: '{imputation_strategy_to_use}') fitted successfully on training data columns: {valid_imputation_cols}\")\n",
    "\n",
    "            print(f\"  Applying imputation transform to 'train_df' for columns: {valid_imputation_cols}...\")\n",
    "            # .transform returns a NumPy array, so reassign it back to the DataFrame columns\n",
    "            train_df[valid_imputation_cols] = imputer_fitted.transform(train_df[valid_imputation_cols])\n",
    "            print(\"  Imputation transform applied to 'train_df'.\")\n",
    "\n",
    "            # Save the fitted imputer locally\n",
    "            # IMPUTER_SAVE_PATH should be fully resolved, e.g., <...>/04_Fit_Preprocessors_OASIS2/simple_imputer_median_oasis2.joblib\n",
    "            IMPUTER_SAVE_PATH.parent.mkdir(parents=True, exist_ok=True) # Ensure directory exists\n",
    "            joblib.dump(imputer_fitted, IMPUTER_SAVE_PATH)\n",
    "            print(f\"  Fitted imputer saved locally to: {IMPUTER_SAVE_PATH}\")\n",
    "\n",
    "            # Log imputer as a W&B artifact\n",
    "            if run: # Check if W&B run is active\n",
    "                print(\"  Logging imputer as W&B artifact...\")\n",
    "                # Consistent artifact naming including dataset and strategy\n",
    "                imputer_artifact_base_name = IMPUTER_SAVE_PATH.stem # e.g., \"simple_imputer_median_oasis2\"\n",
    "                imputer_artifact_name = f\"{imputer_artifact_base_name}\" # Or just imputer_artifact_base_name\n",
    "                imputer_artifact_type = f\"preprocessor_{DATASET_IDENTIFIER}\" # e.g., \"preprocessor_oasis2\"\n",
    "                \n",
    "                imputer_description = (\n",
    "                    f\"SimpleImputer(strategy='{imputation_strategy_to_use}') fitted on {DATASET_IDENTIFIER.upper()} \"\n",
    "                    f\"training data columns: {valid_imputation_cols}.\"\n",
    "                )\n",
    "                imputer_metadata = {\n",
    "                    'columns_imputed': valid_imputation_cols, \n",
    "                    'imputation_strategy': imputation_strategy_to_use,\n",
    "                    'dataset_identifier': DATASET_IDENTIFIER,\n",
    "                    'saved_filename': IMPUTER_SAVE_PATH.name # Log the actual filename\n",
    "                }\n",
    "                \n",
    "                imputer_wandb_artifact = wandb.Artifact(\n",
    "                    imputer_artifact_name, \n",
    "                    type=imputer_artifact_type, \n",
    "                    description=imputer_description,\n",
    "                    metadata=imputer_metadata\n",
    "                )\n",
    "                imputer_wandb_artifact.add_file(str(IMPUTER_SAVE_PATH)) # Add the .joblib file\n",
    "                # Define aliases for easy retrieval, e.g., \"latest\" and strategy-specific\n",
    "                aliases = [\"latest\", f\"imputer_{imputation_strategy_to_use.lower()}_{time.strftime('%Y%m%d')}\"]\n",
    "                run.log_artifact(imputer_wandb_artifact, aliases=aliases)\n",
    "                print(f\"  Imputer artifact '{imputer_artifact_name}' (aliases: {aliases}) logged to W&B.\")\n",
    "            else:\n",
    "                print(\"  W&B run not active. Skipping artifact logging for imputer.\")\n",
    "\n",
    "    except Exception as e_imputer:\n",
    "        print(f\"  CRITICAL ERROR fitting or saving imputer: {e_imputer}\")\n",
    "        imputer_fitted = None # Ensure it's None if fitting/saving failed\n",
    "else:\n",
    "    if not ('imputation_cols_to_fit' in locals() and imputation_cols_to_fit):\n",
    "        print(\"  No columns were identified for imputation in the previous step. Skipping imputer fitting.\")\n",
    "    elif not ('train_df' in locals() and not train_df.empty):\n",
    "        print(\"  Training data (train_df) is empty or not defined. Skipping imputer fitting.\")\n",
    "    elif not ('IMPUTER_SAVE_PATH' in locals() and IMPUTER_SAVE_PATH is not None):\n",
    "        print(\"  IMPUTER_SAVE_PATH is not defined. Skipping imputer fitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae8886",
   "metadata": {},
   "source": [
    "## 7. Fit and Save Scaler\n",
    "\n",
    "Similarly, for the `scaling_cols_to_fit`, a scaler (e.g., `StandardScaler`) is initialized and fitted *only* on the training data (potentially after imputation, if the columns overlap and imputation was performed in-place on `train_df`). This fitted scaler learns the mean and standard deviation from the training data. The fitted object is saved locally and logged as a versioned W&B artifact for consistent application to all data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fit and Save Scaler ---\n",
    "print(\"\\n--- Fitting and Saving Scaler ---\")\n",
    "\n",
    "scaler_fitted = None # Initialize\n",
    "\n",
    "# Ensure scaling_cols_to_fit is defined and train_df is not empty\n",
    "if 'scaling_cols_to_fit' in locals() and scaling_cols_to_fit and \\\n",
    "   'train_df' in locals() and not train_df.empty:\n",
    "\n",
    "    # Determine scaling strategy from base_config, default to 'standard_scaler'\n",
    "    # Note: StandardScaler is a class, the strategy name in config might be 'standard_scaler'\n",
    "    scaling_strategy_name = base_config.get('preprocessing_config', {}).get('scaling_strategy', 'standard_scaler')\n",
    "    print(f\"  Using scaling strategy: '{scaling_strategy_name}' for columns: {scaling_cols_to_fit}\")\n",
    "    \n",
    "    # Select data for scaler fitting (potentially imputed if imputation_cols overlap with scaling_cols)\n",
    "    # train_df here should be the version after imputation has been applied if imputation_cols_to_fit was not empty.\n",
    "    data_for_scaling = train_df[scaling_cols_to_fit]\n",
    "    \n",
    "    # Check if data_for_scaling still has any NaN values (imputer might not have covered all scaling_cols)\n",
    "    if data_for_scaling.isnull().sum().any():\n",
    "        print(f\"  Warning: Data for scaling still contains NaN values AFTER potential imputation for columns: \"\n",
    "              f\"{data_for_scaling.isnull().sum()[data_for_scaling.isnull().sum() > 0].index.tolist()}\")\n",
    "        print(\"  This might cause issues with StandardScaler. Consider refining imputation_cols or handling NaNs before scaling.\")\n",
    "        # Option: Impute remaining NaNs in data_for_scaling with mean/median just for fitting scaler,\n",
    "        # but this implies imputer should have handled these columns. For now, proceed with warning.\n",
    "        # data_for_scaling = data_for_scaling.fillna(data_for_scaling.median()) # Example: quick fix\n",
    "\n",
    "    try:\n",
    "        if scaling_strategy_name.lower() == 'standard_scaler':\n",
    "            scaler_fitted = StandardScaler()\n",
    "        # Add elif for other scalers like MinMaxScaler if you plan to use them via config\n",
    "        # elif scaling_strategy_name.lower() == 'min_max_scaler':\n",
    "        #     scaler_fitted = MinMaxScaler()\n",
    "        else:\n",
    "            print(f\"  Warning: Unknown scaling strategy '{scaling_strategy_name}'. Defaulting to StandardScaler.\")\n",
    "            scaler_fitted = StandardScaler()\n",
    "            scaling_strategy_name = \"StandardScaler\" # Update for logging\n",
    "\n",
    "        # Fit scaler ONLY on the training data's specified columns\n",
    "        scaler_fitted.fit(data_for_scaling)\n",
    "        print(\"  Scaler fitted successfully on training data.\")\n",
    "\n",
    "        # Save the fitted scaler locally using SCALER_SAVE_PATH\n",
    "        joblib.dump(scaler_fitted, SCALER_SAVE_PATH) # SCALER_SAVE_PATH from Cell 2\n",
    "        print(f\"  Fitted scaler saved locally to: {SCALER_SAVE_PATH}\")\n",
    "\n",
    "        # Log scaler as a W&B artifact\n",
    "        if run:\n",
    "            print(\"  Logging scaler as W&B artifact...\")\n",
    "            scaler_artifact_name = f\"scaler_{scaling_strategy_name.lower().replace('_scaler','')}_{DATASET_IDENTIFIER}\"\n",
    "            scaler_artifact_type = f\"preprocessor_{DATASET_IDENTIFIER}\"\n",
    "            scaler_description = (\n",
    "                f\"{scaling_strategy_name} fitted on {DATASET_IDENTIFIER} training data columns: {scaling_cols_to_fit}\"\n",
    "            )\n",
    "            scaler_wandb_artifact = wandb.Artifact(\n",
    "                scaler_artifact_name, \n",
    "                type=scaler_artifact_type, \n",
    "                description=scaler_description,\n",
    "                metadata={'columns_scaled': scaling_cols_to_fit, 'strategy': scaling_strategy_name}\n",
    "            )\n",
    "            scaler_wandb_artifact.add_file(str(SCALER_SAVE_PATH))\n",
    "            run.log_artifact(scaler_wandb_artifact, aliases=[\"latest\", f\"{scaling_strategy_name.lower()}_{time.strftime('%Y%m%d')}\"])\n",
    "            print(f\"  Scaler artifact '{scaler_artifact_name}' logged to W&B.\")\n",
    "\n",
    "    except Exception as e_scaler:\n",
    "        print(f\"  Error fitting or saving scaler: {e_scaler}\")\n",
    "        scaler_fitted = None # Ensure it's None if fitting/saving failed\n",
    "else:\n",
    "    print(\"  No columns identified for scaling, or training data is empty. Skipping scaler fitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b178fcf",
   "metadata": {},
   "source": [
    "## 7. Finalize W&B Run\n",
    "\n",
    "Finish the Weights & Biases run associated with fitting and saving the preprocessors. The saved `.joblib` files (fitted imputer and scaler) and their corresponding W&B artifacts, along with the definitive logged configuration for features and preprocessing, are now ready for use in the downstream data loading pipeline (`OASISDataset`) and model training stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d3fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Finish W&B Run for Notebook 04 ---\n",
    "print(f\"\\n--- {NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER} complete. Finishing W&B run. ---\")\n",
    "\n",
    "if run: # Check if 'run' object exists and is an active run\n",
    "    try:\n",
    "        # Add any final summary metrics for NB04 to run.summary if applicable\n",
    "        # For example, number of features imputed/scaled if not already in config.\n",
    "        run.summary[\"num_imputation_cols_fitted\"] = len(imputation_cols_to_fit) if 'imputation_cols_to_fit' in locals() else 0\n",
    "        run.summary[\"num_scaling_cols_fitted\"] = len(scaling_cols_to_fit) if 'scaling_cols_to_fit' in locals() else 0\n",
    "        \n",
    "        run.finish()\n",
    "        run_name_to_print = run.name_synced if hasattr(run, 'name_synced') and run.name_synced else \\\n",
    "                            run.name if hasattr(run, 'name') and run.name else \\\n",
    "                            run.id if hasattr(run, 'id') else \"current run\"\n",
    "        print(f\"W&B run '{run_name_to_print}' finished successfully.\")\n",
    "    except Exception as e_finish_run_nb04:\n",
    "        print(f\"Error during wandb.finish() for Notebook 04: {e_finish_run_nb04}\")\n",
    "else:\n",
    "    print(\"No active W&B run to finish for this session.\")\n",
    "\n",
    "print(f\"\\n--- Notebook {NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER} execution finished. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro_predcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
