{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "046d57cd",
   "metadata": {},
   "source": [
    "# Notebook 05: OASIS-2 Data Loading Pipeline Test\n",
    "\n",
    "**Project Phase:** 1 (Data Processing - DataLoader Verification)\n",
    "**Dataset:** OASIS-2 Longitudinal MRI & Clinical Data\n",
    "\n",
    "**Purpose:**\n",
    "This notebook is dedicated to verifying the integrity and correct behavior of the custom data loading pipeline, primarily the `OASISDataset` class and `pad_collate_fn` (defined in `src/datasets.py`). This involves:\n",
    "\n",
    "1.  **Load Training & Validation Data Splits:** Use paths resolved by `get_dataset_stage_paths` (from `config.json`) to access `cohort_train.parquet` and `cohort_validation.parquet` (outputs from Notebook 03).\n",
    "2.  **Consume Preprocessor Artifacts & Fetch NB04 Config:**\n",
    "    * Consume the versioned **fitted preprocessor W&B Artifacts** (e.g., `scaler_standard_oasis2:latest`, `imputer_median_oasis2:latest`) produced by Notebook 04.\n",
    "    * Download these artifacts to get local paths to the `.joblib` files.\n",
    "    * Identify the W&B Run from Notebook 04 that **produced** these preprocessor artifacts using `artifact.logged_by()`.\n",
    "    * Fetch the definitive **`features` (time-varying, static) and `preprocess` (imputation/scaling columns and strategies) configurations** directly from this producer Notebook 04 run's W&B config. This ensures `OASISDataset` is initialized with the authoritative settings.\n",
    "3.  **Instantiate `OASISDataset`:** Create dataset instances for both training and validation data, ensuring they correctly use the downloaded preprocessors and the fetched feature configurations, with the option to `include_mri=True`.\n",
    "4.  **Test `DataLoader` with `pad_collate_fn`:** Wrap the datasets in PyTorch `DataLoader`s.\n",
    "5.  **Verify Batch Structure:** Iterate through a few batches from the `train_loader` and:\n",
    "    * Unpack all expected components (padded tabular sequences, padded MRI sequences, lengths, targets, masks).\n",
    "    * Print their shapes, data types, and example values.\n",
    "    * Confirm that padding, masking, and data types are as expected for model input.\n",
    "6.  Log key batch characteristics and test status to a new W&B run for this notebook.\n",
    "\n",
    "**Input:**\n",
    "* `config.json`: Main project configuration file.\n",
    "* **W&B Artifact Names for Fitted Preprocessors:** e.g., `\"scaler_standard_oasis2:latest\"`, `\"imputer_median_oasis2:latest\"` (produced by Notebook 04).\n",
    "* `cohort_train.parquet`, `cohort_validation.parquet`: Data splits (output from Notebook 03, paths obtained via `get_dataset_stage_paths`).\n",
    "* `src/datasets.py`: Contains `OASISDataset` and `pad_collate_fn`.\n",
    "* Directory containing preprocessed MRI scans (if `include_mri=True`).\n",
    "\n",
    "**Output:**\n",
    "* Console output displaying the properties (shapes, types, example values) of generated batches for verification.\n",
    "* W&B Run: Logs the configuration used (including source NB04 run ID and consumed preprocessor artifact names) and key characteristics of the test batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30999e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: notebooks/05_Test_DataLoader.ipynb\n",
    "# Purpose: Test the custom OASISDataset and pad_collate_fn\n",
    "#          to ensure data loading, preprocessing (using saved objects),\n",
    "#          sequencing, padding, and batching works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac1a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import joblib \n",
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import wandb \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef5c00d",
   "metadata": {},
   "source": [
    "## 1. Setup: Project Configuration, Paths, and Utilities\n",
    "\n",
    "Initialize the notebook environment:\n",
    "* Determines the project root and adds the `src` directory to `sys.path`.\n",
    "* Imports custom utilities: `initialize_wandb_run` and `get_dataset_stage_paths`.\n",
    "* Imports `OASISDataset` and `pad_collate_fn` from `src/datasets.py`.\n",
    "* Loads the main project configuration (`base_config`) from `config.json`.\n",
    "* Defines dataset and notebook-specific identifiers.\n",
    "* **Uses `get_dataset_paths` to resolve paths for the input training and validation data splits (from Notebook 03) and the general MRI data directory.** Paths to preprocessor `.joblib` files will be obtained later by downloading W&B artifacts.\n",
    "* Defines parameters for DataLoader testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b6c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Project Setup, Configuration Loading, and Utility Imports ---\n",
    "print(\"--- Initializing Project Setup & Configuration for NB05 ---\")\n",
    "\n",
    "# Initialize\n",
    "PROJECT_ROOT = None\n",
    "base_config = {} \n",
    "\n",
    "try:\n",
    "    current_notebook_path = Path.cwd() \n",
    "    potential_project_root = current_notebook_path.parent \n",
    "    if (potential_project_root / \"src\").is_dir() and (potential_project_root / \"config.json\").is_file():\n",
    "        PROJECT_ROOT = potential_project_root\n",
    "    else: \n",
    "        PROJECT_ROOT = current_notebook_path\n",
    "    if not (PROJECT_ROOT / \"src\").is_dir() or not (PROJECT_ROOT / \"config.json\").is_file():\n",
    "        raise FileNotFoundError(f\"Could not find 'src' or 'config.json'. PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "    if str(PROJECT_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    print(f\"PROJECT_ROOT: {PROJECT_ROOT}, added to sys.path.\")\n",
    "\n",
    "    from src.wandb_utils import initialize_wandb_run\n",
    "    from src.paths_utils import get_dataset_paths \n",
    "    from src.datasets import OASISDataset, pad_collate_fn\n",
    "    print(\"Successfully imported custom utilities and dataset classes.\")\n",
    "\n",
    "except Exception as e_setup:\n",
    "    print(f\"CRITICAL ERROR during initial setup: {e_setup}\")\n",
    "    # exit()\n",
    "\n",
    "# --- Load Main Project Configuration ---\n",
    "print(\"\\n--- Loading Main Project Configuration ---\")\n",
    "try:\n",
    "    if PROJECT_ROOT is None: raise ValueError(\"PROJECT_ROOT not set.\")\n",
    "    CONFIG_PATH_MAIN = PROJECT_ROOT / 'config.json'\n",
    "    with open(CONFIG_PATH_MAIN, 'r', encoding='utf-8') as f:\n",
    "        base_config = json.load(f)\n",
    "    print(f\"Main project config loaded from: {CONFIG_PATH_MAIN}\")\n",
    "\n",
    "    WANDB_ENTITY = base_config.get('wandb', {}).get('entity')\n",
    "    WANDB_PROJECT = base_config.get('wandb', {}).get('project_name')\n",
    "    if not WANDB_ENTITY or not WANDB_PROJECT:\n",
    "        raise KeyError(\"WANDB_ENTITY or WANDB_PROJECT not found in config.json ['wandb'] section.\")\n",
    "    print(f\"W&B Entity: {WANDB_ENTITY}, Project: {WANDB_PROJECT}\")\n",
    "\n",
    "except Exception as e_cfg:\n",
    "    print(f\"CRITICAL ERROR loading main config.json: {e_cfg}\")\n",
    "    # exit() \n",
    "\n",
    "# --- Define Dataset, Notebook Specifics, and Resolve Key INPUT Paths ---\n",
    "DATASET_IDENTIFIER = \"oasis2\" \n",
    "NOTEBOOK_MODULE_NAME = \"05_Test_DataLoader\"\n",
    "\n",
    "# Paths for data splits (inputs from NB03)\n",
    "TRAIN_DATA_PATH = None\n",
    "VAL_DATA_PATH = None\n",
    "MRI_DATA_DIR = None # General preprocessed MRI directory\n",
    "\n",
    "# Preprocessor paths will be determined by downloading artifacts later\n",
    "SCALER_PATH_FROM_ARTIFACT = None \n",
    "IMPUTER_PATH_FROM_ARTIFACT = None\n",
    "\n",
    "try:\n",
    "    if not base_config: raise ValueError(\"base_config is empty.\")\n",
    "    \n",
    "    # Get paths for training stage data (train/val splits) and MRI dir\n",
    "    # The preprocessor paths returned by this utility won't be used directly for loading preprocessors,\n",
    "    # as we'll use artifacts, but it's good to have them if needed for reference or if artifact download fails.\n",
    "    pipeline_paths_for_nb05_inputs = get_dataset_paths(\n",
    "        PROJECT_ROOT, \n",
    "        base_config, \n",
    "        DATASET_IDENTIFIER, \n",
    "        stage=\"training\" # We test DataLoaders on training and validation data\n",
    "    )\n",
    "    TRAIN_DATA_PATH = pipeline_paths_for_nb05_inputs.get('train_data_parquet')\n",
    "    VAL_DATA_PATH = pipeline_paths_for_nb05_inputs.get('val_data_parquet')\n",
    "    MRI_DATA_DIR = pipeline_paths_for_nb05_inputs.get('mri_data_dir') # For OASISDataset\n",
    "\n",
    "    if not all([TRAIN_DATA_PATH, VAL_DATA_PATH, MRI_DATA_DIR]):\n",
    "        raise ValueError(\"Failed to resolve one or more critical data paths from get_dataset_stage_paths.\")\n",
    "\n",
    "    print(f\"\\nKey input paths for Notebook 05 ({DATASET_IDENTIFIER}):\")\n",
    "    print(f\"  Input Training Data Parquet (from NB03): {TRAIN_DATA_PATH}\")\n",
    "    print(f\"  Input Validation Data Parquet (from NB03): {VAL_DATA_PATH}\")\n",
    "    print(f\"  Input MRI Data Directory: {MRI_DATA_DIR}\")\n",
    "    \n",
    "    # Verify existence of these critical input files/dirs\n",
    "    for p_name, p_obj in [(\"Training Data\", TRAIN_DATA_PATH), (\"Validation Data\", VAL_DATA_PATH)]:\n",
    "        if not p_obj.is_file(): raise FileNotFoundError(f\"CRITICAL: {p_name} not found at: {p_obj}\")\n",
    "    if not MRI_DATA_DIR.is_dir(): raise FileNotFoundError(f\"CRITICAL: MRI Data Directory not found at: {MRI_DATA_DIR}\")\n",
    "    print(\"All critical input data paths for NB05 verified.\")\n",
    "\n",
    "except (KeyError, ValueError, FileNotFoundError) as e_paths_nb05:\n",
    "    print(f\"CRITICAL ERROR during path setup for NB05: {e_paths_nb05}\")\n",
    "    # exit()\n",
    "except Exception as e_general_nb05_setup:\n",
    "    print(f\"CRITICAL ERROR during setup for NB05: {e_general_nb05_setup}\")\n",
    "    # exit()\n",
    "\n",
    "# --- Parameters for DataLoader Testing ---\n",
    "BATCH_SIZE_FOR_TESTING = 4 \n",
    "NUM_BATCHES_TO_INSPECT = 2\n",
    "# Flag to test MRI inclusion; set to False to test tabular-only path of OASISDataset/pad_collate_fn\n",
    "TEST_WITH_MRI = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5200e574",
   "metadata": {},
   "source": [
    "## 2. Initialize W&B Run & Define Artifacts to Consume\n",
    "\n",
    "A new W&B run is initiated for this \"Test DataLoader\" notebook. This run will log:\n",
    "* The configuration parameters used by this notebook.\n",
    "* The names of the W&B Preprocessor Artifacts (Scaler and Imputer from Notebook 04) that this test will consume.\n",
    "* The W&B Run ID of the Notebook 04 execution that produced these preprocessors (obtained via artifact lineage).\n",
    "* Key characteristics of the data batches generated by the `DataLoader` to verify the pipeline's integrity.\n",
    "\n",
    "This step also defines the names and expected types of the preprocessor artifacts that will be downloaded from W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize a NEW W&B Run for THIS Notebook 05 execution ---\n",
    "print(\"\\n--- Initializing a New Weights & Biases Run for NB05 (DataLoader Test) ---\")\n",
    "\n",
    "# --- Define W&B Artifact Names for INPUT Preprocessors (Outputs from NB04) ---\n",
    "# These names MUST MATCH the artifact names used by Notebook 04 when it logged them.\n",
    "# They depend on the strategies defined in config.json and the DATASET_IDENTIFIER.\n",
    "\n",
    "# Get strategies from base_config to construct artifact names\n",
    "preprocessing_cfg = base_config.get('preprocessing_config', {})\n",
    "scaling_strategy_name = preprocessing_cfg.get('scaling_strategy', 'standard_scaler')\n",
    "imputation_strategy_name = preprocessing_cfg.get('imputation_strategy', 'median')\n",
    "\n",
    "# Construct artifact names as NB04 would have logged them\n",
    "SCALER_ARTIFACT_NAME = f\"scaler_{scaling_strategy_name.lower().replace('_scaler','').replace('_','')}_{DATASET_IDENTIFIER}\"\n",
    "IMPUTER_ARTIFACT_NAME = f\"simple_imputer_{imputation_strategy_name.lower()}_{DATASET_IDENTIFIER}\"\n",
    "PREPROCESSOR_ARTIFACT_TYPE = f\"preprocessor_{DATASET_IDENTIFIER}\" # Matches type used by NB04\n",
    "PREPROCESSOR_ARTIFACT_VERSION = \"latest\" # Use \"latest\" to get the newest fitted preprocessors\n",
    "\n",
    "print(f\"  Will attempt to use Scaler artifact: {SCALER_ARTIFACT_NAME}:{PREPROCESSOR_ARTIFACT_VERSION}\")\n",
    "print(f\"  Will attempt to use Imputer artifact: {IMPUTER_ARTIFACT_NAME}:{PREPROCESSOR_ARTIFACT_VERSION}\")\n",
    "\n",
    "# Configuration specific to this NB05 run\n",
    "nb05_run_config_log = {\n",
    "    \"notebook_name_code\": f\"{NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER}\",\n",
    "    \"dataset_source\": DATASET_IDENTIFIER,\n",
    "    \"input_train_data_path_used\": str(TRAIN_DATA_PATH), \n",
    "    \"input_val_data_path_used\": str(VAL_DATA_PATH),     \n",
    "    \"mri_data_dir_used\": str(MRI_DATA_DIR),\n",
    "    \"scaler_artifact_to_use\": f\"{SCALER_ARTIFACT_NAME}:{PREPROCESSOR_ARTIFACT_VERSION}\",          \n",
    "    \"imputer_artifact_to_use\": f\"{IMPUTER_ARTIFACT_NAME}:{PREPROCESSOR_ARTIFACT_VERSION}\",        \n",
    "    \"batch_size_tested\": BATCH_SIZE_FOR_TESTING,               \n",
    "    \"include_mri_tested\": TEST_WITH_MRI,\n",
    "    \"execution_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Source NB04 run ID and its config (for features/preprocess lists) will be added after artifact loading\n",
    "}\n",
    "\n",
    "nb_number_prefix_nb05 = NOTEBOOK_MODULE_NAME.split('_')[0] if '_' in NOTEBOOK_MODULE_NAME else \"NB\"\n",
    "job_specific_type_nb05 = f\"{nb_number_prefix_nb05}-TestDataLoader-{DATASET_IDENTIFIER}\"\n",
    "custom_elements_for_name_nb05 = [nb_number_prefix_nb05, DATASET_IDENTIFIER.upper(), \"DLTest\", f\"MRI_{str(TEST_WITH_MRI)}\"]\n",
    "\n",
    "run = initialize_wandb_run( # This 'run' object is for NB05's own logging\n",
    "    base_project_config=base_config,\n",
    "    job_group=\"Verification\", \n",
    "    job_specific_type=job_specific_type_nb05,\n",
    "    run_specific_config=nb05_run_config_log,\n",
    "    custom_run_name_elements=custom_elements_for_name_nb05,\n",
    "    notes=f\"Testing OASISDataset & pad_collate_fn for {DATASET_IDENTIFIER} (MRI: {TEST_WITH_MRI}). Consumes preprocessor artifacts from NB04.\"\n",
    ")\n",
    "\n",
    "if run:\n",
    "    print(f\"New W&B run for NB05 '{run.name}' (Job Type: '{run.job_type}') initialized. View at: {run.url}\")\n",
    "else:\n",
    "    print(\"Proceeding with DataLoader test without W&B logging for this NB05 execution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a01d9",
   "metadata": {},
   "source": [
    "## 3. Load Preprocessor Artifacts & Fetch Definitive Configuration from Producer (NB04) Run\n",
    "\n",
    "This crucial step establishes the link to the preprocessing decisions made in Notebook 04:\n",
    "1.  The W&B Artifacts for the fitted `StandardScaler` and `SimpleImputer` (produced by Notebook 04) are **consumed using `run.use_artifact()`**. This downloads the `.joblib` files to a local directory. These paths will be used by `OASISDataset`.\n",
    "2.  From one of these consumed preprocessor artifacts (e.g., the scaler artifact), the W&B Run that **produced it** (i.e., the relevant Notebook 04 run) is identified using `artifact.logged_by()`.\n",
    "3.  The W&B configuration of this producer Notebook 04 run is then fetched. This configuration contains the `features` and `preprocess` dictionaries that `OASISDataset` needs for consistent data handling (e.g., knowing which exact columns were scaled/imputed, feature lists).\n",
    "This process ensures that `OASISDataset` in this notebook uses the exact same preprocessing logic and feature definitions as intended by the finalized Notebook 04 execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c7f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Preprocessor Artifacts from W&B and Fetch NB04 Config for OASISDataset ---\n",
    "print(f\"\\n--- Loading Preprocessor Artifacts & Fetching Definitive Config from NB04 Producer Run ---\")\n",
    "\n",
    "# These variables are expected to be defined in the W&B Initialization cell for NB05:\n",
    "# SCALER_ARTIFACT_NAME, IMPUTER_ARTIFACT_NAME, PREPROCESSOR_ARTIFACT_TYPE, PREPROCESSOR_ARTIFACT_VERSION\n",
    "# WANDB_ENTITY, WANDB_PROJECT (from base_config)\n",
    "# DATASET_IDENTIFIER\n",
    "# base_config (loaded main config.json)\n",
    "# run (the W&B run object for this NB05 execution)\n",
    "\n",
    "SCALER_PATH_FROM_ARTIFACT = None\n",
    "IMPUTER_PATH_FROM_ARTIFACT = None\n",
    "config_for_dataset_instance = {} # This will be populated from NB04's run config\n",
    "source_nb04_run_id_for_config = \"N/A\" \n",
    "source_nb04_run_name_for_config = \"N/A\"\n",
    "\n",
    "try:\n",
    "    if run is None: \n",
    "        raise ConnectionError(\"W&B run for Notebook 05 not initialized. Cannot use W&B artifacts.\")\n",
    "\n",
    "    # --- Step 1: Use and Download SCALER Artifact ---\n",
    "    scaler_artifact_full_name = f\"{WANDB_ENTITY}/{WANDB_PROJECT}/{SCALER_ARTIFACT_NAME}:{PREPROCESSOR_ARTIFACT_VERSION}\"\n",
    "    print(f\"  Attempting to use and download Scaler artifact: {scaler_artifact_full_name} (Type: {PREPROCESSOR_ARTIFACT_TYPE})\")\n",
    "    scaler_artifact = run.use_artifact(scaler_artifact_full_name, type=PREPROCESSOR_ARTIFACT_TYPE)\n",
    "    scaler_artifact_dir = Path(scaler_artifact.download())\n",
    "    \n",
    "    # Determine the scaler filename within the artifact (based on how NB04 saved it)\n",
    "    scaler_filename_in_artifact = base_config.get(f\"pipeline_artefact_locators_{DATASET_IDENTIFIER}\", {})\\\n",
    "        .get('scaler_fname_pattern', '{scaling_strategy}_{dataset_identifier}.joblib').format(\n",
    "            scaling_strategy=base_config.get('preprocessing_config',{}).get('scaling_strategy','standard_scaler').lower(),\n",
    "            dataset_identifier=DATASET_IDENTIFIER\n",
    "    )\n",
    "    SCALER_PATH_FROM_ARTIFACT = scaler_artifact_dir / scaler_filename_in_artifact\n",
    "    \n",
    "    if not SCALER_PATH_FROM_ARTIFACT.is_file(): # Fallback if specific name not found\n",
    "        joblib_files_scaler = list(scaler_artifact_dir.glob(\"*.joblib\"))\n",
    "        if joblib_files_scaler: \n",
    "            SCALER_PATH_FROM_ARTIFACT = joblib_files_scaler[0]\n",
    "            print(f\"  Warning: Specific scaler file '{scaler_filename_in_artifact}' not found, using first .joblib: {SCALER_PATH_FROM_ARTIFACT.name}\")\n",
    "        else: \n",
    "            raise FileNotFoundError(f\"Scaler .joblib file ('{scaler_filename_in_artifact}' or any .joblib) \"\n",
    "                                    f\"not found in downloaded scaler artifact directory: {scaler_artifact_dir}\")\n",
    "    print(f\"  Scaler artifact '{scaler_artifact.name}' downloaded. Local path: {SCALER_PATH_FROM_ARTIFACT}\")\n",
    "\n",
    "    # --- Step 2: Use and Download IMPUTER Artifact ---\n",
    "    imputer_artifact_full_name = f\"{WANDB_ENTITY}/{WANDB_PROJECT}/{IMPUTER_ARTIFACT_NAME}:{PREPROCESSOR_ARTIFACT_VERSION}\"\n",
    "    print(f\"  Attempting to use and download Imputer artifact: {imputer_artifact_full_name} (Type: {PREPROCESSOR_ARTIFACT_TYPE})\")\n",
    "    imputer_artifact = run.use_artifact(imputer_artifact_full_name, type=PREPROCESSOR_ARTIFACT_TYPE)\n",
    "    imputer_artifact_dir = Path(imputer_artifact.download())\n",
    "    \n",
    "    imputer_filename_in_artifact = base_config.get(f\"pipeline_artefact_locators_{DATASET_IDENTIFIER}\", {})\\\n",
    "        .get('imputer_fname_pattern', 'simple_imputer_{imputation_strategy}_{dataset_identifier}.joblib').format(\n",
    "            imputation_strategy=base_config.get('preprocessing_config',{}).get('imputation_strategy','median'),\n",
    "            dataset_identifier=DATASET_IDENTIFIER\n",
    "    )\n",
    "    IMPUTER_PATH_FROM_ARTIFACT = imputer_artifact_dir / imputer_filename_in_artifact\n",
    "    \n",
    "    if not IMPUTER_PATH_FROM_ARTIFACT.is_file(): # Fallback\n",
    "        joblib_files_imputer = list(imputer_artifact_dir.glob(\"*.joblib\"))\n",
    "        if joblib_files_imputer: \n",
    "            IMPUTER_PATH_FROM_ARTIFACT = joblib_files_imputer[0]\n",
    "            print(f\"  Warning: Specific imputer file '{imputer_filename_in_artifact}' not found, using first .joblib: {IMPUTER_PATH_FROM_ARTIFACT.name}\")\n",
    "        else: \n",
    "            raise FileNotFoundError(f\"Imputer .joblib file ('{imputer_filename_in_artifact}' or any .joblib) \"\n",
    "                                    f\"not found in downloaded imputer artifact directory: {imputer_artifact_dir}\")\n",
    "    print(f\"  Imputer artifact '{imputer_artifact.name}' downloaded. Local path: {IMPUTER_PATH_FROM_ARTIFACT}\")\n",
    "\n",
    "    # --- Step 3: Fetch Full Configuration from the NB04 Run that Produced these Preprocessors ---\n",
    "    # Use one of the consumed artifacts (e.g., scaler_artifact) to find its producer run (the NB04 run).\n",
    "    nb04_producer_run = imputer_artifact.logged_by()     # Verify if the correct run is used in case of multiple NB04 runs\n",
    "    if nb04_producer_run:\n",
    "        source_nb04_run_id_for_config = nb04_producer_run.id\n",
    "        source_nb04_run_name_for_config = nb04_producer_run.name\n",
    "        print(f\"  Fetching full config from producer NB04 run: '{source_nb04_run_name_for_config}' (ID: {source_nb04_run_id_for_config})\")\n",
    "        \n",
    "        config_from_nb04_producer_run = dict(nb04_producer_run.config) # Convert W&B config to standard dict\n",
    "        \n",
    "        # Validate that the critical keys ('features', 'preprocess') are present in the fetched config\n",
    "        # These keys should have been logged by Notebook 04.\n",
    "        if 'features' not in config_from_nb04_producer_run or \\\n",
    "           'preprocess' not in config_from_nb04_producer_run:\n",
    "            print(\"  ERROR: The config fetched from the producer NB04 run is missing critical 'features' or 'preprocess' sections.\")\n",
    "            print(f\"  Available keys in fetched NB04 config: {list(config_from_nb04_producer_run.keys())}\")\n",
    "            raise ValueError(\"Incomplete configuration fetched from NB04 producer run.\")\n",
    "        \n",
    "        config_for_dataset_instance = config_from_nb04_producer_run \n",
    "        # This now holds the authoritative config (features, preprocess, cnn_model_params, etc.)\n",
    "        # that OASISDataset will use.\n",
    "        \n",
    "        print(f\"  Successfully fetched configuration from NB04 producer run for OASISDataset.\")\n",
    "        \n",
    "        # Update current NB05 run's config with info about the source NB04 run and artifacts used\n",
    "        if run: # Ensure NB05's run object is valid\n",
    "            run.config.update({\n",
    "                \"source_config_details/nb04_producer_run_id\": source_nb04_run_id_for_config,\n",
    "                \"source_config_details/nb04_producer_run_name\": source_nb04_run_name_for_config,\n",
    "                \"source_config_details/scaler_artifact_used_name_version\": scaler_artifact.name, # Logs e.g. \"scaler_standard_oasis2:vX\"\n",
    "                \"source_config_details/imputer_artifact_used_name_version\": imputer_artifact.name,\n",
    "                # Log the key parts of the config that OASISDataset will actually use for clarity\n",
    "                \"dataset_config_used/features\": config_for_dataset_instance.get('features',{}),\n",
    "                \"dataset_config_used/preprocess\": config_for_dataset_instance.get('preprocess',{}),\n",
    "                \"dataset_config_used/cnn_model_params\": config_for_dataset_instance.get('cnn_model_params',{}),\n",
    "                \"dataset_config_used/preprocessing_config_mri\": config_for_dataset_instance.get('preprocessing_config',{})\n",
    "            }, allow_val_change=True)\n",
    "            print(\"  NB05 W&B run config updated with source NB04 run info and the dataset config to be used.\")\n",
    "    else: \n",
    "        raise ConnectionError(\"Could not retrieve the W&B run that produced the preprocessor artifacts \"\n",
    "                              \"(via artifact.logged_by()). Definitive configuration cannot be fetched.\")\n",
    "\n",
    "except wandb.errors.CommError as e_wandb_comm:\n",
    "    print(f\"CRITICAL W&B Communication Error: {e_wandb_comm}\")\n",
    "    print(\"  This could be due to invalid artifact names, types, versions, or general W&B API issues.\")\n",
    "    print(f\"  Attempted to use Scaler: {WANDB_ENTITY}/{WANDB_PROJECT}/{SCALER_ARTIFACT_NAME}:{PREPROCESSOR_ARTIFACT_VERSION}\")\n",
    "    print(f\"  Attempted to use Imputer: {WANDB_ENTITY}/{WANDB_PROJECT}/{IMPUTER_ARTIFACT_NAME}:{PREPROCESSOR_ARTIFACT_VERSION}\")\n",
    "    if run: run.finish(exit_code=1)\n",
    "    # exit()\n",
    "except FileNotFoundError as e_fnf_artifact:\n",
    "    print(f\"CRITICAL FileNotFoundError after attempting artifact download: {e_fnf_artifact}\")\n",
    "    if run: run.finish(exit_code=1)\n",
    "    # exit()\n",
    "except Exception as e_artifact_other:\n",
    "    print(f\"CRITICAL ERROR loading preprocessor artifacts or fetching config from NB04 run: {e_artifact_other}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    if run: run.finish(exit_code=1)\n",
    "    # exit()\n",
    "\n",
    "# Ensure config_for_dataset_instance is somewhat populated for OASISDataset,\n",
    "# even if with fallbacks from base_config, to prevent immediate errors if the above failed\n",
    "# and execution wasn't halted by an exit() or re-raised error.\n",
    "if not config_for_dataset_instance or 'features' not in config_for_dataset_instance or 'preprocess' not in config_for_dataset_instance :\n",
    "    print(\"CRITICAL WARNING: 'config_for_dataset_instance' (from NB04 W&B run) is not correctly populated.\")\n",
    "    print(\"  OASISDataset will likely fail or use incorrect default features.\")\n",
    "    # Define minimal fallback structure based on base_config if critical failure occurred\n",
    "    config_for_dataset_instance.setdefault('features', base_config.get('feature_definitions_fallback', {}).get('features_for_model', {'time_varying': [], 'static': []}))\n",
    "    config_for_dataset_instance.setdefault('preprocess', base_config.get('feature_definitions_fallback', {}).get('preprocess_details_for_model', {'imputation_cols': [], 'scaling_cols': []}))\n",
    "    config_for_dataset_instance.setdefault('cnn_model_params', base_config.get('cnn_model_params', {})) \n",
    "    config_for_dataset_instance.setdefault('preprocessing_config', base_config.get('preprocessing_config', {})) \n",
    "    if run: run.config.update({\"warning_nb04_config_fetch_failed\": True}, allow_val_change=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae601155",
   "metadata": {},
   "source": [
    "## 4. Instantiate `OASISDataset` Instances\n",
    "\n",
    "With all necessary configurations, data paths, and preprocessor paths now available (preprocessor paths pointing to locally downloaded W&B Artifacts, and feature/preprocessing configurations fetched from the authoritative Notebook 04 W&B run), we instantiate the custom `OASISDataset` class.\n",
    "\n",
    "Separate instances are created for the training and validation data splits (`cohort_train.parquet` and `cohort_validation.parquet`, respectively). The `OASISDataset` will:\n",
    "* Load the specified Parquet data file for the split.\n",
    "* Internally load the fitted imputer and scaler using the provided artifact paths.\n",
    "* Apply imputation and scaling to the appropriate columns based on the fetched `config_for_dataset_instance`.\n",
    "* Handle the selection of time-varying and static features as defined in `config_for_dataset_instance`.\n",
    "* Perform encoding of categorical features (e.g., 'M/F' to 'M/F_encoded') if indicated by the feature configuration.\n",
    "* Optionally load and prepare corresponding MRI scan data if the `TEST_WITH_MRI` flag is set to `True`.\n",
    "\n",
    "This step critically tests the core data loading, preprocessing application, and feature assembly logic encapsulated within the `OASISDataset` class, ensuring it uses the precise settings defined by Notebook 04."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1d87ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate OASISDataset for Training and Validation Sets ---\n",
    "print(\"\\n--- Instantiating Datasets ---\")\n",
    "\n",
    "# Initialize dataset variables to ensure they are defined\n",
    "train_dataset: OASISDataset | None = None\n",
    "val_dataset: OASISDataset | None = None\n",
    "\n",
    "# --- Prerequisite Variable Check ---\n",
    "# These variables are expected to be defined and populated from the preceding cells:\n",
    "#   TRAIN_DATA_PATH (Path object to train parquet file)\n",
    "#   VAL_DATA_PATH (Path object to validation parquet file)\n",
    "#   SCALER_PATH_FROM_ARTIFACT (Path object to downloaded scaler.joblib)\n",
    "#   IMPUTER_PATH_FROM_ARTIFACT (Path object to downloaded imputer.joblib)\n",
    "#   config_for_dataset_instance (dict: config fetched from NB04 W&B run for OASISDataset)\n",
    "#   MRI_DATA_DIR (Path object to preprocessed MRI scans directory)\n",
    "#   TEST_WITH_MRI (bool: flag to include MRI data in this test run)\n",
    "#   DATASET_IDENTIFIER (str: e.g., \"oasis2\") - for logging/consistency, though not directly used by OASISDataset here\n",
    "#   run (wandb.Run object for NB05) - for potential logging within this cell\n",
    "\n",
    "required_vars_for_dataset_instantiation = {\n",
    "    'TRAIN_DATA_PATH': TRAIN_DATA_PATH,\n",
    "    'VAL_DATA_PATH': VAL_DATA_PATH,\n",
    "    'SCALER_PATH_FROM_ARTIFACT': SCALER_PATH_FROM_ARTIFACT,\n",
    "    'IMPUTER_PATH_FROM_ARTIFACT': IMPUTER_PATH_FROM_ARTIFACT,\n",
    "    'config_for_dataset_instance': config_for_dataset_instance,\n",
    "    'MRI_DATA_DIR': MRI_DATA_DIR,\n",
    "    'TEST_WITH_MRI': TEST_WITH_MRI \n",
    "    # Note: DATASET_IDENTIFIER and run are used for logging, not direct OASISDataset args\n",
    "}\n",
    "\n",
    "proceed_with_instantiation = True\n",
    "for var_name, var_value in required_vars_for_dataset_instantiation.items():\n",
    "    if var_value is None: # Check for None explicitly\n",
    "        print(f\"  CRITICAL ERROR: Prerequisite variable '{var_name}' is None.\")\n",
    "        proceed_with_instantiation = False\n",
    "    # Also check if config_for_dataset_instance is empty or missing crucial keys\n",
    "    if var_name == 'config_for_dataset_instance' and \\\n",
    "       (not var_value or 'features' not in var_value or 'preprocess' not in var_value):\n",
    "        print(f\"  CRITICAL ERROR: 'config_for_dataset_instance' is empty or missing 'features'/'preprocess' keys.\")\n",
    "        # from pprint import pprint; pprint(var_value) # Uncomment for detailed debug if needed\n",
    "        proceed_with_instantiation = False\n",
    "        \n",
    "if not proceed_with_instantiation:\n",
    "    print(\"Halting dataset instantiation due to missing or invalid prerequisites from previous cells.\")\n",
    "    if run: run.finish(exit_code=1) # Mark W&B run as failed\n",
    "    # exit() # Or raise an error\n",
    "else:\n",
    "    try:\n",
    "        print(f\"\\nInstantiating train_dataset for {DATASET_IDENTIFIER.upper()} (MRI Included: {TEST_WITH_MRI})...\")\n",
    "        train_dataset = OASISDataset(\n",
    "            data_parquet_path=TRAIN_DATA_PATH,\n",
    "            scaler_path=SCALER_PATH_FROM_ARTIFACT,   # Use path to downloaded artifact\n",
    "            imputer_path=IMPUTER_PATH_FROM_ARTIFACT, # Use path to downloaded artifact\n",
    "            config=config_for_dataset_instance,      # Use config fetched from NB04 W&B run\n",
    "            mri_data_dir=MRI_DATA_DIR if TEST_WITH_MRI else None,\n",
    "            include_mri=TEST_WITH_MRI \n",
    "        )\n",
    "        num_train_subjects = len(train_dataset)\n",
    "        print(f\"  Train dataset created successfully. Number of subjects (sequences): {num_train_subjects}\")\n",
    "        if run: run.log({f'dataset_test_{DATASET_IDENTIFIER}/train_subject_count': num_train_subjects})\n",
    "\n",
    "\n",
    "        print(f\"\\nInstantiating val_dataset for {DATASET_IDENTIFIER.upper()} (MRI Included: {TEST_WITH_MRI})...\")\n",
    "        val_dataset = OASISDataset(\n",
    "            data_parquet_path=VAL_DATA_PATH,\n",
    "            scaler_path=SCALER_PATH_FROM_ARTIFACT,   # Use the SAME scaler/imputer from training\n",
    "            imputer_path=IMPUTER_PATH_FROM_ARTIFACT,\n",
    "            config=config_for_dataset_instance,      # Use the SAME config from NB04 W&B run\n",
    "            mri_data_dir=MRI_DATA_DIR if TEST_WITH_MRI else None,\n",
    "            include_mri=TEST_WITH_MRI\n",
    "        )\n",
    "        num_val_subjects = len(val_dataset)\n",
    "        print(f\"  Validation dataset created successfully. Number of subjects (sequences): {num_val_subjects}\")\n",
    "        if run: run.log({f'dataset_test_{DATASET_IDENTIFIER}/val_subject_count': num_val_subjects})\n",
    "\n",
    "    except FileNotFoundError as e_fnf_ds_init:\n",
    "        print(f\"CRITICAL ERROR during OASISDataset instantiation: A required file was not found - {e_fnf_ds_init}\")\n",
    "        print(\"  This could be an issue with TRAIN_DATA_PATH, VAL_DATA_PATH, or the downloaded artifact paths \"\n",
    "              \"(SCALER_PATH_FROM_ARTIFACT, IMPUTER_PATH_FROM_ARTIFACT). Verify paths from previous cells.\")\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "    except KeyError as e_key_ds_init:\n",
    "        print(f\"CRITICAL ERROR during OASISDataset instantiation: Missing a key in 'config_for_dataset_instance' - {e_key_ds_init}\")\n",
    "        print(\"  Ensure the configuration fetched from the Notebook 04 W&B run contains the complete \"\n",
    "              \"'features' and 'preprocess' dictionaries with all expected subkeys.\")\n",
    "        # from pprint import pprint; print(\"DEBUG: config_for_dataset_instance being passed to OASISDataset:\"); pprint(config_for_dataset_instance)\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "    except Exception as e_ds_init_other:\n",
    "        print(f\"An unexpected CRITICAL ERROR occurred during OASISDataset instantiation: {e_ds_init_other}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "\n",
    "# Final check to ensure datasets were created for the next cell\n",
    "if train_dataset is None or val_dataset is None:\n",
    "    print(\"CRITICAL ERROR: Dataset instantiation failed. Cannot proceed to DataLoader creation in the next cell.\")\n",
    "    # exit() # This would halt the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32469038",
   "metadata": {},
   "source": [
    "## 5. Create DataLoaders\n",
    "\n",
    "Wrap the `OASISDataset` instances (for training and validation sets) in PyTorch `DataLoader`s. The `DataLoader` is responsible for:\n",
    "* Batching the data (grouping multiple subjects/sequences together).\n",
    "* Shuffling the training data before each epoch (optional, but good practice).\n",
    "* **Critically, using the custom `pad_collate_fn`** (from `src/datasets.py`). This function takes lists of sequences (which can have varying lengths within a batch) and pads them into uniform PyTorch tensors suitable for input into sequence models. It also generates sequence `lengths` and boolean `masks` indicating real vs. padded elements.\n",
    "* Optionally using multiple worker processes for efficient data loading in the background (for this test, `num_workers=0` is often used for simpler debugging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c4797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create DataLoaders ---\n",
    "print(\"\\n--- Creating DataLoaders ---\")\n",
    "\n",
    "# BATCH_SIZE_FOR_TESTING and TEST_WITH_MRI should be defined in a setup cell\n",
    "# pad_collate_fn should be imported from src.datasets\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "\n",
    "if 'train_dataset' in locals() and train_dataset is not None and \\\n",
    "   'val_dataset' in locals() and val_dataset is not None:\n",
    "    \n",
    "    print(f\"  Using BATCH_SIZE: {BATCH_SIZE_FOR_TESTING}\")\n",
    "    # num_workers=0 is often best for local debugging to avoid multiprocessing complexities.\n",
    "    # persistent_workers=False if num_workers > 0, can help with some OS/environment issues.\n",
    "    # For num_workers=0, persistent_workers has no effect and can be omitted.\n",
    "    \n",
    "    try:\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BATCH_SIZE_FOR_TESTING,\n",
    "            shuffle=True, # Shuffle training data each epoch for better generalization\n",
    "            collate_fn=pad_collate_fn,\n",
    "            num_workers=0 \n",
    "            # persistent_workers=False if HP_analysis.get('num_workers',0) > 0 else False # Example from training\n",
    "        )\n",
    "        print(f\"  Training DataLoader created. Number of batches: ~{len(train_loader)}\")\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=BATCH_SIZE_FOR_TESTING,\n",
    "            shuffle=False, # No need to shuffle validation/test data\n",
    "            collate_fn=pad_collate_fn,\n",
    "            num_workers=0\n",
    "        )\n",
    "        print(f\"  Validation DataLoader created. Number of batches: ~{len(val_loader)}\")\n",
    "\n",
    "    except Exception as e_dataloader:\n",
    "        print(f\"CRITICAL ERROR creating DataLoaders: {e_dataloader}\")\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "else:\n",
    "    print(\"Skipping DataLoader creation as datasets are not available.\")\n",
    "\n",
    "# Ensure loaders are defined for subsequent cells\n",
    "if train_loader is None or val_loader is None:\n",
    "    print(\"CRITICAL ERROR: DataLoader creation failed. Cannot proceed to batch iteration test.\")\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a66cbf",
   "metadata": {},
   "source": [
    "## 6. Test Batch Iteration and Verify Batch Contents\n",
    "\n",
    "To confirm that the entire data loading and preprocessing pipeline works as expected, this section iterates through a small number of batches yielded by the `train_loader`. For each test batch, it unpacks all components generated by `pad_collate_fn`:\n",
    "* `sequences_tabular_padded`\n",
    "* `sequences_mri_padded` (if MRI data is included)\n",
    "* `lengths` (original sequence lengths)\n",
    "* `targets`\n",
    "* `masks` (boolean masks for padding)\n",
    "\n",
    "The shapes, data types, and some example values or summaries of these components are printed. This allows for careful verification that the data is correctly formatted for input into a PyTorch sequence model.\n",
    "\n",
    "**Key things to check in the output:**\n",
    "* **No errors** during batch iteration.\n",
    "* `sequences_tabular_padded` shape: `(batch_size, max_seq_len_in_batch, num_tabular_features)`. Dtype: `torch.float32`.\n",
    "* `sequences_mri_padded` shape (if `TEST_WITH_MRI=True`): `(batch_size, max_seq_len_in_batch, C, D, H, W)`. Dtype: `torch.float32`.\n",
    "* `lengths` shape: `(batch_size,)`. Dtype: `torch.int64`. Values should match actual pre-padding sequence lengths.\n",
    "* `targets` shape: `(batch_size, 1)` or `(batch_size,)`. Dtype: `torch.float32`.\n",
    "* `masks` shape: `(batch_size, max_seq_len_in_batch)`. Dtype: `torch.bool`. Values (`True`/`False`) must align with `lengths`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45803f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Batch Iteration from Train Loader ---\n",
    "print(\"\\n--- Testing Batch Iteration from Training DataLoader ---\")\n",
    "\n",
    "# NUM_BATCHES_TO_INSPECT and TEST_WITH_MRI should be defined in a setup cell\n",
    "\n",
    "if 'train_loader' in locals() and train_loader is not None:\n",
    "    if len(train_loader) == 0:\n",
    "        print(\"  Warning: train_loader is empty. Cannot iterate through batches.\")\n",
    "    else:\n",
    "        print(f\"  Iterating through the first {NUM_BATCHES_TO_INSPECT} batch(es) \"\n",
    "              f\"(MRI Included in this test: {TEST_WITH_MRI})...\")\n",
    "        \n",
    "        # Initialize variables to store last batch info for W&B logging, ensure they exist\n",
    "        last_batch_tabular_shape_log = None\n",
    "        last_batch_mri_shape_log = None\n",
    "        last_batch_lengths_log = None\n",
    "        last_batch_targets_shape_log = None\n",
    "\n",
    "        for i, batch_content in enumerate(train_loader):\n",
    "            if i >= NUM_BATCHES_TO_INSPECT:\n",
    "                break\n",
    "\n",
    "            print(f\"\\n--- Contents of Batch {i+1}/{NUM_BATCHES_TO_INSPECT} ---\")\n",
    "            \n",
    "            # Unpack the batch based on whether MRI data is included\n",
    "            # This logic must align with pad_collate_fn's return tuple\n",
    "            if TEST_WITH_MRI: # Expecting 5 items\n",
    "                if len(batch_content) == 5:\n",
    "                    sequences_tabular_padded, sequences_mri_padded, lengths, targets, masks = batch_content\n",
    "                else:\n",
    "                    print(f\"  ERROR: Expected 5 items in batch when TEST_WITH_MRI=True, but got {len(batch_content)}. Skipping this batch display.\")\n",
    "                    continue\n",
    "            else: # Expecting 4 items (tabular only)\n",
    "                if len(batch_content) == 4:\n",
    "                    sequences_tabular_padded, lengths, targets, masks = batch_content\n",
    "                    sequences_mri_padded = None # Explicitly None\n",
    "                else:\n",
    "                    print(f\"  ERROR: Expected 4 items in batch when TEST_WITH_MRI=False, but got {len(batch_content)}. Skipping this batch display.\")\n",
    "                    continue\n",
    "\n",
    "            print(f\"  Tabular Sequences Tensor Shape: {sequences_tabular_padded.shape}\") \n",
    "            print(f\"  Tabular Sequences Tensor Type: {sequences_tabular_padded.dtype}\") \n",
    "            \n",
    "            if sequences_mri_padded is not None:\n",
    "                print(f\"  MRI Sequences Tensor Shape: {sequences_mri_padded.shape}\") \n",
    "                print(f\"  MRI Sequences Tensor Type: {sequences_mri_padded.dtype}\")\n",
    "                last_batch_mri_shape_log = list(sequences_mri_padded.shape)\n",
    "            else:\n",
    "                print(f\"  MRI Sequences Tensor: Not included in this batch/test.\")\n",
    "                last_batch_mri_shape_log = \"Not_Included\"\n",
    "\n",
    "            print(f\"  Lengths Tensor Shape: {lengths.shape}\") \n",
    "            print(f\"  Lengths Tensor Type: {lengths.dtype}\") \n",
    "            print(f\"  Lengths Tensor Values (first {min(5, len(lengths))}): {lengths.tolist()[:min(5, len(lengths.tolist()))]}\") \n",
    "            \n",
    "            print(f\"  Targets Tensor Shape: {targets.shape}\") \n",
    "            print(f\"  Targets Tensor Type: {targets.dtype}\") \n",
    "            print(f\"  Targets Tensor Values (first {min(5, len(targets.tolist()))}): {targets.squeeze().tolist()[:min(5, len(targets.tolist()))]}\") \n",
    "            \n",
    "            print(f\"  Masks Tensor Shape: {masks.shape}\") \n",
    "            print(f\"  Masks Tensor Type: {masks.dtype}\") \n",
    "\n",
    "            # Store shapes from the last iterated batch for logging\n",
    "            last_batch_tabular_shape_log = list(sequences_tabular_padded.shape)\n",
    "            last_batch_lengths_log = lengths.tolist()[:min(5, len(lengths.tolist()))] # Log a sample\n",
    "            last_batch_targets_shape_log = list(targets.shape)\n",
    "\n",
    "            # Print a slice of a sequence and its mask to verify padding\n",
    "            if sequences_tabular_padded.shape[0] > 0 and sequences_tabular_padded.shape[1] > 0:\n",
    "                print(\"\\n  Example Tabular Sequence (First Sample in Batch, First 5 steps, First 3 features):\")\n",
    "                print(sequences_tabular_padded[0, :min(5, sequences_tabular_padded.shape[1]), :min(3, sequences_tabular_padded.shape[2])])\n",
    "                print(\"\\n  Corresponding Mask (First Sample in Batch, First 5 steps):\")\n",
    "                print(masks[0, :min(5, masks.shape[1])])\n",
    "                print(f\"  (Original length for this first sample was: {lengths[0].item()})\")\n",
    "        \n",
    "        # Log info about the last batch tested to W&B\n",
    "        if run:\n",
    "            log_payload_nb05 = {\n",
    "                \"dataloader_test/status\": \"completed_iteration\",\n",
    "                'dataloader_test/num_batches_inspected': i + 1 if 'i' in locals() else 0\n",
    "            }\n",
    "            if last_batch_tabular_shape_log:\n",
    "                 log_payload_nb05['dataloader_test/last_batch_tab_shape'] = last_batch_tabular_shape_log\n",
    "            if last_batch_mri_shape_log:\n",
    "                 log_payload_nb05['dataloader_test/last_batch_mri_shape'] = last_batch_mri_shape_log\n",
    "            if last_batch_lengths_log:\n",
    "                 log_payload_nb05['dataloader_test/last_batch_lengths_example'] = last_batch_lengths_log\n",
    "            if last_batch_targets_shape_log:\n",
    "                log_payload_nb05['dataloader_test/last_batch_target_shape'] = last_batch_targets_shape_log\n",
    "            run.log(log_payload_nb05)\n",
    "            print(\"\\n  Logged batch test summary to W&B.\")\n",
    "\n",
    "        print(f\"\\n--- Finished inspecting {NUM_BATCHES_TO_INSPECT} batch(es). ---\")\n",
    "        print(\"Review shapes, dtypes, lengths, and masks to ensure correctness.\")\n",
    "else:\n",
    "    print(\"Skipping batch iteration test as train_loader is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2835368",
   "metadata": {},
   "source": [
    "## 7. Finalize W&B Run\n",
    "\n",
    "Complete the execution for this DataLoader testing notebook (NB05) and finish the associated Weights & Biases run. This ensures all queued logs and information about the test are uploaded to the W&B platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f76a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Finish W&B Run for Notebook 05 ---\n",
    "print(f\"\\n--- {NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER} (DataLoader Test) complete. Finishing W&B run. ---\")\n",
    "\n",
    "if run: # Check if 'run' object exists and is an active W&B run\n",
    "    try:\n",
    "        # Ensure any final summary or status is updated if needed\n",
    "        run.summary.update({\"overall_notebook_status\": \"Completed Successfully\"})\n",
    "        \n",
    "        run.finish()\n",
    "        run_name_to_print = run.name_synced if hasattr(run, 'name_synced') and run.name_synced else \\\n",
    "                            run.name if hasattr(run, 'name') and run.name else \\\n",
    "                            run.id if hasattr(run, 'id') else \"current NB05 run\"\n",
    "        print(f\"W&B run '{run_name_to_print}' finished successfully.\")\n",
    "    except Exception as e_finish_run_nb05:\n",
    "        print(f\"Error during wandb.finish() for Notebook 05: {e_finish_run_nb05}\")\n",
    "        print(\"The run may not have finalized correctly on the W&B server.\")\n",
    "else:\n",
    "    print(\"No active W&B run to finish for this session.\")\n",
    "\n",
    "print(f\"\\n--- Notebook {NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER} execution finished. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d4d61",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "If the iteration through batches completed successfully and the printed shapes, data types, sequence lengths, targets, and masks appear correct for both tabular and (if tested) MRI data streams, it indicates that the data loading pipeline (`OASISDataset` and `pad_collate_fn`) is functioning as expected with the preprocessors and configurations derived from Notebook 04. The data is now confirmed to be in the appropriate format for input into sequence models for training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro_pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
