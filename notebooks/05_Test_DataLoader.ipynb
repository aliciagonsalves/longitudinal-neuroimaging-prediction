{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: notebooks/05_Test_DataLoader.ipynb\n",
    "# Purpose: Test the custom OASISDataset and pad_collate_fn\n",
    "#          to ensure data loading, preprocessing (using saved objects),\n",
    "#          sequencing, padding, and batching works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Notebook 05: Test Data Loading Pipeline\n",
    "\n",
    "**Purpose:** Verify that the custom `OASISDataset` class and `pad_collate_fn` (defined in `src/datasets.py`) work correctly. This involves:\n",
    "1.  Loading the pre-split data (`.parquet` files from NB 03).\n",
    "2.  Loading and applying the pre-fitted preprocessors (`.joblib` files from NB 04).\n",
    "3.  Correctly structuring data into sequences grouped by subject.\n",
    "4.  Handling variable sequence lengths via padding and masking within batches.\n",
    "5.  Generating batches in the expected format (shapes, data types) for input into a PyTorch sequence model.\n",
    "\n",
    "**Input:**\n",
    "* `cohort_{train|validation}.parquet` (Output from NB 03)\n",
    "* `standard_scaler.joblib` (Output from NB 04)\n",
    "* `simple_imputer_median.joblib` (Output from NB 04)\n",
    "* `src/datasets.py` (Contains `OASISDataset` and `pad_collate_fn`)\n",
    "\n",
    "**Output:** Console output displaying the properties (shapes, types, example values) of generated batches for verification. No data files are saved by this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary libraries (torch, pandas, DataLoader, joblib, etc.). Critically, import the custom `OASISDataset` and `pad_collate_fn` from the `src` directory (ensuring `src` is added to the Python path). Load project configuration from `config.json` to retrieve paths to data splits and saved preprocessors. Define DataLoader parameters like `BATCH_SIZE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch # Assuming PyTorch\n",
    "from torch.utils.data import DataLoader\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add src directory to Python path to allow importing datasets ---\n",
    "try:\n",
    "    # Assumes notebook is in notebooks/ and src/ is parallel\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "    print(f\"Added {module_path} to sys.path\")\n",
    "    \n",
    "    # --- Import custom dataset class and collate function ---\n",
    "    from src.datasets import OASISDataset, pad_collate_fn\n",
    "    print(\"Successfully imported OASISDataset and pad_collate_fn from src/datasets.py\")\n",
    "except ModuleNotFoundError:\n",
    "     print(\"Error: Could not import from src/datasets.py.\")\n",
    "     print(\"Ensure the file exists in the 'src' directory parallel to 'notebooks'.\")\n",
    "     print(f\"Current working directory: {os.getcwd()}\")\n",
    "     print(f\"Sys path: {sys.path}\")\n",
    "     exit()\n",
    "except Exception as e:\n",
    "     print(f\"An unexpected error occurred during import: {e}\")\n",
    "     exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config Loading ---\n",
    "print(\"\\n--- Loading Configuration ---\")\n",
    "CONFIG_PATH = Path('../config.json')\n",
    "run = None # Initialize run to None globally for the notebook if needed elsewhere\n",
    "config = {} # Initialize config\n",
    "# Define default lists in case W&B fails - adjust these if needed or handle error differently\n",
    "time_varying_cols = []\n",
    "static_cols = []\n",
    "scaling_cols = []\n",
    "imputation_cols = []\n",
    "\n",
    "try:\n",
    "    PROJECT_ROOT = CONFIG_PATH.parent.resolve()\n",
    "    with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    print(\"Configuration loaded successfully.\")\n",
    "\n",
    "    # Define paths from config\n",
    "    OUTPUT_DIR_BASE = PROJECT_ROOT / config['data']['output_dir_base']\n",
    "    NB03_OUTPUT_DIR = OUTPUT_DIR_BASE / \"03_Feature_Engineering_Splitting\"\n",
    "    NB04_OUTPUT_DIR = OUTPUT_DIR_BASE / \"04_Fit_Preprocessors\"\n",
    "\n",
    "    # Paths to data splits\n",
    "    TRAIN_DATA_PATH = NB03_OUTPUT_DIR / \"cohort_train.parquet\"\n",
    "    VAL_DATA_PATH = NB03_OUTPUT_DIR / \"cohort_validation.parquet\"\n",
    "    # TEST_DATA_PATH = NB03_OUTPUT_DIR / \"cohort_test.parquet\" # Not to be tested here\n",
    "\n",
    "    # Paths to saved preprocessors\n",
    "    SCALER_PATH = NB04_OUTPUT_DIR / \"standard_scaler.joblib\"\n",
    "    IMPUTER_PATH = NB04_OUTPUT_DIR / \"simple_imputer_median.joblib\"\n",
    "\n",
    "    # --- Define W&B Project/Entity for potential use ---\n",
    "    WANDB_PROJECT = config['wandb']['project_name']\n",
    "    WANDB_ENTITY = config['wandb'].get('entity', None)\n",
    "\n",
    "    # Basic check if required data/preprocessor files exist\n",
    "    if not TRAIN_DATA_PATH.is_file() or not VAL_DATA_PATH.is_file():\n",
    "         raise FileNotFoundError(f\"Train ({TRAIN_DATA_PATH.is_file()}) or Validation ({VAL_DATA_PATH.is_file()}) parquet file not found.\")\n",
    "    if not SCALER_PATH.is_file() or not IMPUTER_PATH.is_file():\n",
    "         raise FileNotFoundError(f\"Scaler ({SCALER_PATH.is_file()}) or Imputer ({IMPUTER_PATH.is_file()}) joblib file not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during setup before W&B init: {e}\")\n",
    "    # Decide if you can proceed without config/paths or exit\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize W&B and Load Configuration from Prior Run ---\n",
    "print(\"\\n--- Initializing W&B to load configuration from NB04 Run ---\")\n",
    "\n",
    "# *** IMPORTANT: Replace 'RUN_ID_FROM_NB04' below with the actual W&B Run ID ***\n",
    "# Obtain this from the output of your successful NB04 run or from the W&B UI.\n",
    "PRIOR_RUN_ID = \"RUN_ID_FROM_NB04\" \n",
    "\n",
    "try:\n",
    "    run = wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        entity=WANDB_ENTITY,\n",
    "        id=PRIOR_RUN_ID,\n",
    "        resume=\"allow\", # Allows fetching config/summary from a finished run\n",
    "        job_type=\"create-dataloaders\", # Job type for this notebook\n",
    "        name=f\"NB05_dataloader_using_{PRIOR_RUN_ID.split('/')[-1]}\", # Create a descriptive name\n",
    "        config=config # Pass the base config loaded from json initially\n",
    "    )\n",
    "    print(f\"W&B run initialized. Attached to prior run: {run.url}\")\n",
    "\n",
    "    # --- Load feature/preprocess lists from the fetched run.config ---\n",
    "    # Use .get() for safety, providing empty list as default if key missing\n",
    "    # Access using the nested structure logged by NB04\n",
    "    features_config = run.config.get('features', {})\n",
    "    preprocess_config = run.config.get('preprocess', {})\n",
    "\n",
    "    time_varying_cols = features_config.get('time_varying', [])\n",
    "    static_cols = features_config.get('static', [])\n",
    "    scaling_cols = preprocess_config.get('scaling_cols', [])\n",
    "    imputation_cols = preprocess_config.get('imputation_cols', [])\n",
    "\n",
    "    # Check if lists were loaded successfully\n",
    "    if not time_varying_cols or not static_cols or not scaling_cols:\n",
    "         print(\"⚠️ Warning: Feature or scaling lists loaded from W&B config are empty!\")\n",
    "         print(f\"  Loaded time_varying: {time_varying_cols}\")\n",
    "         print(f\"  Loaded static: {static_cols}\")\n",
    "         print(f\"  Loaded scaling: {scaling_cols}\")\n",
    "         print(f\"  Loaded imputation: {imputation_cols}\")\n",
    "         print(\"  Check the config of the W&B run specified:\", PRIOR_RUN_ID)\n",
    "         # Decide if you should exit or proceed with empty lists\n",
    "         # exit()\n",
    "    else:\n",
    "        print(\"Feature and preprocess lists loaded successfully from W&B config.\")\n",
    "        print(f\"  Time Varying: {time_varying_cols}\")\n",
    "        print(f\"  Static: {static_cols}\")\n",
    "        print(f\"  Scaling: {scaling_cols}\")\n",
    "        print(f\"  Imputation: {imputation_cols}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing W&B or loading config from run '{PRIOR_RUN_ID}': {e}\")\n",
    "    print(\"Proceeding with potentially empty feature lists defined earlier (or default).\")\n",
    "    # 'run' will be None if wandb.init failed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters for DataLoader ---\n",
    "BATCH_SIZE = 4 # Use a small batch size for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Instantiate Datasets\n",
    "\n",
    "Create instances of the `OASISDataset` class for the training and validation splits, providing the paths to the data files and the saved preprocessor objects. The `config` dictionary is passed to provide feature lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Instantiating Datasets ---\")\n",
    "try:\n",
    "    print(\"Creating train_dataset...\")\n",
    "    train_dataset = OASISDataset(\n",
    "        data_parquet_path=TRAIN_DATA_PATH,\n",
    "        scaler_path=SCALER_PATH,\n",
    "        imputer_path=IMPUTER_PATH,\n",
    "        config=run.config # Pass the loaded config dictionary\n",
    "    )\n",
    "    print(f\"Train dataset created with {len(train_dataset)} subjects.\")\n",
    "\n",
    "    print(\"\\nCreating val_dataset...\")\n",
    "    val_dataset = OASISDataset(\n",
    "        data_parquet_path=VAL_DATA_PATH,\n",
    "        scaler_path=SCALER_PATH, # Use the SAME scaler/imputer fitted on train data\n",
    "        imputer_path=IMPUTER_PATH,\n",
    "        config=run.config\n",
    "    )\n",
    "    print(f\"Validation dataset created with {len(val_dataset)} subjects.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error instantiating datasets: {e}\")\n",
    "    # Potentially print more debug info or raise\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Create DataLoaders\n",
    "\n",
    "Wrap the `OASISDataset` instances in PyTorch `DataLoader`. The DataLoader is responsible for:\n",
    "* Batching the data (grouping multiple subjects/sequences together).\n",
    "* Shuffling the training data before each epoch (optional but recommended).\n",
    "* Using the custom `pad_collate_fn` to take lists of sequences (potentially of different lengths) and pad them into uniform tensors suitable for model input, while also providing sequence lengths and masks.\n",
    "* Potentially using multiple worker processes for efficiency (set `num_workers=0` for easier debugging initially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Creating DataLoaders ---\")\n",
    "# Use num_workers=0 for easier debugging initially, can increase later for speed if needed\n",
    "# persistent_workers=False is often needed on some systems esp. Windows with num_workers > 0\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True, # Shuffle training data each epoch\n",
    "    collate_fn=pad_collate_fn,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False\n",
    ")\n",
    "print(f\"Total batches calculated by DataLoader: {len(train_loader)}\")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # No need to shuffle validation data\n",
    "    collate_fn=pad_collate_fn,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "print(\"DataLoaders created.\")\n",
    "print(f\"Number of batches in train_loader: ~{len(train_loader)}\")\n",
    "print(f\"Number of batches in val_loader: ~{len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Test Batch Iteration\n",
    "\n",
    "To verify the pipeline, we iterate through a small number of batches yielded by the `train_loader`. For each batch, we unpack the contents (padded sequences, lengths, targets, masks) and print their shapes, data types, and some example values or summaries.\n",
    "\n",
    "**Key things to check in the output:**\n",
    "* Are there any errors during iteration?\n",
    "* Is the shape of `sequences_padded` `(batch_size, max_seq_len_in_batch, num_features)`?\n",
    "* Is the shape of `lengths` `(batch_size,)` and do the values match the actual sequence lengths before padding (verifiable by looking at the `masks`)?\n",
    "* Is the shape of `targets` `(batch_size, 1)` or `(batch_size,)`?\n",
    "* Is the shape of `masks` `(batch_size, max_seq_len_in_batch)` and is its dtype `torch.bool`?\n",
    "* Do the mask values (`True`/`False`) correctly correspond to the `lengths` tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing Batch Iteration (Train Loader) ---\")\n",
    "\n",
    "num_batches_to_test = 2\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i >= num_batches_to_test:\n",
    "        break\n",
    "\n",
    "    print(f\"\\n--- Batch {i+1} ---\")\n",
    "    # Unpack the batch (output from pad_collate_fn)\n",
    "    sequences_padded, lengths, targets, masks = batch\n",
    "\n",
    "    print(f\"Sequences Tensor Shape: {sequences_padded.shape}\") # Should be (batch_size, max_seq_len_in_batch, num_features)\n",
    "    print(f\"Sequences Tensor Type: {sequences_padded.dtype}\") # Should be torch.float32\n",
    "    print(f\"Lengths Tensor Shape: {lengths.shape}\") # Should be (batch_size,)\n",
    "    print(f\"Lengths Tensor Type: {lengths.dtype}\") # Should be torch.int64\n",
    "    print(f\"Lengths Tensor Values: {lengths.tolist()}\") # Show actual lengths\n",
    "    print(f\"Targets Tensor Shape: {targets.shape}\") # Should be (batch_size, 1)\n",
    "    print(f\"Targets Tensor Type: {targets.dtype}\") # Should be torch.float32\n",
    "    print(f\"Targets Tensor Values:\\n{targets.squeeze().tolist()}\") # Show target values\n",
    "    print(f\"Masks Tensor Shape: {masks.shape}\") # Should be (batch_size, max_seq_len_in_batch)\n",
    "    print(f\"Masks Tensor Type: {masks.dtype}\") # Should be torch.bool\n",
    "\n",
    "    # Optional: Print a slice of a sequence and its mask to check padding\n",
    "    if sequences_padded.shape[0] > 0 and sequences_padded.shape[1] > 5: # If batch not empty and sequence long enough\n",
    "         print(\"\\nExample Sequence (First 5 steps, First Sample):\")\n",
    "         print(sequences_padded[0, :5, :]) # Print first 5 time steps for first subject in batch\n",
    "         print(\"\\nCorresponding Mask (First 5 steps, First Sample):\")\n",
    "         print(masks[0, :5])\n",
    "         print(f\"(Original length for this sample was: {lengths[0].item()})\")\n",
    "\n",
    "\n",
    "print(f\"\\n--- Finished testing {num_batches_to_test} batches. ---\")\n",
    "print(\"Check shapes, types, lengths, and masks carefully.\")\n",
    "\n",
    "# Optional: Initialize W&B here if you want to log test results\n",
    "# run_test = wandb.init(...)\n",
    "# run_test.log({'test_batch_sequence_shape': list(sequences_padded.shape)})\n",
    "# run_test.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Finish W&B Run ---\n",
    "print(\"\\n--- DataLoader Testing complete. Finishing W&B run. ---\")\n",
    "if run:\n",
    "    # Optional: Log success confirmation or any test metrics\n",
    "    # run.log({\"dataloader_test_status\": \"success\"})\n",
    "    run.finish()\n",
    "    print(\"W&B run finished.\")\n",
    "else:\n",
    "    print(\"No active W&B run to finish.\")\n",
    "\n",
    "print(\"\\nScript execution finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "If the batches iterated successfully and the shapes/types look correct (sequences are padded, lengths match, targets are present, masks align with lengths), the data loading pipeline defined in `src/datasets.py` is working correctly with the preprocessors fitted in Notebook 04. The data is now ready to be used by a sequence model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro_pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
