{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63957a30",
   "metadata": {},
   "source": [
    "# Notebook 08: OASIS-2 Model Analysis - Interpretability & Uncertainty\n",
    "\n",
    "**Project Phase:** 1 (Model Analysis & Evaluation)\n",
    "**Dataset:** OASIS-2 Longitudinal MRI & Clinical Data\n",
    "\n",
    "**Purpose:**\n",
    "This notebook is dedicated to in-depth analysis of trained models (both baseline LSTM and hybrid CNN+LSTM) for predicting CDR progression using the OASIS-2 dataset. Key objectives include:\n",
    "1.  Loading pre-trained model checkpoints and their original training configurations from Weights & Biases (W&B) artifacts and runs.\n",
    "2.  Instantiating the `OASISDataset` with the correct feature sets and preprocessor configurations (sourced from the W&B config of the relevant Notebook 04 run that fitted the preprocessors, which is linked through the training run's config).\n",
    "3.  Performing **Uncertainty Quantification** using MC Dropout to understand model confidence.\n",
    "4.  Conducting **Model Interpretability** analyses:\n",
    "    * **Permutation Feature Importance** for tabular features.\n",
    "    * **Integrated Gradients** (or other saliency methods via Captum) for the 3D CNN component of the hybrid model to identify salient input MRI regions.\n",
    "    * **SHAP (SHapley Additive exPlanations)** to understand feature contributions, focusing on the baseline LSTM and the fusion stage of the hybrid model.\n",
    "5.  Logging all analysis results, visualizations, and summary tables to a new W&B run for this notebook, enabling comparison and reporting in Notebook 09.\n",
    "6.  Saving generated plots and analysis summaries locally in a run-specific output directory.\n",
    "\n",
    "**Workflow:**\n",
    "1.  **Setup:** Import libraries, configure `sys.path`, load `config.json`. Define analysis parameters.\n",
    "2.  **Path Resolution:** Use `get_dataset_paths` to resolve paths for test data, training data (for SHAP background), and preprocessor files (though preprocessor paths will be confirmed/overridden by the model's training config).\n",
    "3.  **W&B Initialization:** Start a new W&B run for this analysis notebook using `initialize_wandb_run`. Define a run-specific output directory.\n",
    "4.  **Main Analysis Loop (Iterate through models to analyze):**\n",
    "    * **Dynamic Model Selection:** Fetch W&B run paths for models to analyze (e.g., based on tags or job types, or from a predefined list).\n",
    "    * **Load Model & Training Config:** Use `load_model_from_wandb_artifact` to load the model and its original training `config`.\n",
    "    * **Prepare `config_for_this_model_dataset`:** Combine the loaded model's training config (which includes feature lists from NB03 and preprocessor choices from NB04) with `base_config` to create the definitive configuration for `OASISDataset`.\n",
    "    * **Instantiate `analysis_dataset` and `analysis_loader`:** Use the test data.\n",
    "    * **Run Selected Analyses (based on flags):**\n",
    "        * MC Dropout Analysis.\n",
    "        * Permutation Feature Importance.\n",
    "        * CNN Interpretability (Integrated Gradients) - for hybrid models.\n",
    "        * SHAP Analysis (Baseline LSTM, Hybrid Fusion Stage).\n",
    "    * Store results for each model.\n",
    "5.  **Summarize & Log Overall Results:** Create a summary table of all analyses for all models and log to W&B.\n",
    "6.  **Finalize W&B Run.**\n",
    "\n",
    "**Input:**\n",
    "* `config.json`: Main project configuration file.\n",
    "* **W&B Run Paths/IDs of trained models (from Notebook 06 & 07)**: These runs contain the model artifacts and their training configurations (which in turn reference the NB04 run's config for features/preprocessing).\n",
    "* `cohort_test.parquet` (for main analysis) and `cohort_train.parquet` (for SHAP background) - outputs from Notebook 03.\n",
    "* Paths to preprocessor `.joblib` files - these will be confirmed/derived from the loaded model's `original_training_config` which should point back to NB04's artifacts/config.\n",
    "* `src/` utility modules.\n",
    "\n",
    "**Output:**\n",
    "* **Local Files (in run-specific output directory for this NB08 run, e.g., `notebooks/outputs/08_Model_Analysis_OASIS2/<run_name>/`):**\n",
    "    * Plots from MC Dropout, Permutation Importance, Integrated Gradients, SHAP.\n",
    "    * Potentially, tables of results.\n",
    "* **W&B Run (for this Notebook 08 execution):**\n",
    "    * Logged analysis configuration.\n",
    "    * For each analyzed model:\n",
    "        * MC Dropout metrics and plots.\n",
    "        * Permutation Importance scores and plot.\n",
    "        * Integrated Gradients visualizations (for hybrid).\n",
    "        * SHAP summary plots.\n",
    "    * A final summary table comparing key analysis results across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: notebooks/08_Model_Analysis_Interpretability_Uncertainty_OASIS2.ipynb\n",
    "# Purpose: Load trained models, perform uncertainty quantification (MC Dropout),\n",
    "#          and various interpretability analyses (Permutation Importance, Integrated Gradients, SHAP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba307670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Libraries & Imports ---\n",
    "import wandb\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "# joblib might be used by OASISDataset if it were to load preprocessors directly,\n",
    "# but here paths passed to it are from downloaded artifacts or resolved.\n",
    "from tqdm.auto import tqdm # tqdm is used by utility functions\n",
    "import time\n",
    "import shap\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn # For criterion definition if needed\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add src directory to Python path ---\n",
    "\n",
    "# Initialize\n",
    "PROJECT_ROOT = None \n",
    "try:\n",
    "    current_notebook_path = Path.cwd() \n",
    "    PROJECT_ROOT = current_notebook_path.parent \n",
    "    if not (PROJECT_ROOT / \"src\").exists(): \n",
    "        PROJECT_ROOT = current_notebook_path\n",
    "    if str(PROJECT_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    print(f\"PROJECT_ROOT: {PROJECT_ROOT}\") # Changed from \"Attempting to use\"\n",
    "    if not (PROJECT_ROOT / \"src\").exists():\n",
    "        raise FileNotFoundError(\"Could not reliably find 'src' directory from PROJECT_ROOT.\")\n",
    "except Exception as e_path:\n",
    "    print(f\"Error setting up PROJECT_ROOT and sys.path: {e_path}\")\n",
    "    # exit() \n",
    "\n",
    "# --- Import Custom Modules & Functions ---\n",
    "try:\n",
    "    from src.datasets import OASISDataset, pad_collate_fn\n",
    "    from src.models import BaselineLSTMRegressor, ModularLateFusionLSTM \n",
    "    from src.wandb_utils import initialize_wandb_run, load_model_from_wandb_artifact\n",
    "    from src.plotting_utils import finalize_plot\n",
    "    from src.paths_utils import get_dataset_paths, get_notebook_run_output_dir # Changed from get_dataset_paths\n",
    "    from src.uncertainty_utils import get_mc_dropout_predictions, calculate_uncertainty_metrics \n",
    "    from src.interpretability_utils import (\n",
    "         calculate_permutation_importance,\n",
    "         generate_integrated_gradients_cnn,\n",
    "         explain_lstm_with_shap,\n",
    "         explain_hybrid_fusion_with_shap\n",
    "     )\n",
    "    from src.evaluation_utils import evaluate_model \n",
    "    print(\"Successfully imported all required custom modules and classes.\")\n",
    "except ModuleNotFoundError as e_mod:\n",
    "    print(f\"ModuleNotFoundError: {e_mod}. Ensure all src/ files exist and sys.path is correct.\")\n",
    "    # exit()\n",
    "except ImportError as e_imp:\n",
    "    print(f\"ImportError: {e_imp}. Check for circular dependencies or errors within src modules.\")\n",
    "    # exit()\n",
    "except Exception as e_gen_imp:\n",
    "    print(f\"An unexpected error occurred during custom module imports: {e_gen_imp}\")\n",
    "    # exit()\n",
    "\n",
    "\n",
    "# --- Load Main Project Configuration ---\n",
    "print(\"\\n--- Loading Main Project Configuration ---\")\n",
    "base_config = {}\n",
    "WANDB_ENTITY = None\n",
    "WANDB_PROJECT = None\n",
    "try:\n",
    "    if PROJECT_ROOT is None: raise ValueError(\"PROJECT_ROOT not defined.\")\n",
    "    CONFIG_PATH_MAIN = PROJECT_ROOT / 'config.json'\n",
    "    with open(CONFIG_PATH_MAIN, 'r', encoding='utf-8') as f:\n",
    "        base_config = json.load(f)\n",
    "    WANDB_ENTITY = base_config.get('wandb', {}).get('entity')\n",
    "    WANDB_PROJECT = base_config.get('wandb', {}).get('project_name')\n",
    "    if not WANDB_ENTITY or not WANDB_PROJECT:\n",
    "        raise KeyError(\"'wandb:entity' or 'wandb:project_name' not found in config.json.\")\n",
    "    print(f\"Main project config loaded. W&B Entity: {WANDB_ENTITY}, Project: {WANDB_PROJECT}\")\n",
    "except Exception as e_cfg:\n",
    "    print(f\"CRITICAL ERROR loading main config.json or W&B details: {e_cfg}\")\n",
    "    # exit()\n",
    "\n",
    "\n",
    "# --- Device Setup ---\n",
    "print(\"\\n--- Setting up PyTorch Device ---\")\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU).\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA and MPS not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d681a324",
   "metadata": {},
   "source": [
    "## 2. Configuration for Notebook 08 Analysis\n",
    "\n",
    "This section defines the parameters and settings specific to this analysis notebook:\n",
    "* **Models to Analyze (`MODELS_TO_ANALYZE_FETCH`):** A dictionary specifying which trained models (by their W&B Run Path) will be loaded and analyzed. This will be updated to dynamically fetch runs.\n",
    "* **MC Dropout Parameters:** `N_MC_SAMPLES` (number of Monte Carlo forward passes).\n",
    "* **Detailed Analysis Sample Size:** `N_SAMPLES_FOR_DETAILED_MC_ANALYSIS` (e.g., for per-sample SHAP or IG visualizations).\n",
    "* **Analysis Hyperparameters (`HP_analysis`):** A dictionary holding other analysis-specific settings, including:\n",
    "    * `analysis_batch_size`: Batch size for the `analysis_loader`.\n",
    "    * Boolean flags to control which analyses are run (e.g., `RUN_MC_DROPOUT`, `RUN_PERMUTATION_IMPORTANCE`, `RUN_INTEGRATED_GRADIENTS`, `RUN_SHAP_BASELINE`, `RUN_SHAP_HYBRID_FUSION`).\n",
    "    * Parameters for specific interpretability methods (e.g., `shap_num_background_samples`, `ig_n_steps`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc61b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration for Notebook 08 Analysis ---\n",
    "print(\"\\n--- Configuring Notebook 08 Analysis Parameters ---\")\n",
    "\n",
    "DATASET_IDENTIFIER = \"oasis2\" # Ensure this is consistent\n",
    "NOTEBOOK_MODULE_NAME = \"08_Model_Analysis_Interpretability_Uncertainty\"\n",
    "# Key from config.json locators for this notebook's output subfolder\n",
    "NB08_OUTPUT_LOCATOR_KEY = base_config.get(f\"pipeline_artefact_locators_{DATASET_IDENTIFIER}\", {})\\\n",
    "                                     .get(\"analysis_subdir\", \"08_Model_Analysis_Default_Outputs\")\n",
    "\n",
    "\n",
    "# --- Dynamic Fetching of Models to Analyze (Replaces hardcoded MODELS_TO_ANALYZE_FETCH) ---\n",
    "print(\"\\n--- Dynamically Fetching Trained Model Runs from W&B for Analysis ---\")\n",
    "MODELS_TO_ANALYZE_FETCH = {}\n",
    "try:\n",
    "    if WANDB_ENTITY and WANDB_PROJECT: # Ensure these are defined from base_config\n",
    "        wandb_api = wandb.Api(timeout=29) # Set a timeout for API calls\n",
    "        \n",
    "        # Define criteria for runs to analyze\n",
    "        # This uses the job_type defined by initialize_wandb_run in NB06 and NB07\n",
    "        model_run_criteria = {\n",
    "            \"BaselineLSTM_OASIS2_Best\": {\n",
    "                \"job_type\": f\"Training-06-BaselineLSTM-{DATASET_IDENTIFIER}\", \n",
    "                # Add tags here if you use them, e.g., \"tags\": \"best_candidate_baseline\"\n",
    "            },\n",
    "            \"HybridCNNLSTM_OASIS2_Best\": {\n",
    "                \"job_type\": f\"Training-07-HybridCNNLSTM-{DATASET_IDENTIFIER}\", \n",
    "                # Add tags here if you use them, e.g., \"tags\": \"best_candidate_hybrid\"\n",
    "            }\n",
    "            # Add more criteria for other models if needed\n",
    "        }\n",
    "\n",
    "        for nickname, criteria in model_run_criteria.items():\n",
    "            filters = {\"$and\": [{\"state\": \"finished\"}]} # Start with finished runs\n",
    "            if \"job_type\" in criteria and criteria[\"job_type\"]: # Check if job_type is not None or empty\n",
    "                filters[\"$and\"].append({\"job_type\": criteria[\"job_type\"]})\n",
    "            if \"tags\" in criteria and criteria[\"tags\"]: # Check if tags are not None or empty\n",
    "                # Ensure tags are passed as a list if it's a single tag string for \"$in\"\n",
    "                tags_to_filter = criteria[\"tags\"]\n",
    "                if isinstance(tags_to_filter, str):\n",
    "                    tags_to_filter = [tags_to_filter]\n",
    "                filters[\"$and\"].append({\"tags\": {\"$in\": tags_to_filter}})\n",
    "            \n",
    "            print(f\"  Querying W&B for '{nickname}' with Job Type: '{criteria.get('job_type', 'Any')}' and Tags: '{criteria.get('tags', 'Any')}'\")\n",
    "            \n",
    "            selected_runs = wandb_api.runs(\n",
    "                path=f\"{WANDB_ENTITY}/{WANDB_PROJECT}\",\n",
    "                filters=filters,\n",
    "                order=\"-created_at\" # Get the most recent run satisfying criteria\n",
    "            )\n",
    "            \n",
    "            if selected_runs:\n",
    "                # Take the first run from the ordered list\n",
    "                MODELS_TO_ANALYZE_FETCH[nickname] = selected_runs[0].path # \"entity/project/run_id\"\n",
    "                print(f\"    Found for '{nickname}': {selected_runs[0].name} (Run Path: {selected_runs[0].path})\")\n",
    "            else:\n",
    "                print(f\"    WARNING: No runs found via W&B API for '{nickname}' with specified criteria.\")\n",
    "                # Fallback to your manually defined MODELS_TO_ANALYZE if dynamic fetch fails\n",
    "                if 'MODELS_TO_ANALYZE' in locals() and isinstance(MODELS_TO_ANALYZE, dict):\n",
    "                    manual_fallback_path = MODELS_TO_ANALYZE.get(nickname)\n",
    "                    if manual_fallback_path and \"RUN_ID_\" not in manual_fallback_path.upper(): # Check if not placeholder\n",
    "                        MODELS_TO_ANALYZE_FETCH[nickname] = manual_fallback_path\n",
    "                        print(f\"      Using manually specified fallback path for '{nickname}': {manual_fallback_path}\")\n",
    "                    else:\n",
    "                        print(f\"      No valid manual fallback path found for '{nickname}' in MODELS_TO_ANALYZE.\")\n",
    "        \n",
    "        if not MODELS_TO_ANALYZE_FETCH:\n",
    "            print(\"WARNING: No models were successfully fetched dynamically or found as valid manual fallbacks. \"\n",
    "                  \"The main analysis loop will be empty.\")\n",
    "        else:\n",
    "            print(\"\\nFinal list of models to be analyzed in this session:\")\n",
    "            for name, path in MODELS_TO_ANALYZE_FETCH.items(): \n",
    "                print(f\"  - {name}: {path}\")\n",
    "\n",
    "    else: # WANDB_ENTITY or WANDB_PROJECT were not defined\n",
    "        print(\"WARNING: WANDB_ENTITY or WANDB_PROJECT not defined from base_config. Cannot dynamically fetch runs.\")\n",
    "        # Fallback to using the manually defined MODELS_TO_ANALYZE dictionary\n",
    "        if 'MODELS_TO_ANALYZE' in locals() and isinstance(MODELS_TO_ANALYZE, dict):\n",
    "            MODELS_TO_ANALYZE_FETCH = {\n",
    "                name: path for name, path in MODELS_TO_ANALYZE.items() \n",
    "                if \"RUN_ID_\" not in path.upper() # Ensure it's not a placeholder\n",
    "            }\n",
    "            if MODELS_TO_ANALYZE_FETCH:\n",
    "                print(f\"Using manually defined MODELS_TO_ANALYZE_FETCH: {MODELS_TO_ANALYZE_FETCH}\")\n",
    "            else:\n",
    "                print(\"Manually defined MODELS_TO_ANALYZE is also empty or contains only placeholders.\")\n",
    "        else:\n",
    "            print(\"Manually defined MODELS_TO_ANALYZE dictionary not found.\")\n",
    "\n",
    "except Exception as e_fetch_runs:\n",
    "    print(f\"An error occurred during dynamic fetching of W&B runs: {e_fetch_runs}\")\n",
    "    print(\"Falling back to manually defined MODELS_TO_ANALYZE_FETCH if available, or it will be empty.\")\n",
    "    # Ensure MODELS_TO_ANALYZE_FETCH exists, even if empty, to prevent later NameErrors\n",
    "    if 'MODELS_TO_ANALYZE_FETCH' not in locals():\n",
    "        MODELS_TO_ANALYZE_FETCH = {}\n",
    "    if not MODELS_TO_ANALYZE_FETCH and 'MODELS_TO_ANALYZE' in locals() and isinstance(MODELS_TO_ANALYZE, dict):\n",
    "        MODELS_TO_ANALYZE_FETCH = {\n",
    "            name: path for name, path in MODELS_TO_ANALYZE.items() \n",
    "            if \"RUN_ID_\" not in path.upper()\n",
    "        }\n",
    "        if MODELS_TO_ANALYZE_FETCH:\n",
    "             print(f\"Using manually defined MODELS_TO_ANALYZE_FETCH due to error: {MODELS_TO_ANALYZE_FETCH}\")\n",
    "\n",
    "\n",
    "# --- Analysis-Specific Hyperparameters & Control Flags ---\n",
    "N_MC_SAMPLES = 30  \n",
    "N_SAMPLES_FOR_DETAILED_ANALYSIS = 5 # Reduced for quicker local analysis runs\n",
    "\n",
    "HP_analysis = {\n",
    "    'analysis_batch_size': 4,        # Batch size for DataLoaders created in this notebook\n",
    "    'num_detailed_samples': N_SAMPLES_FOR_DETAILED_ANALYSIS,\n",
    "\n",
    "    # --- Flags to control which analyses are run ---\n",
    "    'RUN_MC_DROPOUT': True,\n",
    "    'RUN_PERMUTATION_IMPORTANCE': True,\n",
    "    'RUN_INTEGRATED_GRADIENTS': True, # For CNN part of hybrid models\n",
    "    'RUN_SHAP_BASELINE': True,        # SHAP for baseline LSTM\n",
    "    'RUN_SHAP_HYBRID_FUSION': True,   # SHAP for hybrid model's fusion stage\n",
    "\n",
    "    # --- Parameters for specific interpretability methods ---\n",
    "    'pfi_num_permutations': 5, # For Permutation Feature Importance\n",
    "    'ig_n_steps': 10,          # n_steps for Integrated Gradients (keep low for local runs)\n",
    "    'ig_captum_internal_batch_size': None, # Optional for Captum's IG\n",
    "\n",
    "    'shap_num_background_samples': 20, # Max samples for SHAP background dataset\n",
    "    'shap_background_batch_size': 10,  # Batch size for creating SHAP background tensor\n",
    "    'shap_explain_batch_size': N_SAMPLES_FOR_DETAILED_ANALYSIS, # How many test instances to explain with SHAP\n",
    "    'shap_kernel_nsamples': 50, # nsamples for KernelExplainer (for hybrid fusion SHAP)\n",
    "    'shap_kmeans_k': 5         # k for shap.kmeans for KernelExplainer background\n",
    "}\n",
    "print(\"\\nAnalysis hyperparameters (HP_analysis) and control flags defined.\")\n",
    "# from pprint import pprint; print(\"HP_analysis:\"); pprint(HP_analysis) # Optional debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e120ced4",
   "metadata": {},
   "source": [
    "## 3. Define Data Paths for Analysis\n",
    "\n",
    "Resolve paths to the necessary data files using the `get_dataset_paths` utility. This notebook primarily requires:\n",
    "* **Test Data (`cohort_test.parquet`):** For evaluating model performance and running most interpretability analyses.\n",
    "* **Training Data (`cohort_train.parquet`):** Specifically needed as background data for SHAP explanations.\n",
    "* **Preprocessor Paths (`scaler.joblib`, `imputer.joblib`):** While `OASISDataset` will ultimately use preprocessor paths defined in the *training run's configuration* (fetched from NB04 via NB06/07 config), resolving these paths here based on the *current main `config.json`* can be useful for direct instantiation of `OASISDataset` if needed for specific SHAP background data preparation, or as a reference. The `config_for_this_model_dataset` prepared later will ensure the correct preprocessors tied to each model are used.\n",
    "* **MRI Data Directory:** For hybrid models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50105fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Data Paths for Analysis ---\n",
    "print(\"\\n--- Defining Data Paths for Analysis using utility ---\")\n",
    "\n",
    "# DATASET_IDENTIFIER should be \"oasis2\" (defined in setup)\n",
    "# PROJECT_ROOT and base_config should be available from setup\n",
    "\n",
    "TEST_DATA_PATH_NB08 = None\n",
    "TRAIN_DATA_PATH_FOR_SHAP_BG_NB08 = None\n",
    "# SCALER_PATH_NB08 and IMPUTER_PATH_NB08 will be derived from the specific model's training config later\n",
    "# However, we can resolve the *expected* paths based on current config for direct use if needed (e.g. SHAP bg dataset)\n",
    "EXPECTED_SCALER_PATH_NB08 = None\n",
    "EXPECTED_IMPUTER_PATH_NB08 = None\n",
    "MRI_DATA_DIR_NB08 = None\n",
    "\n",
    "try:\n",
    "    # Get paths for the \"analysis\" stage, which includes test data and also train data (for SHAP bg)\n",
    "    # get_dataset_paths with stage=\"analysis\" should return 'test_data_parquet' and 'train_data_for_analysis_bg'\n",
    "    # It also returns scaler_path and imputer_path based on current config.json, which can serve as a reference\n",
    "    # or for instantiating OASISDataset directly for SHAP background if needed.\n",
    "    analysis_stage_paths = get_dataset_paths(PROJECT_ROOT, base_config, DATASET_IDENTIFIER, stage=\"analysis\")\n",
    "    \n",
    "    TEST_DATA_PATH_NB08 = analysis_stage_paths.get('test_data_parquet')\n",
    "    TRAIN_DATA_PATH_FOR_SHAP_BG_NB08 = analysis_stage_paths.get('train_data_for_analysis_bg') # This key is from paths_utils\n",
    "    if TRAIN_DATA_PATH_FOR_SHAP_BG_NB08 is None: # Fallback if 'train_data_for_analysis_bg' isn't a key\n",
    "        TRAIN_DATA_PATH_FOR_SHAP_BG_NB08 = analysis_stage_paths.get('train_data_parquet')\n",
    "\n",
    "    EXPECTED_SCALER_PATH_NB08 = analysis_stage_paths.get('scaler_path') \n",
    "    EXPECTED_IMPUTER_PATH_NB08 = analysis_stage_paths.get('imputer_path')\n",
    "    MRI_DATA_DIR_NB08 = analysis_stage_paths.get('mri_data_dir')\n",
    "\n",
    "    if not all([TEST_DATA_PATH_NB08, TRAIN_DATA_PATH_FOR_SHAP_BG_NB08, \n",
    "                EXPECTED_SCALER_PATH_NB08, EXPECTED_IMPUTER_PATH_NB08, MRI_DATA_DIR_NB08]):\n",
    "        raise ValueError(\"One or more essential paths for NB08 analysis could not be resolved from get_dataset_paths.\")\n",
    "\n",
    "    print(f\"  Test Data Path (for analysis_dataset): {TEST_DATA_PATH_NB08}\")\n",
    "    print(f\"  Train Data Path (for SHAP background): {TRAIN_DATA_PATH_FOR_SHAP_BG_NB08}\")\n",
    "    print(f\"  Expected Scaler Path (reference): {EXPECTED_SCALER_PATH_NB08}\")\n",
    "    print(f\"  Expected Imputer Path (reference): {EXPECTED_IMPUTER_PATH_NB08}\")\n",
    "    print(f\"  MRI Data Directory: {MRI_DATA_DIR_NB08}\")\n",
    "\n",
    "    # Verify existence of data files needed\n",
    "    if not TEST_DATA_PATH_NB08.is_file(): raise FileNotFoundError(f\"Test data parquet not found: {TEST_DATA_PATH_NB08}\")\n",
    "    if not TRAIN_DATA_PATH_FOR_SHAP_BG_NB08.is_file(): raise FileNotFoundError(f\"Train data for SHAP background not found: {TRAIN_DATA_PATH_FOR_SHAP_BG_NB08}\")\n",
    "    # Preprocessor files will be effectively checked when OASISDataset tries to load them via paths from config_for_this_model_dataset\n",
    "    if not MRI_DATA_DIR_NB08.is_dir(): raise FileNotFoundError(f\"MRI Data Directory not found: {MRI_DATA_DIR_NB08}\")\n",
    "    print(\"Key data paths for NB08 resolved and inputs verified.\")\n",
    "\n",
    "except (KeyError, ValueError, FileNotFoundError) as e_paths_nb08:\n",
    "    print(f\"CRITICAL ERROR during path setup for NB08: {e_paths_nb08}\")\n",
    "    # exit()\n",
    "except Exception as e_general_nb08_paths:\n",
    "    print(f\"CRITICAL ERROR during path setup for NB08: {e_general_nb08_paths}\")\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd8d162",
   "metadata": {},
   "source": [
    "## 4. Initialize Weights & Biases Run for Notebook 08 Analysis\n",
    "\n",
    "A new W&B run is initiated for this comprehensive model analysis notebook (NB08). This run will track:\n",
    "* The configuration parameters specific to this analysis execution (e.g., list of models analyzed, MC Dropout settings, SHAP parameters).\n",
    "* All generated plots from uncertainty and interpretability analyses.\n",
    "* Summary tables or metrics comparing different models or analysis results.\n",
    "A run-specific local output directory is also created for saving plots and other files generated by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize W&B Run for this Analysis Notebook (NB08) ---\n",
    "print(\"\\n--- Initializing Weights & Biases Run for Notebook 08 Analysis ---\")\n",
    "\n",
    "# Configuration specific to this NB08 run\n",
    "nb08_run_config_log = {\n",
    "    \"notebook_name_code\": f\"{NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER}\",\n",
    "    \"dataset_source\": DATASET_IDENTIFIER,\n",
    "    # MODELS_TO_ANALYZE_FETCH will be populated after dynamic fetching\n",
    "    \"models_to_be_analyzed_sources\": MODELS_TO_ANALYZE_FETCH if 'MODELS_TO_ANALYZE_FETCH' in locals() and MODELS_TO_ANALYZE_FETCH else \"Manual List or Fetch Failed\",\n",
    "    \"num_mc_samples_configured\": N_MC_SAMPLES,\n",
    "    \"num_detailed_analysis_samples_target\": N_SAMPLES_FOR_DETAILED_ANALYSIS,\n",
    "    \"execution_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "# Merge HP_analysis which contains the boolean flags for analyses and SHAP/IG params\n",
    "if 'HP_analysis' in locals() and isinstance(HP_analysis, dict):\n",
    "    nb08_run_config_log.update(HP_analysis) \n",
    "\n",
    "nb_prefix_nb08 = NOTEBOOK_MODULE_NAME.split('_')[0] if '_' in NOTEBOOK_MODULE_NAME else \"NB\"\n",
    "# Using DATASET_IDENTIFIER in job_specific_type\n",
    "job_specific_type_nb08 = f\"{nb_prefix_nb08}-ModelAnalysis-{DATASET_IDENTIFIER}\" \n",
    "custom_elements_name_nb08 = [nb_prefix_nb08, DATASET_IDENTIFIER.upper(), \"FullAnalysis\"]\n",
    "\n",
    "run_nb08 = initialize_wandb_run( # Assign to run_nb08\n",
    "    base_project_config=base_config,\n",
    "    job_group=\"Analysis\",\n",
    "    job_specific_type=job_specific_type_nb08,\n",
    "    run_specific_config=nb08_run_config_log,\n",
    "    custom_run_name_elements=custom_elements_name_nb08,\n",
    "    notes=f\"Comprehensive Analysis (Uncertainty, Interpretability) for {DATASET_IDENTIFIER.upper()} models.\"\n",
    ")\n",
    "\n",
    "RUN_OUTPUT_DIR_NB08 = None # Initialize for clarity\n",
    "if run_nb08:\n",
    "    print(f\"W&B run for NB08 '{run_nb08.name}' (Job Type: '{run_nb08.job_type}') initialized. View at: {run_nb08.url}\")\n",
    "    # Use the locator key defined in config.json for NB08's output subfolder\n",
    "    # NB08_OUTPUT_LOCATOR_KEY was defined in Cell 3 of this notebook.\n",
    "    RUN_OUTPUT_DIR_NB08 = get_notebook_run_output_dir(\n",
    "        PROJECT_ROOT, base_config, NB08_OUTPUT_LOCATOR_KEY, run_nb08, DATASET_IDENTIFIER\n",
    "    )\n",
    "    print(f\"Outputs for this NB08 run will be saved locally to: {RUN_OUTPUT_DIR_NB08}\")\n",
    "    # Log the actual output directory to W&B config\n",
    "    run_nb08.config.update({\"run_outputs/local_analysis_dir\": str(RUN_OUTPUT_DIR_NB08)}, allow_val_change=True)\n",
    "else:\n",
    "    print(\"W&B run initialization failed for NB08. Local outputs may go to a default fallback path.\")\n",
    "    # Define a fallback RUN_OUTPUT_DIR_NB08 for local-only execution without W&B\n",
    "    # NB08_OUTPUT_LOCATOR_KEY might not be defined if base_config load failed, so use NOTEBOOK_MODULE_NAME\n",
    "    fallback_locator = NB08_OUTPUT_LOCATOR_KEY if 'NB08_OUTPUT_LOCATOR_KEY' in locals() and NB08_OUTPUT_LOCATOR_KEY else \\\n",
    "                       f\"{NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER}_outputs\"\n",
    "    RUN_OUTPUT_DIR_NB08 = get_notebook_run_output_dir(\n",
    "        PROJECT_ROOT, base_config if base_config else {}, # Pass empty dict if base_config failed\n",
    "        fallback_locator, \n",
    "        None, DATASET_IDENTIFIER\n",
    "    )\n",
    "    print(f\"Using fallback local output directory for NB08: {RUN_OUTPUT_DIR_NB08}\")\n",
    "\n",
    "# Ensure RUN_OUTPUT_DIR_NB08 is a Path object for subsequent use\n",
    "if not isinstance(RUN_OUTPUT_DIR_NB08, Path):\n",
    "    RUN_OUTPUT_DIR_NB08 = Path(RUN_OUTPUT_DIR_NB08)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8357dc62",
   "metadata": {},
   "source": [
    "## 5. Main Analysis Loop: Iterate Through Models\n",
    "\n",
    "This section iterates through the models specified in `MODELS_TO_ANALYZE_FETCH`. For each model:\n",
    "1.  **Load Model and Original Training Configuration:** The `load_model_from_wandb_artifact` utility is used to download the model artifact (e.g., the 'best' version) from the specified W&B training run and instantiate the model. The original W&B configuration from that training run (`original_model_train_config`) is also fetched.\n",
    "2.  **Prepare `config_for_this_model_dataset`:** A definitive configuration dictionary is prepared for instantiating `OASISDataset`. This uses the `features`, `preprocess`, `cnn_model_params`, and `preprocessing_config` (for MRI suffix) that were logged to the W&B config of the *original training run* of the model (which themselves were sourced from the relevant Notebook 04 run). This ensures that `OASISDataset` processes data for analysis in a way that is perfectly consistent with how data was processed during that model's training.\n",
    "3.  **Instantiate `analysis_dataset` and `analysis_loader`:** An `OASISDataset` instance is created using the `TEST_DATA_PATH_NB08` and the `config_for_this_model_dataset`. Preprocessor paths for this step should ideally also be derived from `original_model_train_config` if that run logged specific preprocessor artifact versions it used. For simplicity in this phase, we might assume all analyzed models used a common set of preprocessors whose paths (`SCALER_PATH_NB08`, `IMPUTER_PATH_NB08`) were resolved earlier. A `DataLoader` then provides batches for analysis.\n",
    "4.  **Perform Selected Analyses:** Based on the boolean flags set in `HP_analysis` (e.g., `RUN_MC_DROPOUT`), the following analyses are performed:\n",
    "    * **MC Dropout:** Quantifies predictive uncertainty.\n",
    "    * **Permutation Feature Importance:** Assesses tabular feature importance.\n",
    "    * **Integrated Gradients:** (For hybrid models) Visualizes CNN voxel attributions.\n",
    "    * **SHAP Analysis:** Provides SHAP values for baseline LSTM (on tabular features) and for the fusion stage of the hybrid model.\n",
    "5.  **Log Results:** All generated metrics, tables, and plots are logged to the current Notebook 08 W&B run (`run_nb08`) and saved locally to `RUN_OUTPUT_DIR_NB08`.\n",
    "6.  Aggregate summary results for all analyzed models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3d407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Analysis Loop ---\n",
    "print(\"\\n--- Starting Main Analysis Loop for Selected Models ---\")\n",
    "\n",
    "# Ensure necessary global variables for the loop are defined\n",
    "required_globals_for_loop = [\n",
    "    'TEST_DATA_PATH_NB08', 'TRAIN_DATA_PATH_FOR_SHAP_BG_NB08', \n",
    "    'EXPECTED_SCALER_PATH_NB08', 'EXPECTED_IMPUTER_PATH_NB08', 'MRI_DATA_DIR_NB08',\n",
    "    'RUN_OUTPUT_DIR_NB08', 'HP_analysis', 'MODELS_TO_ANALYZE_FETCH', \n",
    "    'base_config', 'device', 'run_nb08', 'N_MC_SAMPLES', 'N_SAMPLES_FOR_DETAILED_ANALYSIS',\n",
    "    'DATASET_IDENTIFIER' # For consistent W&B logging keys/artifact names\n",
    "]\n",
    "if not all(var_name in locals() and locals()[var_name] is not None for var_name in required_globals_for_loop):\n",
    "    print(\"CRITICAL ERROR: One or more global setup variables for the analysis loop are not defined or are None. \"\n",
    "          \"Please check preceding cells (Setup, Config for NB08, Path Definitions, W&B Init for NB08).\")\n",
    "    # exit() # Or raise error\n",
    "else:\n",
    "    print(f\"Analysis outputs will be saved to: {RUN_OUTPUT_DIR_NB08}\")\n",
    "    # Ensure output directory exists (it should have been created by get_notebook_run_output_dir)\n",
    "    if not RUN_OUTPUT_DIR_NB08.exists(): RUN_OUTPUT_DIR_NB08.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "all_models_analysis_results = [] # To store a summary of results for each model\n",
    "\n",
    "for model_nickname, model_training_run_path in MODELS_TO_ANALYZE_FETCH.items():\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(f\"ANALYZING MODEL: {model_nickname} (Source W&B Run: {model_training_run_path})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    current_model_results = {\"model_nickname\": model_nickname, \"source_wandb_run_path\": model_training_run_path}\n",
    "\n",
    "    # --- 1. Load Model and its Original Training Configuration ---\n",
    "    print(f\"  Loading model and original training config for {model_nickname}...\")\n",
    "    loaded_model, original_model_train_config_dict, is_model_hybrid = load_model_from_wandb_artifact(\n",
    "        run_path=model_training_run_path, \n",
    "        base_config_dict=base_config, \n",
    "        device_to_load=device\n",
    "    )\n",
    "\n",
    "    if loaded_model is None:\n",
    "        print(f\"  ERROR: Failed to load model for '{model_nickname}'. Skipping this model.\")\n",
    "        current_model_results[\"status_load_model\"] = \"Failed\"\n",
    "        all_models_analysis_results.append(current_model_results)\n",
    "        if run_nb08: run_nb08.log({f\"{model_nickname}/status/model_load\": \"Failed\"})\n",
    "        continue \n",
    "    \n",
    "    current_model_results[\"model_type_is_hybrid\"] = is_model_hybrid\n",
    "    print(f\"  Model '{model_nickname}' loaded successfully. Type: {'Hybrid' if is_model_hybrid else 'Baseline'}.\")\n",
    "\n",
    "    # --- 2. Prepare `config_for_this_model_dataset` ---\n",
    "    # This uses the *actual configuration* that OASISDataset used during the model's training,\n",
    "    # fetched from the training run's W&B config (logged by NB06/NB07, sourced from NB04).\n",
    "    config_for_this_model_dataset = {\n",
    "        'features': original_model_train_config_dict.get('dataset_config_used/features', {}),\n",
    "        'preprocess': original_model_train_config_dict.get('dataset_config_used/preprocess', {}),\n",
    "        'cnn_model_params': original_model_train_config_dict.get('dataset_config_used/cnn_model_params', \n",
    "                                                                base_config.get('cnn_model_params', {})),\n",
    "        'preprocessing_config': original_model_train_config_dict.get('dataset_config_used/preprocessing_config_mri', \n",
    "                                                                     base_config.get('preprocessing_config', {}))\n",
    "    }\n",
    "    if not config_for_this_model_dataset.get('features') or not config_for_this_model_dataset.get('preprocess'):\n",
    "        print(f\"  WARNING: Critical 'features' or 'preprocess' sections missing in config_for_this_model_dataset \"\n",
    "              f\"derived from run {model_training_run_path}. Analysis might use fallbacks or fail.\")\n",
    "        # Provide minimal fallback if critical keys are missing from original_model_train_config_dict\n",
    "        config_for_this_model_dataset.setdefault('features', {'time_varying':[], 'static':[]})\n",
    "        config_for_this_model_dataset.setdefault('preprocess', {'imputation_cols':[], 'scaling_cols':[]})\n",
    "        config_for_this_model_dataset.setdefault('cnn_model_params', base_config.get('cnn_model_params', {}))\n",
    "        config_for_this_model_dataset.setdefault('preprocessing_config', base_config.get('preprocessing_config', {}))\n",
    "\n",
    "\n",
    "    # --- 3. Instantiate `analysis_dataset` (Test Set) and `analysis_loader` ---\n",
    "    # For analysis, use preprocessors consistent with what this model was trained on.\n",
    "    # EXPECTED_SCALER_PATH_NB08 and EXPECTED_IMPUTER_PATH_NB08 (defined globally in NB08)\n",
    "    # are assumed to be the paths to the .joblib files from the \"official\" NB04 run\n",
    "    # whose config was used by this model's training run (NB06/NB07).\n",
    "    # A more advanced MLOps setup would involve NB06/07 logging the *exact artifact versions*\n",
    "    # of preprocessors they used, and NB08 fetching those specific versions.\n",
    "    # For now, this assumes all analyzed models used preprocessors at EXPECTED_..._PATH.\n",
    "    print(f\"  Instantiating analysis_dataset (test set) for {model_nickname}...\")\n",
    "    try:\n",
    "        analysis_dataset = OASISDataset(\n",
    "            data_parquet_path=TEST_DATA_PATH_NB08,    \n",
    "            scaler_path=EXPECTED_SCALER_PATH_NB08,    # Path to .joblib from an official NB04 run\n",
    "            imputer_path=EXPECTED_IMPUTER_PATH_NB08,  # Path to .joblib from an official NB04 run\n",
    "            config=config_for_this_model_dataset,     # Config from this model's training run (NB04 via NB06/07)\n",
    "            mri_data_dir=MRI_DATA_DIR_NB08 if is_model_hybrid else None,\n",
    "            include_mri=is_model_hybrid\n",
    "        )\n",
    "        analysis_loader = DataLoader(\n",
    "            analysis_dataset,\n",
    "            batch_size=HP_analysis.get('analysis_batch_size', 4), \n",
    "            shuffle=False, \n",
    "            collate_fn=pad_collate_fn,\n",
    "            num_workers=0 \n",
    "        )\n",
    "        print(f\"  Analysis DataLoader created for {model_nickname} with {len(analysis_dataset)} subjects.\")\n",
    "    except Exception as e_ds_load_loop:\n",
    "        print(f\"  ERROR creating analysis dataset/loader for {model_nickname}: {e_ds_load_loop}\")\n",
    "        current_model_results[\"status_dataset_load\"] = f\"Error: {e_ds_load_loop}\"\n",
    "        all_models_analysis_results.append(current_model_results)\n",
    "        if run_nb08: run_nb08.log({f\"{model_nickname}/status/dataset_load\": \"Failed\"})\n",
    "        continue \n",
    "\n",
    "    # --- A. MC Dropout Analysis ---\n",
    "    if HP_analysis.get('RUN_MC_DROPOUT', False):\n",
    "        print(f\"\\n  --- A. MC Dropout Analysis for {model_nickname} ---\")\n",
    "        try:\n",
    "            all_predictions_mc, all_actuals_mc = get_mc_dropout_predictions(\n",
    "                loaded_model, analysis_loader, N_MC_SAMPLES, device\n",
    "            )\n",
    "            if all_predictions_mc and all_actuals_mc: # Ensure both lists are populated\n",
    "                mc_uncertainty_stats = calculate_uncertainty_metrics(all_predictions_mc)\n",
    "                if mc_uncertainty_stats:\n",
    "                    # Filter out NaN values before calculating mean for robustness\n",
    "                    valid_variances = [s['variance'] for s in mc_uncertainty_stats if 'variance' in s and not np.isnan(s['variance'])]\n",
    "                    valid_std_devs = [s['std_dev'] for s in mc_uncertainty_stats if 'std_dev' in s and not np.isnan(s['std_dev'])]\n",
    "                    avg_mc_variance = np.mean(valid_variances) if valid_variances else np.nan\n",
    "                    avg_mc_std_dev = np.mean(valid_std_devs) if valid_std_devs else np.nan\n",
    "                    \n",
    "                    current_model_results[\"Avg_MC_Variance\"] = avg_mc_variance\n",
    "                    current_model_results[\"Avg_MC_Std_Dev\"] = avg_mc_std_dev\n",
    "                    print(f\"    {model_nickname} - Avg MC Variance: {avg_mc_variance:.4e}, Avg MC Std Dev: {avg_mc_std_dev:.4f}\")\n",
    "                    if run_nb08:\n",
    "                        run_nb08.log({\n",
    "                            f\"{model_nickname}/uncertainty/avg_mc_variance\": avg_mc_variance,\n",
    "                            f\"{model_nickname}/uncertainty/avg_mc_std_dev\": avg_mc_std_dev\n",
    "                        })\n",
    "\n",
    "                    # Plotting for MC Dropout\n",
    "                    actuals_np = np.array(all_actuals_mc)\n",
    "                    mean_preds_np = np.array([s['mean'] for s in mc_uncertainty_stats])\n",
    "                    std_devs_np = np.array([s['std_dev'] for s in mc_uncertainty_stats]) # Already filtered for NaNs if valid_std_devs was used\n",
    "\n",
    "                    # 1. Actual vs. Mean Predicted CDR, Colored by Uncertainty\n",
    "                    fig_mc_avp, ax_mc_avp = plt.subplots(figsize=(8, 6))\n",
    "                    scatter = ax_mc_avp.scatter(actuals_np, mean_preds_np, c=std_devs_np, cmap='viridis', alpha=0.7, vmin=0) # Ensure vmin for colorbar\n",
    "                    min_val_plot = min(actuals_np.min(), mean_preds_np.min()) if actuals_np.size > 0 else 0\n",
    "                    max_val_plot = max(actuals_np.max(), mean_preds_np.max()) if actuals_np.size > 0 else 1\n",
    "                    ax_mc_avp.plot([min_val_plot, max_val_plot], [min_val_plot, max_val_plot], 'r--', lw=2, label=\"Ideal\")\n",
    "                    fig_mc_avp.colorbar(scatter, ax=ax_mc_avp, label='Predictive Std Dev (Uncertainty)')\n",
    "                    ax_mc_avp.set_title(f'MC Dropout: Actual vs. Mean Predicted CDR ({model_nickname})')\n",
    "                    ax_mc_avp.set_xlabel('Actual CDR')\n",
    "                    ax_mc_avp.set_ylabel('Mean Predicted CDR')\n",
    "                    ax_mc_avp.legend()\n",
    "                    ax_mc_avp.grid(True, linestyle='--', alpha=0.7)\n",
    "                    finalize_plot(fig_mc_avp, plt, run_nb08, \n",
    "                                  f\"{model_nickname}/uncertainty/plot_actual_vs_pred_by_std\", \n",
    "                                  RUN_OUTPUT_DIR_NB08 / f\"{model_nickname}_mc_actual_vs_pred.png\")\n",
    "\n",
    "                    # 2. Prediction Error vs. Uncertainty\n",
    "                    errors_np = np.abs(actuals_np - mean_preds_np)\n",
    "                    fig_mc_evu, ax_mc_evu = plt.subplots(figsize=(8, 6))\n",
    "                    ax_mc_evu.scatter(std_devs_np, errors_np, alpha=0.7)\n",
    "                    ax_mc_evu.set_title(f'MC Dropout: Prediction Error vs. Uncertainty ({model_nickname})')\n",
    "                    ax_mc_evu.set_xlabel('Predictive Std Dev (Uncertainty)')\n",
    "                    ax_mc_evu.set_ylabel('Absolute Prediction Error |Actual - Mean Pred|')\n",
    "                    ax_mc_evu.grid(True, linestyle='--', alpha=0.7)\n",
    "                    finalize_plot(fig_mc_evu, plt, run_nb08, \n",
    "                                  f\"{model_nickname}/uncertainty/plot_error_vs_std\", \n",
    "                                  RUN_OUTPUT_DIR_NB08 / f\"{model_nickname}_mc_error_vs_uncertainty.png\")\n",
    "                    \n",
    "                    # 3. Histogram of Predictive Standard Deviations\n",
    "                    fig_mc_hist, ax_mc_hist = plt.subplots(figsize=(8, 6))\n",
    "                    sns.histplot(std_devs_np, kde=True, bins=15, ax=ax_mc_hist, stat=\"density\")\n",
    "                    ax_mc_hist.set_title(f'MC Dropout: Distribution of Predictive Std Devs ({model_nickname})')\n",
    "                    ax_mc_hist.set_xlabel('Predictive Std Dev (Uncertainty)')\n",
    "                    ax_mc_hist.set_ylabel('Density')\n",
    "                    finalize_plot(fig_mc_hist, plt, run_nb08, \n",
    "                                  f\"{model_nickname}/uncertainty/plot_uncertainty_dist\", \n",
    "                                  RUN_OUTPUT_DIR_NB08 / f\"{model_nickname}_mc_uncertainty_distribution.png\")\n",
    "                    \n",
    "                    # Display detailed per-sample results table for a few samples\n",
    "                    print(f\"    MC Dropout Results for first {N_SAMPLES_FOR_DETAILED_ANALYSIS} test samples ({model_nickname}):\")\n",
    "                    mc_results_df_list = []\n",
    "                    for i_mc in range(min(N_SAMPLES_FOR_DETAILED_ANALYSIS, len(all_actuals_mc))):\n",
    "                        mc_results_df_list.append({\n",
    "                            \"Sample_Index\": i_mc, \n",
    "                            \"Actual_CDR\": all_actuals_mc[i_mc],\n",
    "                            \"MC_Mean_Pred\": mc_uncertainty_stats[i_mc]['mean'],\n",
    "                            \"MC_Std_Dev\": mc_uncertainty_stats[i_mc]['std_dev'],\n",
    "                            \"MC_Variance\": mc_uncertainty_stats[i_mc]['variance']\n",
    "                        })\n",
    "                    if mc_results_df_list:\n",
    "                        display(pd.DataFrame(mc_results_df_list)) # For Jupyter display\n",
    "                        if run_nb08: # Log as W&B Table\n",
    "                            run_nb08.log({f\"{model_nickname}/uncertainty/detailed_mc_samples_table\": wandb.Table(dataframe=pd.DataFrame(mc_results_df_list))})\n",
    "                else: \n",
    "                    print(f\"    MC Dropout produced empty or invalid uncertainty stats for {model_nickname}.\")\n",
    "                    current_model_results[\"status_mc_dropout\"] = \"EmptyStats\"\n",
    "            else: \n",
    "                print(f\"    MC Dropout did not produce predictions for {model_nickname}.\")\n",
    "                current_model_results[\"status_mc_dropout\"] = \"NoPredictions\"\n",
    "        except Exception as e_mc_main:\n",
    "            print(f\"    ERROR during MC Dropout analysis for {model_nickname}: {e_mc_main}\")\n",
    "            current_model_results[\"status_mc_dropout\"] = f\"Error: {str(e_mc_main)[:100]}\"\n",
    "            import traceback; traceback.print_exc()\n",
    "    else:\n",
    "        print(f\"  Skipping MC Dropout Analysis for {model_nickname} as per RUN_MC_DROPOUT flag.\")\n",
    "\n",
    "\n",
    "    # --- B. Permutation Feature Importance (PFI) ---\n",
    "    if HP_analysis.get('RUN_PERMUTATION_IMPORTANCE', False):\n",
    "        print(f\"\\n  --- B. Permutation Feature Importance for {model_nickname} ---\")\n",
    "        try:\n",
    "            # Get tabular features from the analysis_dataset instance\n",
    "            # model_input_features from OASISDataset includes both time-varying and static (after M/F encoding)\n",
    "            tabular_features_for_pfi = analysis_dataset.model_input_features \n",
    "            if not tabular_features_for_pfi:\n",
    "                print(\"    Skipping PFI: No tabular features found in analysis_dataset.\")\n",
    "            else:\n",
    "                tabular_feature_indices_pfi = {name: i for i, name in enumerate(tabular_features_for_pfi)}\n",
    "                \n",
    "                # Define criterion if not already defined (e.g., if MC Dropout was skipped)\n",
    "                if 'criterion' not in locals() or criterion is None: criterion = nn.MSELoss()\n",
    "                \n",
    "                print(\"    Calculating baseline R2 for PFI using analysis_loader (test set)...\")\n",
    "                model_unpack_flag_pfi = \"hybrid\" if is_model_hybrid else \"baseline\"\n",
    "                baseline_eval_metrics_pfi = evaluate_model(\n",
    "                    loaded_model, analysis_loader, criterion, device, \n",
    "                    model_name_for_batch_unpack=model_unpack_flag_pfi\n",
    "                )\n",
    "                baseline_r2_for_pfi = baseline_eval_metrics_pfi.get('r2', np.nan) # Default to NaN\n",
    "                current_model_results[\"PFI_Baseline_R2\"] = baseline_r2_for_pfi\n",
    "                print(f\"    Baseline R2 for PFI: {baseline_r2_for_pfi:.4f}\")\n",
    "\n",
    "                if not np.isnan(baseline_r2_for_pfi): # Proceed only if baseline R2 is valid\n",
    "                    perm_importance_results = calculate_permutation_importance(\n",
    "                        loaded_model, analysis_loader, \n",
    "                        feature_names_to_permute=tabular_features_for_pfi,\n",
    "                        tabular_feature_indices=tabular_feature_indices_pfi,\n",
    "                        metric_fn=lambda y_true, y_pred: r2_score(np.array(y_true), np.array(y_pred)), # Ensure numpy arrays\n",
    "                        baseline_score=baseline_r2_for_pfi,\n",
    "                        device=device, is_hybrid_model=is_model_hybrid,\n",
    "                        num_permutations=HP_analysis.get('pfi_num_permutations', 5)\n",
    "                    )\n",
    "                    current_model_results[\"PFI_Importances_R2_Drop\"] = perm_importance_results\n",
    "                    print(f\"    {model_nickname} - PFI (Drop in R2):\")\n",
    "                    # Print top N important features, e.g., top 10 or all if fewer than 10\n",
    "                    num_pfi_to_print = min(10, len(perm_importance_results))\n",
    "                    for feat, imp in sorted(perm_importance_results.items(), key=lambda item: item[1], reverse=True)[:num_pfi_to_print]:\n",
    "                        print(f\"      {feat}: {imp:.4f}\")\n",
    "                    \n",
    "                    if run_nb08:\n",
    "                        run_nb08.log({f\"{model_nickname}/interpretability/pfi_r2_drop_dict\": perm_importance_results})\n",
    "                        if perm_importance_results:\n",
    "                            perm_df_pfi = pd.DataFrame(\n",
    "                                list(perm_importance_results.items()), \n",
    "                                columns=['Feature', 'Importance_R2_Drop']\n",
    "                            ).sort_values(by='Importance_R2_Drop', ascending=False)\n",
    "                            \n",
    "                            # Log PFI results as a W&B Table\n",
    "                            try:\n",
    "                                run_nb08.log({f\"{model_nickname}/interpretability/pfi_table\": wandb.Table(dataframe=perm_df_pfi)})\n",
    "                            except Exception as e_pfi_tbl_log: print(f\"Warning: Could not log PFI table to W&B. Error: {e_pfi_tbl_log}\")\n",
    "\n",
    "                            # Plot PFI (e.g., top 15 features)\n",
    "                            fig_pfi, ax_pfi = plt.subplots(figsize=(10, max(6, len(perm_df_pfi.head(15)) * 0.35)))\n",
    "                            sns.barplot(x='Importance_R2_Drop', y='Feature', data=perm_df_pfi.head(15), ax=ax_pfi, palette=\"viridis\")\n",
    "                            ax_pfi.set_title(f'Permutation Feature Importance (Top 15) for {model_nickname}')\n",
    "                            finalize_plot(fig_pfi, plt, run_nb08, \n",
    "                                          f\"{model_nickname}/interpretability/plot_pfi_top15\", \n",
    "                                          RUN_OUTPUT_DIR_NB08 / f\"{model_nickname}_pfi_top15_plot.png\")\n",
    "                else: \n",
    "                    print(\"    Skipping PFI calculation: Baseline R2 is NaN or no tabular features.\")\n",
    "        except Exception as e_pfi_main:\n",
    "            print(f\"    ERROR during Permutation Feature Importance for {model_nickname}: {e_pfi_main}\")\n",
    "            current_model_results[\"status_pfi\"] = f\"Error: {str(e_pfi_main)[:100]}\"\n",
    "            import traceback; traceback.print_exc()\n",
    "    else:\n",
    "        print(f\"  Skipping Permutation Feature Importance for {model_nickname} as per RUN_PERMUTATION_IMPORTANCE flag.\")\n",
    "\n",
    "\n",
    "    # --- C. CNN Interpretability (Integrated Gradients) ---\n",
    "    # RUN_SALIENCY_MAPS flag was renamed to RUN_INTEGRATED_GRADIENTS in HP_analysis\n",
    "    if is_model_hybrid and hasattr(loaded_model, 'cnn_feature_extractor') and HP_analysis.get('RUN_INTEGRATED_GRADIENTS', False):\n",
    "        print(f\"\\n  --- C. CNN Interpretability (Integrated Gradients) for {model_nickname} ---\")\n",
    "        try:\n",
    "            # Determine how many samples/batches to get IG for.\n",
    "            # For visualization, usually one batch (or first few samples from it) is enough.\n",
    "            num_ig_batches_to_process = 1 \n",
    "            ig_samples_visualized_count = 0\n",
    "            \n",
    "            for batch_idx_ig, ig_batch_data_full in enumerate(analysis_loader):\n",
    "                if batch_idx_ig >= num_ig_batches_to_process: break\n",
    "                \n",
    "                # Unpack: analysis_loader for hybrid yields (tab_seq, mri_seq, lengths, targets, masks)\n",
    "                _, s_mri_sequences_for_ig, s_lengths_for_ig, _, _ = ig_batch_data_full\n",
    "                \n",
    "                # Collect the first valid MRI scan from each sequence in the current batch\n",
    "                mris_to_explain_in_batch_list = []\n",
    "                for i_seq in range(s_mri_sequences_for_ig.size(0)): \n",
    "                    if s_lengths_for_ig[i_seq].item() > 0: # If sequence has at least one MRI\n",
    "                        mris_to_explain_in_batch_list.append(s_mri_sequences_for_ig[i_seq, 0, :, :, :, :]) # Get 0th MRI of sequence\n",
    "                \n",
    "                if not mris_to_explain_in_batch_list:\n",
    "                    print(f\"    No valid MRIs in analysis_loader batch {batch_idx_ig} for IG. Skipping this batch.\")\n",
    "                    continue\n",
    "\n",
    "                mri_batch_for_ig_utility_input = torch.stack(mris_to_explain_in_batch_list).to(device)\n",
    "                print(f\"    Explaining MRI batch (shape: {mri_batch_for_ig_utility_input.shape}) for Integrated Gradients...\")\n",
    "\n",
    "                attributions_ig_batch = generate_integrated_gradients_cnn(\n",
    "                    cnn_model_part=loaded_model.cnn_feature_extractor, \n",
    "                    mri_input_tensor_batch=mri_batch_for_ig_utility_input,\n",
    "                    n_steps=HP_analysis.get('ig_n_steps', 25), # Use a reasonable default\n",
    "                    captum_internal_batch_size=HP_analysis.get('ig_captum_internal_batch_size') # Can be None\n",
    "                )\n",
    "\n",
    "                if attributions_ig_batch is not None:\n",
    "                    current_model_results[\"IG_Attributions_Shape\"] = list(attributions_ig_batch.shape)\n",
    "                    print(f\"    Generated IG attribution maps, batch shape: {attributions_ig_batch.shape}\")\n",
    "                    \n",
    "                    # Visualize IG for a limited number of samples from this batch\n",
    "                    num_samples_to_plot_ig = min(attributions_ig_batch.shape[0], HP_analysis.get('num_detailed_samples', 1))\n",
    "                    print(f\"    Visualizing IG for first {num_samples_to_plot_ig} sample(s) in this batch...\")\n",
    "\n",
    "                    for sample_idx_plot_ig in range(num_samples_to_plot_ig):\n",
    "                        # Assuming single channel attribution, C=1\n",
    "                        if attributions_ig_batch.shape[1] == 1: \n",
    "                            attr_map_single_sample = attributions_ig_batch[sample_idx_plot_ig, 0] # Shape (D, H, W)\n",
    "                            original_mri_single_sample = mri_batch_for_ig_utility_input[sample_idx_plot_ig, 0].cpu().numpy() # (D,H,W)\n",
    "\n",
    "                            # Choose a central slice (e.g., axial) for visualization\n",
    "                            slice_idx_d = attr_map_single_sample.shape[0] // 2 \n",
    "                            attr_slice = attr_map_single_sample[slice_idx_d, :, :]\n",
    "                            mri_slice = original_mri_single_sample[slice_idx_d, :, :]\n",
    "\n",
    "                            fig_ig, axs_ig = plt.subplots(1, 2, figsize=(12, 5))\n",
    "                            fig_ig.suptitle(f\"Integrated Gradients for {model_nickname} - Explained Sample {sample_idx_plot_ig} (Batch {batch_idx_ig}), Axial Slice {slice_idx_d}\", fontsize=14)\n",
    "                            \n",
    "                            axs_ig[0].imshow(np.rot90(mri_slice), cmap='gray')\n",
    "                            axs_ig[0].set_title(\"Original MRI Slice\")\n",
    "                            axs_ig[0].axis('off')\n",
    "\n",
    "                            im_ig = axs_ig[1].imshow(np.rot90(mri_slice), cmap='gray') # Show original as background\n",
    "                            im_attr = axs_ig[1].imshow(np.rot90(attr_slice), cmap='hot', alpha=0.6, \n",
    "                                                       vmin=np.percentile(attr_slice,1), vmax=np.percentile(attr_slice,99)) # Overlay heatmap\n",
    "                            axs_ig[1].set_title(\"IG Attribution Overlay\")\n",
    "                            axs_ig[1].axis('off')\n",
    "                            fig_ig.colorbar(im_attr, ax=axs_ig[1], label=\"Attribution Strength\", fraction=0.046, pad=0.04)\n",
    "                            \n",
    "                            finalize_plot(fig_ig, plt, run_nb08, \n",
    "                                          f\"{model_nickname}/interpretability/ig_sample{ig_samples_visualized_count + sample_idx_plot_ig}_axial\", \n",
    "                                          RUN_OUTPUT_DIR_NB08 / f\"{model_nickname}_ig_sample{ig_samples_visualized_count + sample_idx_plot_ig}_axial_slice{slice_idx_d}.png\")\n",
    "                        else:\n",
    "                            print(f\"    Cannot plot IG for sample {sample_idx_plot_ig}, unexpected channel count: {attributions_ig_batch.shape[1]}\")\n",
    "                    \n",
    "                    ig_samples_visualized_count += attributions_ig_batch.shape[0]\n",
    "                    if run_nb08: run_nb08.log({f\"{model_nickname}/interpretability/ig_batch_processed_successfully\": True})\n",
    "                else:\n",
    "                    print(f\"    Integrated Gradients generation returned None for {model_nickname}.\")\n",
    "                    if run_nb08: run_nb08.log({f\"{model_nickname}/interpretability/ig_generation_failed\": True})\n",
    "                # break # Usually explain one batch for IG visualization is enough for a demo\n",
    "        except Exception as e_ig_main:\n",
    "            print(f\"    ERROR during Integrated Gradients for {model_nickname}: {e_ig_main}\")\n",
    "            current_model_results[\"status_ig\"] = f\"Error: {str(e_ig_main)[:100]}\"\n",
    "            import traceback; traceback.print_exc()\n",
    "    elif is_model_hybrid and not HP_analysis.get('RUN_INTEGRATED_GRADIENTS', False):\n",
    "        print(f\"  Skipping CNN Interpretability (Integrated Gradients) for {model_nickname} as per RUN_INTEGRATED_GRADIENTS flag.\")\n",
    "    # No IG for baseline models\n",
    "\n",
    "\n",
    "    # --- D. SHAP Analysis ---\n",
    "    # SHAP for Baseline Model (Tabular LSTM)\n",
    "    if not is_model_hybrid and HP_analysis.get('RUN_SHAP_BASELINE', False):\n",
    "        print(f\"\\n  --- D. SHAP Analysis for {model_nickname} (BaselineLSTM using DeepExplainer) ---\")\n",
    "        try:\n",
    "            # Create DataLoaders for SHAP: background from training data, instances from test data (analysis_dataset)\n",
    "            # For baseline, OASISDataset needs config_for_this_model_dataset (for this baseline model) and include_mri=False\n",
    "            # Preprocessor paths EXPECTED_SCALER_PATH_NB08 and EXPECTED_IMPUTER_PATH_NB08 are used.\n",
    "            print(\"    SHAP (Baseline): Instantiating training dataset for background...\")\n",
    "            train_dataset_for_shap_bg_baseline = OASISDataset(\n",
    "                TRAIN_DATA_PATH_FOR_SHAP_BG_NB08, \n",
    "                EXPECTED_SCALER_PATH_NB08, # Assuming these are the correct preprocessors\n",
    "                EXPECTED_IMPUTER_PATH_NB08,    \n",
    "                config=config_for_this_model_dataset, # Config for the current baseline model\n",
    "                include_mri=False # Explicitly False for baseline\n",
    "            )\n",
    "            shap_background_loader_baseline = DataLoader( \n",
    "                torch.utils.data.Subset(train_dataset_for_shap_bg_baseline, list(range(min(HP_analysis.get('shap_num_background_samples', 20), len(train_dataset_for_shap_bg_baseline))))),\n",
    "                batch_size=HP_analysis.get('shap_background_batch_size', 10), \n",
    "                shuffle=True, collate_fn=pad_collate_fn, num_workers=0\n",
    "            )\n",
    "            # analysis_dataset is already instantiated for the current model (baseline)\n",
    "            shap_instances_loader_baseline = DataLoader( \n",
    "                 torch.utils.data.Subset(analysis_dataset, list(range(min(HP_analysis.get('num_detailed_samples', 5), len(analysis_dataset))))),\n",
    "                 batch_size=HP_analysis.get('shap_explain_batch_size', HP_analysis.get('num_detailed_samples', 5)),\n",
    "                 shuffle=False, collate_fn=pad_collate_fn, num_workers=0\n",
    "            )\n",
    "            current_feature_names_for_shap = analysis_dataset.model_input_features\n",
    "\n",
    "            shap_values_lstm, explained_instances_np_lstm = explain_lstm_with_shap(\n",
    "                loaded_model, shap_background_loader_baseline, shap_instances_loader_baseline, \n",
    "                device, feature_names=current_feature_names_for_shap\n",
    "            )\n",
    "            if shap_values_lstm is not None and explained_instances_np_lstm is not None:\n",
    "                current_model_results[\"SHAP_Baseline_Values_Shape\"] = list(shap_values_lstm.shape)\n",
    "                print(\"    SHAP values for BaselineLSTM obtained.\")\n",
    "                # Plotting (Bar plot of mean absolute SHAP values)\n",
    "                if shap_values_lstm.ndim == 3 and len(current_feature_names_for_shap) == shap_values_lstm.shape[2]:\n",
    "                    mean_abs_shap_baseline = np.mean(np.abs(shap_values_lstm), axis=(0,1))\n",
    "                    shap_summary_df_baseline = pd.DataFrame({\n",
    "                        'feature': current_feature_names_for_shap, \n",
    "                        'mean_abs_shap': mean_abs_shap_baseline\n",
    "                    }).sort_values(by='mean_abs_shap', ascending=False)\n",
    "                    \n",
    "                    fig_shap_bl, ax_shap_bl = plt.subplots(figsize=(10, max(6, len(shap_summary_df_baseline) * 0.35)))\n",
    "                    sns.barplot(x='mean_abs_shap', y='feature', data=shap_summary_df_baseline.head(15), ax=ax_shap_bl, palette=\"mako\")\n",
    "                    ax_shap_bl.set_title(f'SHAP Mean Abs. Feature Importance for {model_nickname}')\n",
    "                    finalize_plot(fig_shap_bl, plt, run_nb08, \n",
    "                                  f\"{model_nickname}/interpretability/shap_baseline_mean_abs_bar\", \n",
    "                                  RUN_OUTPUT_DIR_NB08 / f\"{model_nickname}_shap_baseline_barplot.png\")\n",
    "                    \n",
    "                    # Beeswarm if seq_len was 1\n",
    "                    if shap_values_lstm.shape[1] == 1 and shap is not None : # Check if shap imported in NB08\n",
    "                        shap_values_2d = shap_values_lstm.squeeze(axis=1)\n",
    "                        explained_instances_2d = explained_instances_np_lstm.squeeze(axis=1)\n",
    "                        plt.figure() # New figure for beeswarm\n",
    "                        shap.summary_plot(shap_values_2d, features=explained_instances_2d, feature_names=current_feature_names_for_shap, show=False)\n",
    "                        plt.title(f\"SHAP Summary (Beeswarm) for {model_nickname}\")\n",
    "                        finalize_plot(plt.gcf(), plt, run_nb08, # Get current figure\n",
    "                                      f\"{model_nickname}/interpretability/shap_baseline_beeswarm\",\n",
    "                                      RUN_OUTPUT_DIR_NB08 / f\"{model_nickname}_shap_baseline_beeswarm.png\")\n",
    "                else: print(\"    SHAP values for baseline have unexpected shape or feature name mismatch for plotting.\")\n",
    "                if run_nb08: run_nb08.log({f\"{model_nickname}/interpretability/shap_baseline_generated\": True})\n",
    "            else: \n",
    "                print(f\"    SHAP (Baseline) analysis did not produce results for {model_nickname}.\")\n",
    "                current_model_results[\"status_shap_baseline\"] = \"NoResults\"\n",
    "        except Exception as e_shap_baseline_main:\n",
    "            print(f\"    ERROR during SHAP (Baseline) for {model_nickname}: {e_shap_baseline_main}\")\n",
    "            current_model_results[\"status_shap_baseline\"] = f\"Error: {str(e_shap_baseline_main)[:100]}\"\n",
    "            import traceback; traceback.print_exc()\n",
    "\n",
    "    # SHAP for Hybrid Model (Fusion Stage)\n",
    "    if is_model_hybrid and HP_analysis.get('RUN_SHAP_HYBRID_FUSION', False):\n",
    "        print(f\"\\n  --- D. SHAP Analysis for {model_nickname} (Hybrid Model - Fusion Stage using KernelExplainer) ---\")\n",
    "        try:\n",
    "            # Create DataLoaders for SHAP background and instances (these need full hybrid inputs)\n",
    "            train_dataset_for_hybrid_shap_bg = OASISDataset(\n",
    "                TRAIN_DATA_PATH_FOR_SHAP_BG_NB08, EXPECTED_SCALER_PATH_NB08, EXPECTED_IMPUTER_PATH_NB08,\n",
    "                config=config_for_this_model_dataset, # Config for the current hybrid model\n",
    "                mri_data_dir=MRI_DATA_DIR_NB08, include_mri=True\n",
    "            )\n",
    "            hybrid_shap_background_loader = DataLoader(\n",
    "                 torch.utils.data.Subset(train_dataset_for_hybrid_shap_bg, \n",
    "                                         list(range(min(HP_analysis.get('shap_num_background_samples', 20), len(train_dataset_for_hybrid_shap_bg))))),\n",
    "                 batch_size=HP_analysis.get('shap_background_batch_size', 4), \n",
    "                 shuffle=False, collate_fn=pad_collate_fn, num_workers=0\n",
    "            )\n",
    "            hybrid_shap_instances_loader = DataLoader(\n",
    "                 torch.utils.data.Subset(analysis_dataset, list(range(min(HP_analysis.get('num_detailed_samples', 5), len(analysis_dataset))))),\n",
    "                 batch_size=HP_analysis.get('shap_explain_batch_size', HP_analysis.get('num_detailed_samples', 5)),\n",
    "                 shuffle=False, collate_fn=pad_collate_fn, num_workers=0\n",
    "            )\n",
    "            \n",
    "            # Add comment on ConvergenceWarnings for KernelExplainer\n",
    "            print(\"    Note: shap.KernelExplainer might produce ConvergenceWarnings from scikit-learn if features are highly collinear.\")\n",
    "\n",
    "            shap_values_f, explained_df_f, mri_f_count, tab_f_count = explain_hybrid_fusion_with_shap(\n",
    "                loaded_model, hybrid_shap_background_loader, hybrid_shap_instances_loader, device,\n",
    "                num_background_samples_for_kmeans=HP_analysis.get('shap_kmeans_k', 5),\n",
    "                num_shap_samples=HP_analysis.get('shap_kernel_nsamples', 50)\n",
    "            )\n",
    "            if shap_values_f is not None and explained_df_f is not None:\n",
    "                current_model_results[\"SHAP_HybridFusion_Values_Shape\"] = list(shap_values_f.shape)\n",
    "                current_model_results[\"SHAP_HybridFusion_MRI_Feat_Count\"] = mri_f_count\n",
    "                current_model_results[\"SHAP_HybridFusion_Tab_Feat_Count\"] = tab_f_count\n",
    "                print(\"    SHAP values for hybrid fusion stage obtained.\")\n",
    "\n",
    "                # Plotting for SHAP hybrid fusion (bar plot of all fused features)\n",
    "                if shap is not None: # Ensure shap module is imported in NB08\n",
    "                    fig_shap_hf, ax_shap_hf = plt.subplots(figsize=(12, max(8, explained_df_f.shape[1] * 0.25) )) # Dynamic height\n",
    "                    shap.summary_plot(shap_values_f, features=explained_df_f, plot_type=\"bar\", show=False, axis_color=\"black\", color_bar=False, fig=fig_shap_hf) # Pass fig\n",
    "                    # Manually adjust title for the figure, not the axes returned by shap.summary_plot\n",
    "                    fig_shap_hf.suptitle(f\"SHAP: Feature Importance at Fusion Stage - {model_nickname}\", fontsize=14) # Use suptitle for figure\n",
    "                    # plt.xlabel(\"Mean |SHAP value| (Average impact on model output magnitude at fusion)\") # shap.summary_plot adds this\n",
    "                    # fig_shap_hf.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust for suptitle\n",
    "                    finalize_plot(fig_shap_hf, plt, run_nb08,\n",
    "                                  f\"{model_nickname}/interpretability/shap_hybrid_fusion_bar\",\n",
    "                                  RUN_OUTPUT_DIR_NB08 / f\"{model_nickname}_SHAP_HybridFusion_BarPlot.png\")\n",
    "                \n",
    "                if mri_f_count > 0 and tab_f_count > 0:\n",
    "                    abs_shap_mri_stream_avg = np.mean(np.sum(np.abs(shap_values_f[:, :mri_f_count]), axis=1))\n",
    "                    abs_shap_tab_stream_avg = np.mean(np.sum(np.abs(shap_values_f[:, mri_f_count:(mri_f_count+tab_f_count)]), axis=1)) # Correct slicing\n",
    "                    current_model_results[\"SHAP_Hybrid_MRI_Stream_AvgTotalAbs\"] = abs_shap_mri_stream_avg\n",
    "                    current_model_results[\"SHAP_Hybrid_Tab_Stream_AvgTotalAbs\"] = abs_shap_tab_stream_avg\n",
    "                    print(f\"    Avg total |SHAP| for MRI stream features at fusion: {abs_shap_mri_stream_avg:.4f}\")\n",
    "                    print(f\"    Avg total |SHAP| for Tabular stream features at fusion: {abs_shap_tab_stream_avg:.4f}\")\n",
    "                    if run_nb08:\n",
    "                        run_nb08.log({\n",
    "                            f\"{model_nickname}/interpretability/shap_hybrid_fusion_mri_stream_avg_total_abs\": abs_shap_mri_stream_avg,\n",
    "                            f\"{model_nickname}/interpretability/shap_hybrid_fusion_tab_stream_avg_total_abs\": abs_shap_tab_stream_avg\n",
    "                        })\n",
    "                if run_nb08: run_nb08.log({f\"{model_nickname}/interpretability/shap_hybrid_fusion_generated\": True})\n",
    "            else: \n",
    "                print(f\"    SHAP (Hybrid Fusion) analysis did not produce results for {model_nickname}.\")\n",
    "                current_model_results[\"status_shap_hybrid\"] = \"NoResults\"\n",
    "        except Exception as e_shap_hybrid_main:\n",
    "            print(f\"    ERROR during SHAP (Hybrid Fusion) for {model_nickname}: {e_shap_hybrid_main}\")\n",
    "            current_model_results[\"status_shap_hybrid\"] = f\"Error: {str(e_shap_hybrid_main)[:100]}\"\n",
    "            import traceback; traceback.print_exc()\n",
    "    \n",
    "    # General skip message if RUN_SHAP_ANALYSIS was false for the model type\n",
    "    elif not HP_analysis.get('RUN_SHAP_BASELINE', False) and not HP_analysis.get('RUN_SHAP_HYBRID_FUSION', False):\n",
    "        print(f\"\\n  --- D. SHAP Analysis: Skipped for {model_nickname} as per relevant RUN_SHAP flags ---\")\n",
    "\n",
    "\n",
    "    all_models_analysis_results.append(current_model_results)\n",
    "    print(f\"\\n--- Finished All Analyses for: {model_nickname} ---\")\n",
    "# --- End of Main Analysis Loop ---\n",
    "\n",
    "# Convert overall results to DataFrame for final summary display and logging\n",
    "if all_models_analysis_results:\n",
    "    nb08_summary_df = pd.DataFrame(all_models_analysis_results)\n",
    "    # Ensure critical columns are present, fill with NaN if a model failed an analysis\n",
    "    expected_cols = [\"model_nickname\", \"source_wandb_run_path\", \"model_type_is_hybrid\", \n",
    "                     \"Avg_MC_Variance\", \"Avg_MC_Std_Dev\", \n",
    "                     \"PFI_Baseline_R2\", \"PFI_Importances (R2_Drop)\",\n",
    "                     \"SHAP_Baseline_Values_Shape\", \n",
    "                     \"SHAP_HybridFusion_Values_Shape\", \"SHAP_Hybrid_MRI_Stream_AvgAbs\", \"SHAP_Hybrid_Tab_Stream_AvgAbs\",\n",
    "                     \"status_mc_dropout\", \"status_pfi\", \"status_ig\", \"status_shap_baseline\", \"status_shap_hybrid\"]\n",
    "    for col in expected_cols:\n",
    "        if col not in nb08_summary_df.columns:\n",
    "            nb08_summary_df[col] = np.nan # Add missing columns with NaNs\n",
    "            \n",
    "    print(\"\\n\\n--- Notebook 08: Overall Analysis Summary ---\")\n",
    "    # display(nb08_summary_df) # Use display for richer output in Jupyter\n",
    "    print(nb08_summary_df.to_string()) # Print full DataFrame to console\n",
    "\n",
    "    if run_nb08: \n",
    "        try:\n",
    "            summary_table_wandb = wandb.Table(dataframe=nb08_summary_df.fillna(\"N/A\")) # Replace NaNs for W&B Table\n",
    "            run_nb08.log({\"analysis_run_summary/all_models_table\": summary_table_wandb})\n",
    "            print(\"Overall analysis summary table logged to W&B.\")\n",
    "        except Exception as e_log_summary_tbl:\n",
    "            print(f\"Error logging summary table to W&B: {e_log_summary_tbl}\")\n",
    "else:\n",
    "    print(\"\\nNo models were analyzed, or no results collected. Summary DataFrame not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5571a1",
   "metadata": {},
   "source": [
    "## 6. Finalize W&B Run for Notebook 08 Analysis\n",
    "\n",
    "Complete the execution of this analysis notebook and finish its associated Weights & Biases run. This ensures all queued logs, metrics, configurations, plots, and summary tables are fully uploaded and synchronized with the W&B platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Finish W&B Run for this Analysis Notebook (NB08) ---\n",
    "# run_nb08 is the W&B run object for this notebook.\n",
    "\n",
    "if run_nb08:\n",
    "    print(f\"\\n--- Finishing W&B run '{run_nb08.name}' for Notebook 08 ---\")\n",
    "    try:\n",
    "        # Add a final status to the summary if needed\n",
    "        if not all_models_analysis_results: # Check if the main list is empty\n",
    "             run_nb08.summary[\"overall_analysis_status\"] = \"NoModelsSuccessfullyAnalyzed\"\n",
    "        elif nb08_summary_df.empty: # Check if the summary DataFrame is empty\n",
    "             run_nb08.summary[\"overall_analysis_status\"] = \"AnalysisRanButSummaryDFEmpty\"\n",
    "        else:\n",
    "             run_nb08.summary[\"overall_analysis_status\"] = \"Completed\"\n",
    "        \n",
    "        run_nb08.finish()\n",
    "        # Use a more robust way to get the final run name for printing\n",
    "        final_run_name_nb08 = run_nb08.name_synced if hasattr(run_nb08, 'name_synced') and run_nb08.name_synced else \\\n",
    "                              run_nb08.name if hasattr(run_nb08, 'name') and run_nb08.name else \\\n",
    "                              run_nb08.id if hasattr(run_nb08, 'id') else \"current NB08 run\"\n",
    "        print(f\"W&B run '{final_run_name_nb08}' finished successfully.\")\n",
    "    except Exception as e_finish_nb08:\n",
    "        print(f\"Error during wandb.finish() for Notebook 08: {e_finish_nb08}\")\n",
    "        print(\"The W&B run may not have finalized correctly on the server.\")\n",
    "else:\n",
    "    print(\"\\nNo active W&B run for Notebook 08 to finish (likely initialization failed or was skipped).\")\n",
    "\n",
    "print(f\"\\n--- Notebook {NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER} execution finished. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro_predcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
