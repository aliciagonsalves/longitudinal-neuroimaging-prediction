{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f98e754",
   "metadata": {},
   "source": [
    "# Notebook 06: Train Baseline LSTM Model (OASIS-2)\n",
    "\n",
    "**Project Phase:** 1 (Model Training - Baseline)\n",
    "**Dataset:** OASIS-2 Longitudinal MRI & Clinical Data\n",
    "\n",
    "**Purpose:**\n",
    "This notebook trains the `BaselineLSTMRegressor` (defined in `src/models.py`) to predict the next visit's Clinical Dementia Rating (CDR) score. It utilizes only longitudinal tabular clinical and demographic features derived from the OASIS-2 dataset. This model serves as a crucial baseline for evaluating the performance of more complex, multimodal architectures.\n",
    "\n",
    "**Workflow:**\n",
    "1.  **Setup:** Import necessary libraries, configure `sys.path` for `src/` utilities, and load the main project configuration (`config.json`). Define training hyperparameters (`HP`).\n",
    "2.  **Path Resolution:** Use the `get_dataset_paths` utility to resolve paths for input data splits (`train`, `validation`, `test` Parquet files from Notebook 03).\n",
    "3.  **W&B Initialization & Artifact/Config Ingestion:**\n",
    "    * Initialize a new Weights & Biases (W&B) run for this training experiment using the `initialize_wandb_run` utility.\n",
    "    * Consume the versioned **fitted preprocessor W&B Artifacts** (e.g., `scaler_standard_oasis2:latest`, `imputer_median_oasis2:latest`) that were produced and logged by Notebook 04. This step downloads the `.joblib` preprocessor files.\n",
    "    * From one of these consumed preprocessor artifacts, identify the W&B Run from Notebook 04 that **produced** them using `artifact.logged_by()`.\n",
    "    * Fetch the **authoritative `features` (time-varying, static) and `preprocess` (imputation/scaling columns, strategies) configurations** directly from this producer Notebook 04 run's W&B config. This becomes the `config_for_dataset` passed to `OASISDataset`.\n",
    "    * Update the current training run's (`HP['input_size']`) based on the fetched feature lists.\n",
    "4.  **Setup Device:** Detect and set the appropriate PyTorch device (CPU, CUDA, MPS).\n",
    "5.  **Load Data & Create DataLoaders:** Instantiate `OASISDataset` for training and validation splits, passing the local paths to the downloaded preprocessor artifacts and the definitive `config_for_dataset`. MRI data is explicitly excluded (`include_mri=False`). Wrap datasets in PyTorch `DataLoader`s using `pad_collate_fn`.\n",
    "6.  **Define Model, Loss, Optimizer:** Instantiate `BaselineLSTMRegressor`, `MSELoss` criterion, and `Adam` optimizer using parameters from `HP`.\n",
    "7.  **Train Model:** Execute the main training loop by calling the `train_model` utility function from `src/training_utils.py`. This utility handles epoch iteration, training/validation phases (using `evaluate_model`), W&B metric logging, best model checkpointing (local save + W&B artifact), periodic checkpointing, and early stopping.\n",
    "8.  **Evaluate Best Model on Test Set:**\n",
    "    * Load the best model checkpoint saved during training (identified by `train_model`).\n",
    "    * Instantiate `OASISDataset` and `DataLoader` for the test set.\n",
    "    * Use the `evaluate_model` utility to calculate performance metrics (Loss, MAE, R2, MSE) on the test set.\n",
    "    * Log these test metrics to the W&B run's summary.\n",
    "9.  **Finalize W&B Run.**\n",
    "\n",
    "**Input:**\n",
    "* `config.json`: Main project configuration file.\n",
    "* W&B Artifact Names for training, validation, and test data splits (e.g., `cohort-split-train_oasis2:latest` - from Notebook 03).\n",
    "* W&B Artifact Names for fitted preprocessors (Scaler & Imputer - from Notebook 04).\n",
    "* `src/` modules: `datasets.py`, `models.py`, `training_utils.py`, `evaluation_utils.py`, `wandb_utils.py`, `paths_utils.py`.\n",
    "\n",
    "**Output:**\n",
    "* **Local Files (in run-specific output directory for this NB06 run, e.g., `notebooks/outputs/06_Train_Baseline_Model_OASIS2/<run_name>/`):**\n",
    "    * Trained `BaselineLSTMRegressor` model checkpoints (`best_model_epoch_X.pth`, `model_epoch_X_periodic.pth`).\n",
    "* **W&B Run:**\n",
    "    * Logged hyperparameters (`HP`), including source NB04 run ID and resolved feature/preprocessing configurations.\n",
    "    * Per-epoch training and validation metrics.\n",
    "    * Best model checkpoint logged as a W&B Artifact (e.g., `Training-NB06-BaselineLSTM-OASIS2-checkpoint:best`).\n",
    "    * Periodic model checkpoints logged as W&B Artifacts.\n",
    "    * Final test set performance metrics in the run's summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff92904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: notebooks/06_Train_Baseline_Model.ipynb\n",
    "# Purpose: Train the baseline LSTM regression model to predict next CDR score\n",
    "#          using pre-computed features and the prepared data splits.\n",
    "#          Loads configuration (feature lists, etc.) from the relevant NB04 W&B run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc231bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Libraries & Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09839f81",
   "metadata": {},
   "source": [
    "## 1. Setup: Project Configuration, Paths, Utilities, and Hyperparameters\n",
    "\n",
    "This section initializes the notebook environment:\n",
    "* Determines the project's root directory (`PROJECT_ROOT`) and adds the `src` directory to `sys.path` for custom module imports.\n",
    "* Imports all necessary custom utility functions from the `src/` modules (`wandb_utils`, `paths_utils`, `datasets`, `models`, `training_utils`, `evaluation_utils`).\n",
    "* Loads the main project configuration (`base_config`) from `config.json`.\n",
    "* Defines dataset and notebook-specific identifiers (e.g., `DATASET_IDENTIFIER`, `NOTEBOOK_MODULE_NAME`).\n",
    "* **Uses the `get_dataset_paths` utility to resolve paths to input data splits** (training, validation, and test `.parquet` files produced by Notebook 03). The paths to preprocessor `.joblib` files will be obtained later by downloading W&B artifacts from a Notebook 04 run.\n",
    "* Defines the initial set of training hyperparameters (`HP`) for the `BaselineLSTMRegressor` model. The `input_size` hyperparameter will be dynamically updated later based on feature configurations fetched from Notebook 04.\n",
    "* Defines the locator key from `config.json` that specifies the base output directory for this notebook's locally saved artifacts (e.g., model checkpoints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0832f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Project Setup, Configuration Loading, Utility Imports & Hyperparameters ---\n",
    "print(\"--- Initializing Project Setup, Configuration, and HPs for NB06 ---\")\n",
    "\n",
    "# Initialize\n",
    "PROJECT_ROOT = None\n",
    "base_config = {} \n",
    "\n",
    "try:\n",
    "    current_notebook_path = Path.cwd() \n",
    "    potential_project_root = current_notebook_path.parent \n",
    "    if (potential_project_root / \"src\").is_dir() and (potential_project_root / \"config.json\").is_file():\n",
    "        PROJECT_ROOT = potential_project_root\n",
    "    else: \n",
    "        PROJECT_ROOT = current_notebook_path\n",
    "    if not (PROJECT_ROOT / \"src\").is_dir() or not (PROJECT_ROOT / \"config.json\").is_file():\n",
    "        raise FileNotFoundError(f\"Could not find 'src' or 'config.json'. PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "    if str(PROJECT_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    print(f\"PROJECT_ROOT: {PROJECT_ROOT}, added to sys.path.\")\n",
    "\n",
    "    # Import all necessary custom utilities\n",
    "    from src.wandb_utils import initialize_wandb_run, load_model_from_wandb_artifact # load_model for test phase\n",
    "    from src.paths_utils import get_dataset_paths, get_notebook_run_output_dir\n",
    "    from src.datasets import OASISDataset, pad_collate_fn\n",
    "    from src.models import BaselineLSTMRegressor\n",
    "    from src.training_utils import train_model\n",
    "    from src.evaluation_utils import evaluate_model\n",
    "    print(\"Successfully imported all required custom utilities and classes.\")\n",
    "\n",
    "except Exception as e_setup:\n",
    "    print(f\"CRITICAL ERROR during initial setup or imports: {e_setup}\")\n",
    "    # exit()\n",
    "\n",
    "# --- Load Main Project Configuration ---\n",
    "print(\"\\n--- Loading Main Project Configuration ---\")\n",
    "try:\n",
    "    if PROJECT_ROOT is None: raise ValueError(\"PROJECT_ROOT not set.\")\n",
    "    CONFIG_PATH_MAIN = PROJECT_ROOT / 'config.json'\n",
    "    with open(CONFIG_PATH_MAIN, 'r', encoding='utf-8') as f:\n",
    "        base_config = json.load(f)\n",
    "    print(f\"Main project config loaded from: {CONFIG_PATH_MAIN}\")\n",
    "except Exception as e_cfg:\n",
    "    print(f\"CRITICAL ERROR loading main config.json: {e_cfg}\")\n",
    "    # exit() \n",
    "\n",
    "# --- Define Dataset, Notebook Specifics, and Resolve Data Split Paths ---\n",
    "DATASET_IDENTIFIER = \"oasis2\" \n",
    "NOTEBOOK_MODULE_NAME = \"06_Train_Baseline_Model\"\n",
    "# Key from config.json locators for this notebook's *output directory* (for model checkpoints)\n",
    "NB06_OUTPUT_LOCATOR_KEY = base_config.get(f\"pipeline_artefact_locators_{DATASET_IDENTIFIER}\", {})\\\n",
    "                                     .get(\"train_baseline_subdir_nb06_key\", \"train_baseline_subdir\")\n",
    "                                    \n",
    "\n",
    "TRAIN_DATA_PATH = None\n",
    "VAL_DATA_PATH = None\n",
    "TEST_DATA_PATH = None\n",
    "# Scaler and Imputer paths will be determined by downloading artifacts in the next cell\n",
    "\n",
    "try:\n",
    "    if not base_config: raise ValueError(\"base_config is empty.\")\n",
    "    \n",
    "    # Get paths for data splits using the utility\n",
    "    # Training data paths\n",
    "    train_stage_paths = get_dataset_paths(\n",
    "        PROJECT_ROOT, base_config, DATASET_IDENTIFIER, stage=\"training\"\n",
    "    )\n",
    "    TRAIN_DATA_PATH = train_stage_paths.get('train_data_parquet')\n",
    "    VAL_DATA_PATH = train_stage_paths.get('val_data_parquet')\n",
    "    \n",
    "    # Test data path\n",
    "    test_stage_paths = get_dataset_paths(\n",
    "        PROJECT_ROOT, base_config, DATASET_IDENTIFIER, stage=\"testing\" # Or \"analysis\"\n",
    "    )\n",
    "    TEST_DATA_PATH = test_stage_paths.get('test_data_parquet')\n",
    "    \n",
    "    if not all([TRAIN_DATA_PATH, VAL_DATA_PATH, TEST_DATA_PATH]):\n",
    "        raise ValueError(\"Failed to resolve one or more critical data split paths via get_dataset_stage_paths.\")\n",
    "\n",
    "    print(f\"\\nKey data input paths for Notebook 06 ({DATASET_IDENTIFIER}):\")\n",
    "    print(f\"  Training Data Parquet (from NB03): {TRAIN_DATA_PATH}\")\n",
    "    print(f\"  Validation Data Parquet (from NB03): {VAL_DATA_PATH}\")\n",
    "    print(f\"  Test Data Parquet (from NB03): {TEST_DATA_PATH}\")\n",
    "    \n",
    "    for p_name, p_obj in [(\"Training Data\", TRAIN_DATA_PATH), (\"Validation Data\", VAL_DATA_PATH), (\"Test Data\", TEST_DATA_PATH)]:\n",
    "        if not p_obj.is_file(): raise FileNotFoundError(f\"CRITICAL: {p_name} parquet file not found at: {p_obj}\")\n",
    "    print(\"All critical input data split paths for NB06 verified.\")\n",
    "\n",
    "except (KeyError, ValueError, FileNotFoundError) as e_paths_nb06:\n",
    "    print(f\"CRITICAL ERROR during path setup for NB06: {e_paths_nb06}\")\n",
    "    # exit()\n",
    "except Exception as e_general_nb06_setup:\n",
    "    print(f\"CRITICAL ERROR during setup for NB06: {e_general_nb06_setup}\")\n",
    "    # exit()\n",
    "\n",
    "# --- Define Training Hyperparameters (HP) ---\n",
    "print(\"\\n--- Defining Training Hyperparameters (HP) for BaselineLSTM ---\")\n",
    "HP = {\n",
    "    'model_type': 'BaselineLSTM', \n",
    "    'dataset_identifier': DATASET_IDENTIFIER,\n",
    "    'batch_size': 32,          \n",
    "    'learning_rate': 1e-4,     \n",
    "    'epochs': 50, # Max epochs, early stopping might finish sooner\n",
    "    'lstm_hidden_size': 128,   \n",
    "    'num_lstm_layers': 2,      \n",
    "    'lstm_dropout_prob': 0.3,  \n",
    "    'dataloader_num_workers': 0, \n",
    "    'patience': 10,            \n",
    "    'save_checkpoint_every_n_epochs': 5, \n",
    "    'random_seed': 42,\n",
    "    'input_size': None # CRITICAL: To be updated after fetching config from NB04\n",
    "}\n",
    "# Set seed for reproducibility early\n",
    "np.random.seed(HP['random_seed'])\n",
    "torch.manual_seed(HP['random_seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(HP['random_seed'])\n",
    "print(\"Training hyperparameters (HP) defined. 'input_size' will be set after fetching NB04 config.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81932619",
   "metadata": {},
   "source": [
    "## 2. Initialize W&B Run, Fetch Preprocessor Artifacts & Definitive Configuration from Notebook 04\n",
    "\n",
    "This crucial step prepares the ground for consistent data handling and model training:\n",
    "1.  **Initialize W&B Run for Notebook 06:** A new W&B run is started for this specific `BaselineLSTMRegressor` training experiment using the `initialize_wandb_run` utility. This run will log all hyperparameters, configurations, metrics, and model artifacts.\n",
    "2.  **Define Output Directory:** A unique local directory is created for this W&B run's outputs (e.g., model checkpoints) using `get_notebook_run_output_dir`.\n",
    "3.  **Consume Preprocessor Artifacts:** The W&B Artifacts for the *fitted* `StandardScaler` and `SimpleImputer` (produced by a specific Notebook 04 execution) are identified by name and version (e.g., `:latest`) and downloaded using `run.use_artifact()`. This provides the local paths to the `.joblib` files.\n",
    "4.  **Fetch Authoritative NB04 Configuration:** From one of the consumed preprocessor artifacts (e.g., the scaler artifact), the W&B Run that *produced it* (the relevant Notebook 04 run) is identified via `artifact.logged_by()`.\n",
    "5.  **Set Up `config_for_dataset`:** The W&B configuration of this producer Notebook 04 run is fetched. This configuration contains the **authoritative `features` (time-varying, static) and `preprocess` (imputation/scaling columns, strategies) dictionaries**. This fetched dictionary becomes `config_for_dataset`, which will be passed to `OASISDataset`.\n",
    "6.  **Update `HP['input_size']`:** The `input_size` for the `BaselineLSTMRegressor` is dynamically determined from the length of the feature lists in the fetched `config_for_dataset`.\n",
    "7.  All source information (NB04 run ID, consumed artifact versions, final feature/preprocess configs) is logged to the current Notebook 06 W&B run's configuration for complete traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a1500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize W&B Run, Fetch Preprocessor Artifacts & NB04 Config ---\n",
    "print(\"\\n--- Initializing W&B Run for NB06 & Fetching NB04 Preprocessing Setup ---\")\n",
    "\n",
    "# Variables to be populated by this cell\n",
    "run = None\n",
    "run_output_dir_for_checkpoints = None\n",
    "SCALER_PATH_FROM_ARTIFACT = None\n",
    "IMPUTER_PATH_FROM_ARTIFACT = None\n",
    "config_for_dataset = {} # This will be the authoritative config for OASISDataset\n",
    "source_nb04_run_id_logged = \"N/A\"\n",
    "\n",
    "# --- 1. Initialize W&B Run for THIS Notebook 06 execution ---\n",
    "# We need the 'run' object to use artifacts, so initialize W&B first.\n",
    "# The HP dictionary will be updated with input_size later and then logged more fully.\n",
    "temp_hp_for_init = HP.copy() # Use a copy for initial logging\n",
    "\n",
    "nb_number_prefix_nb06 = NOTEBOOK_MODULE_NAME.split('_')[0] if '_' in NOTEBOOK_MODULE_NAME else \"NB\"\n",
    "job_specific_type_nb06 = f\"{nb_number_prefix_nb06}-BaselineLSTM-{DATASET_IDENTIFIER}\"\n",
    "custom_elements_for_name_nb06 = [\n",
    "    nb_number_prefix_nb06, DATASET_IDENTIFIER.upper(), \"Baseline\",\n",
    "    f\"h{HP['lstm_hidden_size']}\", f\"nl{HP['num_lstm_layers']}\", f\"dp{HP['lstm_dropout_prob']:.1f}\", # Format dropout\n",
    "    f\"lr{HP['learning_rate']:.0e}\", f\"bs{HP['batch_size']}\"\n",
    "]\n",
    "run = initialize_wandb_run(\n",
    "    base_project_config=base_config,\n",
    "    job_group=\"Training\",\n",
    "    job_specific_type=job_specific_type_nb06,\n",
    "    run_specific_config=temp_hp_for_init, # Log initial HP\n",
    "    custom_run_name_elements=custom_elements_for_name_nb06,\n",
    "    notes=f\"Training BaselineLSTMRegressor on {DATASET_IDENTIFIER.upper()}.\"\n",
    ")\n",
    "\n",
    "# --- 2. Define Run-Specific Output Directory for Local Checkpoints ---\n",
    "if run:\n",
    "    print(f\"W&B run '{run.name}' (Job Type: '{run.job_type}') initialized. View at: {run.url}\")\n",
    "    run_output_dir_for_checkpoints = get_notebook_run_output_dir(\n",
    "        PROJECT_ROOT, base_config, NB06_OUTPUT_LOCATOR_KEY, run, DATASET_IDENTIFIER\n",
    "    )\n",
    "    print(f\"Model checkpoints for this run will be saved locally to: {run_output_dir_for_checkpoints}\")\n",
    "    run.config.update({\"run_outputs/local_checkpoint_dir\": str(run_output_dir_for_checkpoints)}, allow_val_change=True)\n",
    "else:\n",
    "    print(\"W&B run initialization failed. Proceeding with local fallback for outputs.\")\n",
    "    # Fallback local output directory if W&B init failed\n",
    "    default_nb_locator = f\"{NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER}_local_outputs\"\n",
    "    run_output_dir_for_checkpoints = get_notebook_run_output_dir(\n",
    "        PROJECT_ROOT, base_config, NB06_OUTPUT_LOCATOR_KEY if base_config else default_nb_locator, \n",
    "        None, DATASET_IDENTIFIER\n",
    "    )\n",
    "    print(f\"Model checkpoints (local fallback) will be saved to: {run_output_dir_for_checkpoints}\")\n",
    "\n",
    "if not isinstance(run_output_dir_for_checkpoints, Path): # Should be Path from utility\n",
    "    run_output_dir_for_checkpoints = Path(run_output_dir_for_checkpoints)\n",
    "\n",
    "\n",
    "# --- 3. Fetch Preprocessor Artifacts & NB04 Config (requires active NB06 W&B run) ---\n",
    "if run: # Only proceed if W&B run for NB06 is active\n",
    "    try:\n",
    "        # Define expected preprocessor artifact names (must match what NB04 logged)\n",
    "        preprocessing_cfg = base_config.get('preprocessing_config', {})\n",
    "        scaling_strategy_name = preprocessing_cfg.get('scaling_strategy', 'standard_scaler')\n",
    "        imputation_strategy_name = preprocessing_cfg.get('imputation_strategy', 'median')\n",
    "\n",
    "        SCALER_ARTIFACT_LOGICAL_NAME = f\"scaler_{scaling_strategy_name.lower().replace('scaler','').replace('_','')}_{DATASET_IDENTIFIER}\"\n",
    "        IMPUTER_ARTIFACT_LOGICAL_NAME = f\"simple_imputer_{imputation_strategy_name.lower()}_{DATASET_IDENTIFIER}\"\n",
    "        PREPROCESSOR_ARTIFACT_TYPE = f\"preprocessor_{DATASET_IDENTIFIER}\"\n",
    "        PREPROCESSOR_ARTIFACT_VERSION = \"latest\" # Or a specific pinned version\n",
    "\n",
    "        # --- Download SCALER Artifact ---\n",
    "        scaler_artifact_full_wandb_path = f\"{base_config['wandb']['entity']}/{base_config['wandb']['project_name']}/{SCALER_ARTIFACT_LOGICAL_NAME}:{PREPROCESSOR_ARTIFACT_VERSION}\"\n",
    "        print(f\"  Attempting to use Scaler artifact: {scaler_artifact_full_wandb_path}\")\n",
    "        scaler_artifact = run.use_artifact(scaler_artifact_full_wandb_path, type=PREPROCESSOR_ARTIFACT_TYPE)\n",
    "        scaler_artifact_dir = Path(scaler_artifact.download())\n",
    "        scaler_fname_pattern = base_config.get(f\"pipeline_artefact_locators_{DATASET_IDENTIFIER}\", {})\\\n",
    "            .get('scaler_fname_pattern', '{scaling_strategy}_{dataset_identifier}.joblib')\n",
    "        scaler_filename_in_artifact = scaler_fname_pattern.format(\n",
    "            scaling_strategy=scaling_strategy_name.lower(), dataset_identifier=DATASET_IDENTIFIER)\n",
    "        SCALER_PATH_FROM_ARTIFACT = scaler_artifact_dir / scaler_filename_in_artifact\n",
    "        if not SCALER_PATH_FROM_ARTIFACT.is_file(): # Fallback scan\n",
    "            found_scalers = list(scaler_artifact_dir.glob(\"*.joblib\"))\n",
    "            if found_scalers: SCALER_PATH_FROM_ARTIFACT = found_scalers[0]\n",
    "            else: raise FileNotFoundError(f\"Scaler .joblib ('{scaler_filename_in_artifact}') not found in artifact {scaler_artifact.name}\")\n",
    "        print(f\"  Scaler artifact '{scaler_artifact.name}' downloaded. Local path: {SCALER_PATH_FROM_ARTIFACT}\")\n",
    "\n",
    "        # --- Download IMPUTER Artifact ---\n",
    "        imputer_artifact_full_wandb_path = f\"{base_config['wandb']['entity']}/{base_config['wandb']['project_name']}/{IMPUTER_ARTIFACT_LOGICAL_NAME}:{PREPROCESSOR_ARTIFACT_VERSION}\"\n",
    "        print(f\"  Attempting to use Imputer artifact: {imputer_artifact_full_wandb_path}\")\n",
    "        imputer_artifact = run.use_artifact(imputer_artifact_full_wandb_path, type=PREPROCESSOR_ARTIFACT_TYPE)\n",
    "        imputer_artifact_dir = Path(imputer_artifact.download())\n",
    "        imputer_fname_pattern = base_config.get(f\"pipeline_artefact_locators_{DATASET_IDENTIFIER}\", {})\\\n",
    "            .get('imputer_fname_pattern', 'simple_imputer_{imputation_strategy}_{dataset_identifier}.joblib')\n",
    "        imputer_filename_in_artifact = imputer_fname_pattern.format(\n",
    "            imputation_strategy=imputation_strategy_name.lower(), dataset_identifier=DATASET_IDENTIFIER)\n",
    "        IMPUTER_PATH_FROM_ARTIFACT = imputer_artifact_dir / imputer_filename_in_artifact\n",
    "        if not IMPUTER_PATH_FROM_ARTIFACT.is_file(): # Fallback scan\n",
    "            found_imputers = list(imputer_artifact_dir.glob(\"*.joblib\"))\n",
    "            if found_imputers: IMPUTER_PATH_FROM_ARTIFACT = found_imputers[0]\n",
    "            else: raise FileNotFoundError(f\"Imputer .joblib ('{imputer_filename_in_artifact}') not found in artifact {imputer_artifact.name}\")\n",
    "        print(f\"  Imputer artifact '{imputer_artifact.name}' downloaded. Local path: {IMPUTER_PATH_FROM_ARTIFACT}\")\n",
    "\n",
    "        # --- Fetch Full Configuration from the NB04 Run that Produced these Preprocessors ---\n",
    "        nb04_producer_run = imputer_artifact.logged_by() # Use one of the artifacts to get the producer run. CHECK if the run is correct\n",
    "        if nb04_producer_run:\n",
    "            source_nb04_run_id_logged = nb04_producer_run.id\n",
    "            nb04_run_name = nb04_producer_run.name\n",
    "            print(f\"  Fetching config from NB04 producer run: '{nb04_run_name}' (ID: {source_nb04_run_id_logged})\")\n",
    "            config_from_nb04_run = dict(nb04_producer_run.config)\n",
    "            if 'features' not in config_from_nb04_run or 'preprocess' not in config_from_nb04_run:\n",
    "                raise ValueError(\"Fetched NB04 config missing 'features' or 'preprocess' sections.\")\n",
    "            config_for_dataset = config_from_nb04_run # This is the authoritative config for OASISDataset\n",
    "            print(\"  Definitive 'features' and 'preprocess' config for OASISDataset fetched.\")\n",
    "\n",
    "            # Update HP with input_size from this fetched config\n",
    "            time_varying_cfg = config_for_dataset.get('features', {}).get('time_varying', [])\n",
    "            static_cfg = config_for_dataset.get('features', {}).get('static', [])\n",
    "            # Note: 'M/F_encoded' is expected in static_cfg if 'M/F' was handled.\n",
    "            # The input_size should match the number of features OASISDataset will output per time step.\n",
    "            HP['input_size'] = len(time_varying_cfg) + len(static_cfg)\n",
    "            print(f\"  Updated HP['input_size'] to {HP['input_size']} based on fetched feature lists.\")\n",
    "            if HP['input_size'] == 0:\n",
    "                print(\"  WARNING: Calculated input_size is 0. Check fetched feature lists from NB04.\")\n",
    "\n",
    "            # Update current NB06 run's config with source info and final dataset config used\n",
    "            run.config.update({\n",
    "                \"source_config_from_nb04/producer_run_id\": source_nb04_run_id_logged,\n",
    "                \"source_config_from_nb04/producer_run_name\": nb04_run_name,\n",
    "                \"source_config_from_nb04/scaler_artifact_used\": scaler_artifact.name,\n",
    "                \"source_config_from_nb04/imputer_artifact_used\": imputer_artifact.name,\n",
    "                \"dataset_config_used/features\": config_for_dataset.get('features',{}),\n",
    "                \"dataset_config_used/preprocess\": config_for_dataset.get('preprocess',{}),\n",
    "                \"dataset_config_used/cnn_model_params\": config_for_dataset.get('cnn_model_params',{}), # For OASISDataset\n",
    "                \"dataset_config_used/preprocessing_config_mri\": config_for_dataset.get('preprocessing_config',{}), # For OASISDataset\n",
    "                \"model_input_size_resolved\": HP['input_size'] # Log resolved input size\n",
    "            }, allow_val_change=True)\n",
    "            print(\"  NB06 W&B run config updated with source NB04 info and final dataset config.\")\n",
    "        else:\n",
    "            raise ConnectionError(\"Could not retrieve producer run from preprocessor artifact (artifact.logged_by() failed).\")\n",
    "\n",
    "    except Exception as e_fetch_nb04_all:\n",
    "        print(f\"CRITICAL ERROR fetching preprocessor artifacts or config from NB04: {e_fetch_nb04_all}\")\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "else: # W&B run for NB06 failed to initialize\n",
    "    print(\"CRITICAL ERROR: W&B run for NB06 not initialized. Cannot fetch artifacts or proceed with training setup.\")\n",
    "    # Define fallbacks if absolutely necessary for code to not break immediately, but this is a critical failure\n",
    "    SCALER_PATH_FROM_ARTIFACT = Path(base_config.get(f\"pipeline_artefact_locators_{DATASET_IDENTIFIER}\", {}).get('scaler_path_fallback', 'scaler.joblib')) # Highly unlikely to work\n",
    "    IMPUTER_PATH_FROM_ARTIFACT = Path(base_config.get(f\"pipeline_artefact_locators_{DATASET_IDENTIFIER}\", {}).get('imputer_path_fallback', 'imputer.joblib'))\n",
    "    config_for_dataset.setdefault('features', {'time_varying': [], 'static': []})\n",
    "    config_for_dataset.setdefault('preprocess', {'imputation_cols': [], 'scaling_cols': []})\n",
    "    config_for_dataset.setdefault('cnn_model_params', base_config.get('cnn_model_params', {}))\n",
    "    config_for_dataset.setdefault('preprocessing_config', base_config.get('preprocessing_config', {}))\n",
    "    HP['input_size'] = 0 # Fallback\n",
    "    # exit()\n",
    "\n",
    "# Final check on critical variables for OASISDataset\n",
    "if not all([SCALER_PATH_FROM_ARTIFACT, IMPUTER_PATH_FROM_ARTIFACT, config_for_dataset.get('features'), HP.get('input_size') is not None]):\n",
    "    print(\"CRITICAL ERROR: Essential components for OASISDataset (preprocessor paths, feature config, or model input_size) are not properly set up.\")\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6e34e",
   "metadata": {},
   "source": [
    "## 4. Setup Device\n",
    "\n",
    "Detect and set the appropriate PyTorch device (CUDA GPU, MPS for Apple Silicon, or CPU) for model training and data handling. The chosen device is logged to W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe6e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup Device (GPU/CPU/MPS) ---\n",
    "print(\"\\n--- Setting up PyTorch Device ---\")\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU).\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU.\")\n",
    "\n",
    "# Log the determined device to W&B run config\n",
    "if run: \n",
    "    run.config.update({'training_details/device_used': str(device)}, allow_val_change=True)\n",
    "else:\n",
    "    print(f\"Device set to {device} (W&B run not active for logging).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd2717",
   "metadata": {},
   "source": [
    "## 5. Load Data & Create DataLoaders for Baseline Model\n",
    "\n",
    "This section instantiates the custom `OASISDataset` for the training and validation data splits (`cohort_train.parquet` and `cohort_validation.parquet`). Key configurations for the dataset are critical for consistency:\n",
    "\n",
    "* **Preprocessor Paths:** The paths to the *fitted* `StandardScaler` and `SimpleImputer` (obtained by downloading W&B Artifacts logged by Notebook 04) are provided to `OASISDataset`.\n",
    "* **Feature & Preprocessing Configuration:** The `config_for_dataset` dictionary (fetched from the W&B configuration of the Notebook 04 run that produced the preprocessors) is passed to `OASISDataset`. This ensures it uses the authoritative lists of time-varying/static features and imputation/scaling columns.\n",
    "* **MRI Data:** For this baseline model, `include_mri` is explicitly set to `False`.\n",
    "\n",
    "The instantiated `OASISDataset` objects are then wrapped in PyTorch `DataLoader`s, which handle batching and use the custom `pad_collate_fn` to manage variable sequence lengths. The `input_size` hyperparameter for the model is also confirmed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d4383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate Datasets and DataLoaders for Baseline Model ---\n",
    "print(\"\\n--- Loading Data and Creating DataLoaders for Baseline Model Training ---\")\n",
    "\n",
    "# Initialize dataset and loader variables to ensure they are defined\n",
    "train_dataset: OASISDataset | None = None\n",
    "val_dataset: OASISDataset | None = None\n",
    "train_loader: DataLoader | None = None\n",
    "val_loader: DataLoader | None = None\n",
    "\n",
    "# --- Prerequisite Variable Check ---\n",
    "# These are expected to be defined and populated from preceding cells\n",
    "required_vars_for_dataloading = {\n",
    "    'TRAIN_DATA_PATH': TRAIN_DATA_PATH,\n",
    "    'VAL_DATA_PATH': VAL_DATA_PATH,\n",
    "    'SCALER_PATH_FROM_ARTIFACT': SCALER_PATH_FROM_ARTIFACT,\n",
    "    'IMPUTER_PATH_FROM_ARTIFACT': IMPUTER_PATH_FROM_ARTIFACT,\n",
    "    'config_for_dataset': config_for_dataset, # Authoritative config from NB04 run\n",
    "    'HP': HP, # For batch_size, num_workers, and input_size\n",
    "    'pad_collate_fn': pad_collate_fn,\n",
    "    'OASISDataset': OASISDataset\n",
    "}\n",
    "\n",
    "proceed_with_loading = True\n",
    "for var_name, var_value in required_vars_for_dataloading.items():\n",
    "    if var_value is None:\n",
    "        print(f\"  CRITICAL ERROR: Prerequisite variable '{var_name}' for DataLoaders is None.\")\n",
    "        proceed_with_loading = False\n",
    "    if var_name == 'config_for_dataset' and \\\n",
    "       (not var_value or 'features' not in var_value or 'preprocess' not in var_value):\n",
    "        print(f\"  CRITICAL ERROR: 'config_for_dataset' is empty or missing 'features'/'preprocess' keys.\")\n",
    "        proceed_with_loading = False\n",
    "    if var_name == 'HP' and (not var_value or 'input_size' not in var_value or HP.get('input_size') is None):\n",
    "        print(f\"  CRITICAL ERROR: 'HP' dictionary or 'HP['input_size']' is not correctly set \"\n",
    "              \"(should be updated after fetching NB04 config).\")\n",
    "        proceed_with_loading = False\n",
    "\n",
    "if not proceed_with_loading:\n",
    "    print(\"Halting DataLoader creation due to missing or invalid prerequisites from previous cells.\")\n",
    "    if run: run.finish(exit_code=1) # Mark W&B run as failed\n",
    "    # exit() # Or raise an error\n",
    "else:\n",
    "    try:\n",
    "        print(f\"\\nInstantiating training dataset for {DATASET_IDENTIFIER.upper()} (Baseline - MRI Excluded)...\")\n",
    "        train_dataset = OASISDataset(\n",
    "            data_parquet_path=TRAIN_DATA_PATH,\n",
    "            scaler_path=SCALER_PATH_FROM_ARTIFACT,   # Path to downloaded .joblib\n",
    "            imputer_path=IMPUTER_PATH_FROM_ARTIFACT, # Path to downloaded .joblib\n",
    "            config=config_for_dataset,               # Config from NB04 W&B run\n",
    "            include_mri=False                        # Explicitly False for baseline model\n",
    "            # mri_data_dir is not needed when include_mri is False\n",
    "        )\n",
    "        num_train_subjects = len(train_dataset)\n",
    "        print(f\"  Train dataset (baseline) created. Number of subjects (sequences): {num_train_subjects}\")\n",
    "\n",
    "        print(f\"\\nInstantiating validation dataset for {DATASET_IDENTIFIER.upper()} (Baseline - MRI Excluded)...\")\n",
    "        val_dataset = OASISDataset(\n",
    "            data_parquet_path=VAL_DATA_PATH,\n",
    "            scaler_path=SCALER_PATH_FROM_ARTIFACT,   # Use the SAME scaler/imputer\n",
    "            imputer_path=IMPUTER_PATH_FROM_ARTIFACT,\n",
    "            config=config_for_dataset,               # Use the SAME config\n",
    "            include_mri=False\n",
    "        )\n",
    "        num_val_subjects = len(val_dataset)\n",
    "        print(f\"  Validation dataset (baseline) created. Number of subjects (sequences): {num_val_subjects}\")\n",
    "\n",
    "        # --- Create DataLoaders ---\n",
    "        print(\"\\nCreating DataLoaders...\")\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=HP['batch_size'], \n",
    "            shuffle=True, # Shuffle training data each epoch\n",
    "            collate_fn=pad_collate_fn,\n",
    "            num_workers=HP.get('dataloader_num_workers', 0),\n",
    "            persistent_workers=(HP.get('dataloader_num_workers',0) > 0) if HP.get('dataloader_num_workers',0) > 0 else False # Only if num_workers > 0\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=HP['batch_size'], \n",
    "            shuffle=False, # No need to shuffle validation data\n",
    "            collate_fn=pad_collate_fn,\n",
    "            num_workers=HP.get('dataloader_num_workers', 0),\n",
    "            persistent_workers=(HP.get('dataloader_num_workers',0) > 0) if HP.get('dataloader_num_workers',0) > 0 else False\n",
    "        )\n",
    "        print(\"Train and Validation DataLoaders for baseline model created.\")\n",
    "        print(f\"  Number of batches in train_loader: ~{len(train_loader)}\")\n",
    "        print(f\"  Number of batches in val_loader: ~{len(val_loader)}\")\n",
    "\n",
    "        # Log dataset info to W&B for this training run\n",
    "        if run:\n",
    "            run.log({\n",
    "                f'dataset_info_{DATASET_IDENTIFIER}/train_subjects': num_train_subjects,\n",
    "                f'dataset_info_{DATASET_IDENTIFIER}/val_subjects': num_val_subjects,\n",
    "                f'dataset_info_{DATASET_IDENTIFIER}/input_size_for_model': HP['input_size'],\n",
    "                f'dataset_info_{DATASET_IDENTIFIER}/batch_size': HP['batch_size']\n",
    "            })\n",
    "        \n",
    "        # Confirm input_size for the model (this was set in HP after fetching NB04 config)\n",
    "        print(f\"\\nInput size (number of features) for BaselineLSTMRegressor: {HP['input_size']}\")\n",
    "        if HP['input_size'] is None or HP['input_size'] == 0:\n",
    "            print(\"WARNING: HP['input_size'] is None or 0. The model will likely fail. \"\n",
    "                  \"Check fetching of NB04 config and feature list processing.\")\n",
    "\n",
    "    except FileNotFoundError as e_fnf_ds_nb06:\n",
    "        print(f\"CRITICAL ERROR during OASISDataset instantiation in NB06: File not found - {e_fnf_ds_nb06}\")\n",
    "        print(\"  Verify paths to data splits and downloaded preprocessor artifacts from NB04.\")\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "    except KeyError as e_key_ds_nb06:\n",
    "        print(f\"CRITICAL ERROR during OASISDataset instantiation in NB06: Missing a key in 'config_for_dataset' - {e_key_ds_nb06}\")\n",
    "        print(\"  Ensure the configuration fetched from the Notebook 04 W&B run is complete.\")\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "    except Exception as e_ds_init_nb06:\n",
    "        print(f\"An unexpected CRITICAL ERROR occurred during OASISDataset or DataLoader instantiation in NB06: {e_ds_init_nb06}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "\n",
    "# Final check for subsequent cells\n",
    "if train_loader is None or val_loader is None:\n",
    "    print(\"CRITICAL ERROR: DataLoaders were not successfully created. Cannot proceed with model training.\")\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6010fa1",
   "metadata": {},
   "source": [
    "## 6. Define Model, Loss Function, and Optimizer\n",
    "\n",
    "This section sets up the components required for training the `BaselineLSTMRegressor`:\n",
    "1.  **Model Instantiation:** The `BaselineLSTMRegressor` (imported from `src/models.py`) is instantiated using the hyperparameters defined in the `HP` dictionary, including the `input_size` (number of input features per timestep, determined from the NB04 configuration), `lstm_hidden_size`, `num_lstm_layers`, and `lstm_dropout_prob`. The model is then moved to the configured PyTorch `device` (e.g., CUDA, MPS, or CPU).\n",
    "2.  **Loss Function:** The Mean Squared Error loss (`nn.MSELoss`) is chosen, which is appropriate for regression tasks like predicting a continuous CDR score.\n",
    "3.  **Optimizer:** The Adam optimizer (`optim.Adam`) is selected to update the model's weights during training, configured with the learning rate from `HP`.\n",
    "4.  **W&B Model Watching (Optional):** If a W&B run is active, `wandb.watch()` is called to monitor model gradients, parameters, and architecture throughout the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Model, Loss Function, and Optimizer ---\n",
    "print(\"\\n--- Defining Model, Loss Function, and Optimizer for BaselineLSTM ---\")\n",
    "\n",
    "# These variables are expected to be defined from previous cells:\n",
    "# HP (dictionary with model hyperparameters like input_size, lstm_hidden_size, etc.)\n",
    "# device (torch.device)\n",
    "# BaselineLSTMRegressor (imported model class)\n",
    "# run (active W&B run object, or None)\n",
    "\n",
    "model = None\n",
    "criterion = None\n",
    "optimizer = None\n",
    "\n",
    "# Ensure HP dictionary and critical model parameters are available\n",
    "if 'HP' not in locals() or not HP or HP.get('input_size') is None:\n",
    "    print(\"CRITICAL ERROR: HP dictionary not defined or 'input_size' is missing or None. \"\n",
    "          \"Ensure 'input_size' was correctly updated after fetching NB04 config.\")\n",
    "    if run: run.finish(exit_code=1)\n",
    "    # exit() # Or raise error\n",
    "else:\n",
    "    try:\n",
    "        # 1. Instantiate the BaselineLSTMRegressor model\n",
    "        print(f\"  Instantiating BaselineLSTMRegressor with input_size: {HP['input_size']}\")\n",
    "        model = BaselineLSTMRegressor(\n",
    "            input_size=HP['input_size'],\n",
    "            hidden_size=HP['lstm_hidden_size'],\n",
    "            num_layers=HP['num_lstm_layers'],\n",
    "            dropout_prob=HP['lstm_dropout_prob']\n",
    "        )\n",
    "        model.to(device) # Move model to the selected device\n",
    "        print(\"  BaselineLSTMRegressor model instantiated and moved to device.\")\n",
    "        # print(model) # Optional: print model architecture\n",
    "\n",
    "        # 2. Define the Loss Function (Criterion)\n",
    "        criterion = nn.MSELoss()\n",
    "        print(f\"  Loss function set to: {type(criterion).__name__}\")\n",
    "\n",
    "        # 3. Define the Optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr=HP['learning_rate'])\n",
    "        print(f\"  Optimizer set to: {type(optimizer).__name__} with LR={HP['learning_rate']}\")\n",
    "\n",
    "        # 4. Optional: Watch model with W&B for gradients, parameters, etc.\n",
    "        if run:\n",
    "            wandb.watch(model, criterion=criterion, log='all', log_freq=100) # Log gradients, parameters, and outputs\n",
    "            print(\"  W&B model watching enabled.\")\n",
    "\n",
    "    except KeyError as e_key_model:\n",
    "        print(f\"CRITICAL ERROR: Missing a key in HP dictionary needed for model instantiation: {e_key_model}\")\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "    except Exception as e_model_setup:\n",
    "        print(f\"CRITICAL ERROR during model, loss, or optimizer setup: {e_model_setup}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()\n",
    "\n",
    "# Check if model setup was successful for subsequent cells\n",
    "if model is None or criterion is None or optimizer is None:\n",
    "    print(\"CRITICAL ERROR: Model setup failed. Cannot proceed to training.\")\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d2a69d",
   "metadata": {},
   "source": [
    "## 7. Train Baseline Model using Utility Function\n",
    "\n",
    "The core model training and validation process is now encapsulated within the `train_model` utility function (from `src/training_utils.py`). This function is called with the instantiated model, data loaders, criterion, optimizer, device, number of epochs, active W&B run object, checkpoint directory, and other relevant hyperparameters.\n",
    "\n",
    "The `train_model` utility handles:\n",
    "* The main loop over epochs.\n",
    "* The training phase for each epoch (forward pass, loss calculation, backpropagation, optimizer step).\n",
    "* The validation phase for each epoch (using the `evaluate_model` utility passed to it).\n",
    "* Logging all training and validation metrics (Loss, MAE, R2, epoch duration) to W&B.\n",
    "* **Best Model Checkpointing:** Saving the model state (`.pth` file) locally to the run-specific output directory (`run_output_dir_for_checkpoints`) whenever validation loss improves. This best model is also logged as a W&B Artifact with the 'best' alias.\n",
    "* **Periodic Checkpointing:** Saving model state, optimizer state, and current metrics periodically (e.g., every `HP['save_checkpoint_every_n_epochs']` epochs) for resumable training. These are also logged as W&B Artifacts.\n",
    "* **Early Stopping:** Monitoring validation loss and stopping training if no improvement is seen for a defined `patience` number of epochs.\n",
    "\n",
    "The function returns the local path to the best model checkpoint and the best validation loss achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train Baseline Model using the train_model Utility ---\n",
    "print(\"\\n--- Starting Baseline Model Training using 'train_model' utility ---\")\n",
    "\n",
    "# Ensure all necessary inputs for train_model are defined from previous cells:\n",
    "# model, train_loader, val_loader, criterion, optimizer, device, HP, run, \n",
    "# run_output_dir_for_checkpoints (for saving .pth files), evaluate_model (imported function)\n",
    "\n",
    "path_to_best_checkpoint_local = None\n",
    "best_validation_loss_achieved = float('inf')\n",
    "\n",
    "required_vars_for_training = [\n",
    "    'model', 'train_loader', 'val_loader', 'criterion', 'optimizer', 'device', 'HP',\n",
    "    'run_output_dir_for_checkpoints', 'evaluate_model' \n",
    "    # 'run' can be None if W&B init failed, train_model should handle that\n",
    "]\n",
    "\n",
    "proceed_with_training = True\n",
    "for var_name in required_vars_for_training:\n",
    "    if var_name not in locals() or locals()[var_name] is None:\n",
    "        # Special case for 'run' which can be None\n",
    "        if var_name == 'run' and ('run' not in locals() or locals()['run'] is None):\n",
    "            print(\"  Note: W&B run object is None. Training will proceed without W&B logging within train_model.\")\n",
    "            # No need to set proceed_with_training to False just for this\n",
    "        else:\n",
    "            print(f\"  CRITICAL ERROR: Prerequisite variable '{var_name}' for training is not defined or is None.\")\n",
    "            proceed_with_training = False\n",
    "\n",
    "if not proceed_with_training:\n",
    "    print(\"Halting model training due to missing or invalid prerequisites.\")\n",
    "    if run: run.finish(exit_code=1)\n",
    "    # exit()\n",
    "else:\n",
    "    try:\n",
    "        path_to_best_checkpoint_local, best_validation_loss_achieved = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            num_epochs=HP['epochs'],\n",
    "            wandb_run=run, # Pass the active W&B run object (can be None)\n",
    "            checkpoint_dir=run_output_dir_for_checkpoints, \n",
    "            evaluate_fn=evaluate_model, # Pass the imported evaluate_model function\n",
    "            model_type_flag=\"baseline\", # Critical for batch unpacking in evaluate_fn & train_model\n",
    "            hp_dict=HP, # Pass the HP dict for patience, save_every_n_epochs\n",
    "            best_val_loss_init=float('inf') # Start with fresh best loss tracking\n",
    "        )\n",
    "\n",
    "        print(f\"\\n--- Training Complete (via 'train_model' utility) ---\")\n",
    "        print(f\"Best validation loss achieved during training: {best_validation_loss_achieved:.4f}\")\n",
    "        if path_to_best_checkpoint_local and path_to_best_checkpoint_local.exists():\n",
    "            print(f\"Best model checkpoint saved locally at: {path_to_best_checkpoint_local}\")\n",
    "            # Log this path to W&B summary for easy reference if run is active\n",
    "            if run:\n",
    "                run.summary['best_model_local_path'] = str(path_to_best_checkpoint_local)\n",
    "                run.summary['best_val_loss_final'] = best_validation_loss_achieved # Ensure final best_val_loss is in summary\n",
    "        elif path_to_best_checkpoint_local: # Path was returned but file doesn't exist (should not happen if train_model is correct)\n",
    "            print(f\"Warning: train_model returned a best checkpoint path, but file not found: {path_to_best_checkpoint_local}\")\n",
    "        else: # No checkpoint saved (e.g., 0 epochs, or error in checkpointing, or no improvement)\n",
    "            print(\"No best model checkpoint was saved during training (e.g., no improvement in val_loss or training error).\")\n",
    "\n",
    "    except Exception as e_train:\n",
    "        print(f\"CRITICAL ERROR occurred during model training: {e_train}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if run: run.finish(exit_code=1)\n",
    "        # exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99090e8",
   "metadata": {},
   "source": [
    "## 8. Evaluate Best Model on Test Set\n",
    "\n",
    "After training is complete, the best performing model (based on the lowest validation loss achieved during training) is loaded from its saved checkpoint. This model is then evaluated on the held-out test set (`cohort_test.parquet`) to assess its generalization performance.\n",
    "\n",
    "The evaluation uses the `evaluate_model` utility, providing metrics such as Test Loss, Mean Squared Error (MSE), Mean Absolute Error (MAE), and R-squared (R²). These final test metrics are logged to the W&B run's summary for this training experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate Best Model on Test Set ---\n",
    "print(\"\\n--- Evaluating Best Model on Test Set ---\")\n",
    "\n",
    "# Ensure path_to_best_checkpoint_local, HP, config_for_dataset, device are available\n",
    "# Also TEST_DATA_PATH, SCALER_PATH_FROM_ARTIFACT, IMPUTER_PATH_FROM_ARTIFACT\n",
    "\n",
    "if 'path_to_best_checkpoint_local' in locals() and \\\n",
    "   path_to_best_checkpoint_local is not None and \\\n",
    "   path_to_best_checkpoint_local.exists() and \\\n",
    "   'HP' in locals() and HP and \\\n",
    "   'config_for_dataset' in locals() and config_for_dataset and \\\n",
    "   'TEST_DATA_PATH' in locals() and TEST_DATA_PATH.is_file() and \\\n",
    "   'SCALER_PATH_FROM_ARTIFACT' in locals() and SCALER_PATH_FROM_ARTIFACT.is_file() and \\\n",
    "   'IMPUTER_PATH_FROM_ARTIFACT' in locals() and IMPUTER_PATH_FROM_ARTIFACT.is_file():\n",
    "\n",
    "    print(f\"Loading best model for test evaluation from: {path_to_best_checkpoint_local}\")\n",
    "\n",
    "    # 1. Instantiate a new model of the same architecture\n",
    "    # Ensure HP contains the necessary parameters for model instantiation\n",
    "    try:\n",
    "        test_model = BaselineLSTMRegressor(\n",
    "            input_size=HP['input_size'], # Should be correctly set from NB04 config via HP\n",
    "            hidden_size=HP['lstm_hidden_size'],\n",
    "            num_layers=HP['num_lstm_layers'],\n",
    "            dropout_prob=HP['lstm_dropout_prob'] # Use training dropout, will be disabled by model.eval()\n",
    "        )\n",
    "        \n",
    "        # Load the saved state dictionary\n",
    "        test_model.load_state_dict(torch.load(path_to_best_checkpoint_local, map_location=device))\n",
    "        test_model.to(device)\n",
    "        # model.eval() is called inside evaluate_model, but good practice here too\n",
    "        test_model.eval() \n",
    "        print(\"  Best model loaded and set to evaluation mode.\")\n",
    "\n",
    "        # 2. Create DataLoader for the Test Set\n",
    "        print(f\"  Instantiating test dataset from: {TEST_DATA_PATH} (Baseline - MRI Excluded)...\")\n",
    "        test_dataset = OASISDataset(\n",
    "            data_parquet_path=TEST_DATA_PATH,\n",
    "            scaler_path=SCALER_PATH_FROM_ARTIFACT,\n",
    "            imputer_path=IMPUTER_PATH_FROM_ARTIFACT,\n",
    "            config=config_for_dataset, # Use the same authoritative config from NB04\n",
    "            include_mri=False # Explicitly False for baseline model\n",
    "        )\n",
    "        print(f\"  Test dataset created with {len(test_dataset)} subjects.\")\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=HP['batch_size'], # Can use training batch size or a different one\n",
    "            shuffle=False, # No need to shuffle test data\n",
    "            collate_fn=pad_collate_fn,\n",
    "            num_workers=HP.get('dataloader_num_workers', 0)\n",
    "        )\n",
    "        print(f\"  Test DataLoader created. Number of batches: ~{len(test_loader)}\")\n",
    "\n",
    "        # 3. Perform Evaluation using the utility function\n",
    "        # Ensure criterion is defined (it was defined before calling train_model)\n",
    "        if 'criterion' not in locals() or criterion is None: \n",
    "            print(\"  Warning: Criterion not defined, re-initializing to MSELoss for test evaluation.\")\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "        print(f\"  Evaluating best model on {len(test_dataset)} test subjects...\")\n",
    "        test_set_metrics = evaluate_model(\n",
    "            test_model, \n",
    "            test_loader, \n",
    "            criterion, \n",
    "            device, \n",
    "            model_name_for_batch_unpack=\"baseline\"\n",
    "        )\n",
    "\n",
    "        print(f\"\\n--- Test Set Performance of Best Model ---\")\n",
    "        print(f\"  (Model from epoch with validation loss: {best_validation_loss_achieved:.4f})\") # From train_model output\n",
    "        print(f\"  Test Loss (Criterion): {test_set_metrics['loss']:.4f}\")\n",
    "        print(f\"  Test MSE (from preds): {test_set_metrics['mse']:.4f}\") # Added MSE\n",
    "        print(f\"  Test MAE:  {test_set_metrics['mae']:.4f}\")\n",
    "        print(f\"  Test R2:   {test_set_metrics['r2']:.4f}\")\n",
    "\n",
    "        # 4. Log Test Metrics to W&B Summary for this training run\n",
    "        if run:\n",
    "            run.summary[\"test_set/loss\"] = test_set_metrics['loss']\n",
    "            run.summary[\"test_set/mse\"] = test_set_metrics['mse']\n",
    "            run.summary[\"test_set/mae\"] = test_set_metrics['mae']\n",
    "            run.summary[\"test_set/r2\"] = test_set_metrics['r2']\n",
    "            # best_validation_loss_achieved was already logged by train_model to run.summary['best_val_loss_final'] if W&B active\n",
    "            # but good to have it explicitly related to the test checkpoint here too.\n",
    "            run.summary[\"test_set/checkpoint_best_val_loss\"] = best_validation_loss_achieved \n",
    "            print(\"  Test metrics logged to W&B run summary.\")\n",
    "            \n",
    "    except Exception as e_test_eval:\n",
    "        print(f\"CRITICAL ERROR during test set evaluation: {e_test_eval}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if run: run.summary[\"test_set/status\"] = \"EvaluationError\"\n",
    "\n",
    "else:\n",
    "    print(\"Skipping test set evaluation: Best model checkpoint not found, not saved, or other prerequisites missing.\")\n",
    "    if run: run.summary[\"test_set/status\"] = \"SkippedEvaluation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10847ae7",
   "metadata": {},
   "source": [
    "## 9. Finalize W&B Run\n",
    "\n",
    "Complete the execution of this training notebook and finish the associated Weights & Biases run. This ensures all queued logs, metrics, configurations, and model artifacts are fully uploaded and synchronized with the W&B platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411b7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Finish W&B Run for Notebook 06 ---\n",
    "print(f\"\\n--- {NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER} (Baseline Training) complete. Finishing W&B run. ---\")\n",
    "\n",
    "if run: # Check if 'run' object exists and is an active W&B run\n",
    "    try:\n",
    "        # Ensure final best_val_loss is in summary if not already covered by train_model or test summary\n",
    "        if 'best_validation_loss_achieved' in locals() and 'best_val_loss_final' not in run.summary:\n",
    "            run.summary['best_val_loss_final'] = best_validation_loss_achieved\n",
    "        \n",
    "        run.finish()\n",
    "        run_name_to_print = run.name_synced if hasattr(run, 'name_synced') and run.name_synced else \\\n",
    "                            run.name if hasattr(run, 'name') and run.name else \\\n",
    "                            run.id if hasattr(run, 'id') else \"current NB06 run\"\n",
    "        print(f\"W&B run '{run_name_to_print}' finished successfully.\")\n",
    "    except Exception as e_finish_run_nb06:\n",
    "        print(f\"Error during wandb.finish() for Notebook 06: {e_finish_run_nb06}\")\n",
    "        print(\"The run may not have finalized correctly on the W&B server.\")\n",
    "else:\n",
    "    print(\"No active W&B run to finish for this training session.\")\n",
    "\n",
    "print(f\"\\n--- Notebook {NOTEBOOK_MODULE_NAME}_{DATASET_IDENTIFIER} execution finished. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro_predcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
